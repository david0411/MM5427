{
 "cells": [
  {
   "cell_type": "code",
   "id": "1fa30b36-14d0-41c9-a4ec-50494c9a5baa",
   "metadata": {
    "tags": []
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8edb6e16-ccad-41ec-8d0e-7a2ae7240688",
   "metadata": {
    "tags": []
   },
   "source": [
    "# path = 'D:/study/poly/sem2/MM5427 Textual Analysis in Business/group project/groupcode/AnnualReports16_processed2.csv'\n",
    "df = pd.read_csv(\"../document/AnnualReports1718.csv\")\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "52353e31-4270-4595-a0ef-2d1697dba090",
   "metadata": {},
   "source": "# pip install spacy",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3e13188b-ece6-4a6b-92df-10ab0949c712",
   "metadata": {
    "tags": []
   },
   "source": [
    "### test processing"
   ]
  },
  {
   "cell_type": "code",
   "id": "7dd55cfb-f755-414a-95b0-ebaf2f4a68a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "import nltk\n",
    "import spacy \n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from nltk.stem import WordNetLemmatizer"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "011ee415-9c5c-4de6-82dd-5fbff5375ae4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Function to count the number of sentences in a text\n",
    "def count_sentences(text):\n",
    "    # Handle NaN values by returning 0 sentences\n",
    "    if pd.isnull(text):\n",
    "        return 0\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return len(sentences)\n",
    "\n",
    "# Function to delete the first sentence in a text\n",
    "def delete_first_sentence(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    if len(sentences) > 2:\n",
    "        return ' '.join(sentences[2:])\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "# Apply the count_sentences function to the 'item7' column and create a new column 'sentence_count'\n",
    "df['sentence_count'] = df['item7'].apply(count_sentences)\n",
    "\n",
    "# Filter the DataFrame to keep only rows with 10 or more sentences in 'item7'\n",
    "df = df[df['sentence_count'] > 10]\n",
    "\n",
    "# Delete the first sentence in each text in the 'item7' column\n",
    "df['item7'] = df['item7'].apply(delete_first_sentence)\n",
    "\n",
    "# 删除空行并重置索引\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "df.head()\n",
    "print(df.item7)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2e31bcd1-0648-4560-822c-520f73f16f10",
   "metadata": {},
   "source": [
    "df.head(10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "98443ccc-bb21-4293-9fbe-0ecbaa83f860",
   "metadata": {},
   "source": [
    "print(df.item7[3])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9826e8bc-db26-4c0c-b370-21176d3786b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Simple preprocessing by removing extra lines and lowercasing all text\n",
    "df['item7'] = df['item7'].replace('\\n','', regex=True)\n",
    "df['item7'] = df['item7'].replace('\\r','', regex=True)\n",
    "df['item7'] = df['item7'].replace('\\r','', regex=True)\n",
    "df['item7'] = df['item7'].replace('[\\d.,]+|[^\\w\\s]', '', regex=True)\n",
    "df['item7'] = [x.lower() for x in df['item7']]\n",
    "df['item7'] = df['item7'].replace('item 7.','', regex=True)\n",
    "\n",
    "# Futher preprocessing by removing all stopwords and lemmatizing all text\n",
    "documents = []\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for text in df['item7']:\n",
    "    # Load English tokenizer, tagger, parser, NER and word vectors\n",
    "    nlp = English()\n",
    "\n",
    "    #  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "    my_doc = nlp(text)\n",
    "\n",
    "    # Create list of word tokens\n",
    "    token_list = []\n",
    "    for token in my_doc:\n",
    "        token_list.append(token.text)\n",
    "\n",
    "    # Create list of word tokens after removing stopwords\n",
    "    filtered_sentence =[] \n",
    "\n",
    "    for word in token_list:\n",
    "        lexeme = nlp.vocab[word]\n",
    "        if lexeme.is_stop == False:\n",
    "            filtered_sentence.append(word) \n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in filtered_sentence]\n",
    "    document = ' '.join(document)\n",
    "\n",
    "    documents.append(document)\n",
    "\n",
    "df['item7'] = documents\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "53bae740-ca66-4c65-8f89-b40113d3837e",
   "metadata": {},
   "source": [
    "### nrc list"
   ]
  },
  {
   "cell_type": "code",
   "id": "d618ade6-8b24-47b3-9d44-2cc43fc5ebf5",
   "metadata": {
    "tags": []
   },
   "source": [
    "nrc = pd.read_csv('../word_list/NRC-Emotion-Lexicon.txt', sep = '\\t', names = ['term', 'category', 'associated'])\n",
    "nrc.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cf2edc63-f289-4012-a0e1-92b16d28b613",
   "metadata": {
    "tags": []
   },
   "source": [
    "# rearrangement\n",
    "category_list = nrc['category'] .unique().tolist()\n",
    "filtered_df = nrc[nrc['associated'] == 1]\n",
    "grouped_df = filtered_df.groupby('category')['term'].apply(list)\n",
    "grouped_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "101993c3-f97d-477a-b2c9-fc553a261ead",
   "metadata": {
    "tags": []
   },
   "source": [
    "anti_list = grouped_df.loc['anticipation']\n",
    "nrc_pos_list = grouped_df.loc['positive']\n",
    "nrc_neg_list = grouped_df.loc['negative']\n",
    "ang_list = grouped_df.loc['anger']\n",
    "anti_list =  grouped_df.loc['anticipation']\n",
    "dis_list =  grouped_df.loc['disgust']\n",
    "joy_list = grouped_df.loc['joy']\n",
    "fear_list = grouped_df.loc['fear']\n",
    "sad_list =  grouped_df.loc['sadness']\n",
    "surp_list = grouped_df.loc['surprise']\n",
    "tru_list =  grouped_df.loc['trust']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "86d88c4f-2e60-4741-98da-24a9a08cf5ac",
   "metadata": {},
   "source": [
    "### McDonald list"
   ]
  },
  {
   "cell_type": "code",
   "id": "553ec6ea-dd5c-4f16-898c-d7c68c07b293",
   "metadata": {
    "tags": []
   },
   "source": [
    "mcd = pd.read_csv('../word_list/Loughran-McDonald_MasterDictionary_1993-2023.csv')\n",
    "mcd['Word'] = mcd['Word'].str.lower()\n",
    "mcd.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "babda5d2-2205-4651-aec5-f783949f9841",
   "metadata": {
    "tags": []
   },
   "source": [
    "neg_list = set(mcd[mcd['Negative'] != 0]['Word'])\n",
    "pos_list = set(mcd[mcd['Positive'] != 0]['Word'])\n",
    "unc_list= set(mcd[mcd['Uncertainty'] != 0]['Word'])\n",
    "lit_list = set(mcd[mcd['Litigious'] != 0]['Word'])\n",
    "stg_list = set(mcd[mcd['Strong_Modal'] != 0]['Word'])\n",
    "weak_list = set(mcd[mcd['Weak_Modal'] != 0]['Word'])\n",
    "ctr_list = set(mcd[mcd['Constraining'] != 0]['Word'])\n",
    "Comp_list = set(mcd[mcd['Complexity'] != 0]['Word'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8ff05443-30e7-4b95-8a15-60f5ae9fedcd",
   "metadata": {},
   "source": [
    "### count sentiment increment"
   ]
  },
  {
   "cell_type": "code",
   "id": "fd4b4178-42eb-4203-a32e-78a5e2ccf9f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# A Function to Construct a Sentiment Variable Using a Lexicon-Based Approach\n",
    "def sentiment_score(text, sen_list):\n",
    "    temp_list = []\n",
    "    for t in text:\n",
    "        if isinstance(t, str):\n",
    "            temp = 0\n",
    "            for w in sen_list:\n",
    "                temp += t.count(w)\n",
    "            if len(t) != 0:\n",
    "                temp_list.append(temp/len(t))\n",
    "            else:\n",
    "                temp_list.append(0)\n",
    "        else:\n",
    "            temp_list.append(0)\n",
    "    return temp_list"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f7529486-5fe5-463d-ab20-4e8142505cc4",
   "metadata": {
    "tags": []
   },
   "source": [
    "sen_df = pd.DataFrame(df['item7']).copy()\n",
    "sen_df['Pos_Dic'] = sentiment_score(df['item7'], pos_list)\n",
    "sen_df['Neg_Dic'] = sentiment_score(df['item7'], neg_list)\n",
    "sen_df['Anti_Dic'] = sentiment_score(df['item7'], anti_list)\n",
    "sen_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ee87b72c-e1df-48b7-8318-d4e69eaa044a",
   "metadata": {
    "tags": []
   },
   "source": [
    "sen_df['pos_anti_increment'] = (sen_df['Pos_Dic'] + sen_df['Anti_Dic'])/ sen_df['Anti_Dic']\n",
    "sen_df['neg_anti_increment'] = (sen_df['Anti_Dic'] - sen_df['Neg_Dic'])/ sen_df['Anti_Dic']\n",
    "sen_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "33bcf81c-bb19-4c24-b9f9-c2ac51176507",
   "metadata": {
    "tags": []
   },
   "source": [
    "sen_df['result'] = sen_df.apply(lambda row: row['pos_anti_increment'] if row['Pos_Dic'] > row['Neg_Dic'] else row['neg_anti_increment'], axis=1)\n",
    "sen_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0fc81436-c90b-49fd-ad80-7bebe59be0d2",
   "metadata": {},
   "source": [
    "### 前期回报与语气增量的影响"
   ]
  },
  {
   "cell_type": "code",
   "id": "fa81366f-97a4-4761-ba8c-ba615ec2394c",
   "metadata": {
    "tags": []
   },
   "source": [
    "X = np.array(df['pre_alpha']).reshape(-1, 1)\n",
    "y = np.array(sen_df['result']).reshape(-1, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "model.fit(X_train, y_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1409d68f-73fa-4df5-9ffe-3a74c57e2219",
   "metadata": {
    "tags": []
   },
   "source": [
    "import statsmodels.api as sm\n",
    "# Coefficient\n",
    "coefficient = model.coef_[0]\n",
    "print(\"Coefficient:\", coefficient)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# MSE\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# R2\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "# P-value using statsmodels\n",
    "X_train_sm = sm.add_constant(X_train)  # Add a constant term to X2_train\n",
    "model_sm = sm.OLS(y_train, X_train_sm)\n",
    "results = model_sm.fit()\n",
    "p_value = results.pvalues[1]\n",
    "print(\"P-value:\", p_value)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8a09bcf9-d1e3-4804-85d1-f4ee4fdb84ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "前期回报情况不好的可能会对未来更积极"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f2262c-2df2-4743-9817-be00704584a4",
   "metadata": {},
   "source": [
    "### 不稳定因素与预期增量的影响"
   ]
  },
  {
   "cell_type": "code",
   "id": "44ec1d0a-74ab-4e51-a360-d4ca09c264a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "df2 = pd.DataFrame(sen_df['result']).copy()\n",
    "df2['unc_Dic'] = sentiment_score(df['item7'], unc_list)\n",
    "df2['stg_Dic'] = sentiment_score(df['item7'], stg_list)\n",
    "df2['weak_Dic'] = sentiment_score(df['item7'], weak_list)\n",
    "\n",
    "df2['lit_Dic'] = sentiment_score(df['item7'], lit_list)\n",
    "df2['ctr_Dic'] = sentiment_score(df['item7'], ctr_list)\n",
    "\n",
    "df2['unc_risk'] = df2['unc_Dic'] + df2['weak_Dic'] - df2['stg_Dic']\n",
    "df2['lit_risk'] = df2['lit_Dic'] + df2['ctr_Dic']\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "878fd50b-e908-4763-b159-cb9a35e26519",
   "metadata": {
    "tags": []
   },
   "source": [
    "df2.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "07cb3584-6dde-4215-b6f9-f8accdb95f4e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Split the dataset into training and test sets\n",
    "features = df2.loc[:, 'unc_Dic':'lit_risk']\n",
    "X = features\n",
    "y = df2['result']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "280ccb09-a991-4e54-be58-616fa909dab6",
   "metadata": {
    "tags": []
   },
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "results = []\n",
    "\n",
    "# Iterate over each feature and evaluate the linear regression model\n",
    "for feature in features:\n",
    "    # Create the linear regression model\n",
    "    model = sm.OLS(y_train, sm.add_constant(X_train[[feature]]))\n",
    "    results_single = model.fit()\n",
    "\n",
    "    # Predict on the test set\n",
    "    X_test_const = sm.add_constant(X_test[[feature]])\n",
    "    y_pred = results_single.predict(X_test_const)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Get the regression coefficient and p-value\n",
    "    coef = results_single.params[1]\n",
    "    p_value = results_single.pvalues[1]\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({'Feature': feature,'Coefficient': coef, 'P-value': p_value, 'MSE': mse, 'R2 Score': r2})\n",
    "\n",
    "# Create a DataFrame from the results list\n",
    "results_df = pd.DataFrame(results)\n",
    "sorted_results = results_df.sort_values(by=['P-value', 'R2 Score'], ascending=[True, True])\n",
    "print(sorted_results)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f5531768-a3c5-413b-8d81-b02b62165715",
   "metadata": {},
   "source": [
    "弱语气，不稳定风险对期望增量有正向影响显著，强语气反而会弱化期望增量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4e0dd4-2797-454f-bbdf-51e131e7f656",
   "metadata": {},
   "source": [
    "- Index 部分"
   ]
  },
  {
   "cell_type": "code",
   "id": "60cda1cf-d9a2-409d-83c2-12071835b9ec",
   "metadata": {},
   "source": [
    "lexicon = pd.read_csv('../word_list/Future.txt', sep = '\\t', names = ['term', 'category', 'associated'])\n",
    "lexicon.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e517dfd0-65a1-44d4-b1a5-a1f7aa07cdc4",
   "metadata": {},
   "source": [
    "lexicon['term'] = lexicon['term'].str.lower()\n",
    "lexicon.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9a95ea9a-a80a-4a73-948c-248c9644af34",
   "metadata": {},
   "source": [
    "list(lexicon[(lexicon['category'] == 'positive') & (lexicon['associated'] == 1)].term.sample(10))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b3848ad5-cc20-45d2-b6f6-b9f0a605fe73",
   "metadata": {},
   "source": [
    "#重新存储符合条件的词汇\n",
    "future_list = list(lexicon[(lexicon['category'] == 'future') & (lexicon['associated'] == 1)].term)\n",
    "past_list = list(lexicon[(lexicon['category'] == 'past') & (lexicon['associated'] == 1)].term)\n",
    "present_list = list(lexicon[(lexicon['category'] == 'present') & (lexicon['associated'] == 1)].term)\n",
    "\n",
    "positive_list = list(lexicon[(lexicon['category'] == 'positive') & (lexicon['associated'] == 1)].term)\n",
    "negative_list = list(lexicon[(lexicon['category'] == 'negative') & (lexicon['associated'] == 1)].term)\n",
    "\n",
    "print(len(future_list))\n",
    "print(len(past_list))\n",
    "print(len(present_list))\n",
    "print(len(positive_list))\n",
    "print(len(negative_list))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "68bbe439-3f91-4f7d-a887-bf1eb1b1dfad",
   "metadata": {},
   "source": [
    "import re\n",
    "\n",
    "def tense_count(text, tense_list):\n",
    "    f_list = []\n",
    "    for t in text:\n",
    "        f = 0\n",
    "        for w in tense_list:\n",
    "            pattern = w.replace('*', '.*')  # 将*替换为.*\n",
    "            regex = re.compile(pattern)\n",
    "            f += len(regex.findall(t))\n",
    "        f_list.append(f)\n",
    "    return f_list"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b4659ed4-825c-434e-aafb-becc4e9e77f4",
   "metadata": {},
   "source": [
    "df['future_count'] = tense_count(df['item7'], future_list)\n",
    "df['past_count'] = tense_count(df['item7'], past_list)\n",
    "df['present_count'] = tense_count(df['item7'], present_list)\n",
    "\n",
    "df['positive_count'] = tense_count(df['item7'], positive_list)\n",
    "df['negative_count'] = tense_count(df['item7'], negative_list)\n",
    "\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9e676b3e-5dd9-4936-8ab8-005c2778036c",
   "metadata": {},
   "source": [
    "# 计算每个文件的单词总数\n",
    "\n",
    "# 定义一个函数，用于去除标点符号并计算单词总数\n",
    "def count_words_without_punctuation(text):\n",
    "    # 计算单词总数\n",
    "    word_count = len(text.split())\n",
    "    return word_count\n",
    "\n",
    "df['item7'] = df['item7'].astype(str)\n",
    "# 对每个文本计算单词总数\n",
    "df['word_counts'] = df['item7'].apply(count_words_without_punctuation)\n",
    "\n",
    "print(df['word_counts'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "060432d0-3022-4408-bd47-b22df99cfe10",
   "metadata": {},
   "source": [
    "print(df.word_counts[:50])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "457619ec-346f-4125-a61e-599ccd7f70f3",
   "metadata": {},
   "source": [
    "# 计算percent of future words (百分比绝对值，值域在0-100)\n",
    "df.percent_of_future_words = [(100* df.future_count[i] / df.word_counts[i]) for i in range(len(df.word_counts))]\n",
    "print(df.percent_of_future_words[:10])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "58fcdd75-a289-46e6-93dc-7a0ac673ef9e",
   "metadata": {},
   "source": [
    "# 计算percent of past words (百分比绝对值，值域在0-100)\n",
    "df.percent_of_past_words = [(100* df.past_count[i] / df.word_counts[i]) for i in range(len(df.word_counts))]\n",
    "print(df.percent_of_past_words[:10])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b9d9eff5-da09-48de-aaf9-b224833b4f66",
   "metadata": {},
   "source": [
    "# 计算percent of present words (百分比绝对值，值域在0-100)\n",
    "df.percent_of_present_words = [(100* df.present_count[i] / df.word_counts[i]) for i in range(len(df.word_counts))]\n",
    "print(df.percent_of_present_words[:10])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c44b0edc-cdbe-412e-aad7-ae5cdd26e7cb",
   "metadata": {},
   "source": [
    "# 计算percent of positive words (百分比绝对值，值域在0-100)\n",
    "df.percent_of_positive_words = [(100* df.positive_count[i] / df.word_counts[i]) for i in range(len(df.word_counts))]\n",
    "print(df.percent_of_positive_words[:10])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8dd9d79d-2f50-4889-884d-9e05dffc854b",
   "metadata": {},
   "source": [
    "# 计算percent of negative words (百分比绝对值，值域在0-100)\n",
    "df.percent_of_negative_words = [(100* df.negative_count[i] / df.word_counts[i]) for i in range(len(df.word_counts))]\n",
    "print(df.percent_of_negative_words[:10])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "74d1d800-42de-47b1-93ee-8725b3854810",
   "metadata": {},
   "source": [
    "# 计算每一句的Future words VS. Past/Present words index (FvsP) 并存储\n",
    "import math\n",
    "\n",
    "FvsP = []\n",
    "\n",
    "for row in range(len(df)):\n",
    "    FvsP_in_row = math.log ((1 + df.percent_of_future_words[row]) / (1 + df.percent_of_present_words[row] + df.percent_of_past_words[row]))\n",
    "    FvsP.append(FvsP_in_row)  \n",
    "    \n",
    "print(FvsP[:50])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0bfcb1a1-5a60-41cf-985e-f0df14191a5c",
   "metadata": {},
   "source": [
    "# 计算每一句的Positive emotion words VS. Negative emotion words index (PvsN) 并存储\n",
    "\n",
    "PvsN = []\n",
    "\n",
    "for row in range(len(df)):\n",
    "    PvsN_in_row = math.log ((1 + df.percent_of_positive_words[row]) / (1 + df.percent_of_negative_words[row]))\n",
    "    PvsN.append(PvsN_in_row)  \n",
    "    \n",
    "print(PvsN[:50])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e8df4301-ffaa-42ad-97ce-604e0cb88850",
   "metadata": {},
   "source": [
    "# regression-FvsP on return\n",
    "import statsmodels.api as sm\n",
    "\n",
    "df['FvsP'] = FvsP\n",
    "\n",
    "X = df['FvsP']\n",
    "y = df['market_abnormal_return']\n",
    "\n",
    "\n",
    "# Add a constant term to the regression\n",
    "X_wContant = sm.add_constant(X)\n",
    "Model_all_index = sm.OLS(endog=y, exog=X_wContant).fit(maxiter = 5000)\n",
    "\n",
    "print(Model_all_index.summary())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b1504d35-cfe1-4cf6-abe4-99a50b135aa4",
   "metadata": {},
   "source": [
    "# regression-PvsN on return\n",
    "import statsmodels.api as sm\n",
    "\n",
    "df['PvsN'] = PvsN\n",
    "\n",
    "X = df['PvsN']\n",
    "y = df['market_abnormal_return']\n",
    "\n",
    "\n",
    "# Add a constant term to the regression\n",
    "X_wContant = sm.add_constant(X)\n",
    "Model_all_index = sm.OLS(endog=y, exog=X_wContant).fit(maxiter = 5000)\n",
    "\n",
    "print(Model_all_index.summary())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c6ce81a1-5231-4499-8f21-1cb3dee38691",
   "metadata": {},
   "source": [
    "# regression-FvsP + PvsN + control variabels on return\n",
    "import statsmodels.api as sm\n",
    "from itertools import combinations\n",
    "\n",
    "X = df[['FvsP','PvsN']]\n",
    "C = df[['nasdq', 'market_value', 'btm', 'pre_alpha', 'pre_rmse', 'InstOwn_Perc', 'log_share']]\n",
    "y = df['market_abnormal_return']\n",
    "\n",
    "best_model = None\n",
    "best_features = None\n",
    "best_aic = float('inf')\n",
    "\n",
    "# 逐步选择自变量\n",
    "for x in combinations(X, 2):  # 从X中选择2个变量的组合\n",
    "    for c in combinations(C, 7):\n",
    "        # 构建自变量\n",
    "        X = df[list(x)+list(c)]\n",
    "        X = sm.add_constant(X)\n",
    "        # 拟合模型\n",
    "        model = sm.OLS(y, X).fit()\n",
    "        # 计算AIC\n",
    "        aic = model.aic\n",
    "        # 保存最佳模型\n",
    "        if aic < best_aic:\n",
    "            best_model = model\n",
    "            best_features = list(x) + list(c)\n",
    "            best_aic = aic\n",
    "\n",
    "# 输出最佳模型结果\n",
    "if best_model is not None:\n",
    "    print(best_model.summary())\n",
    "    print(\"Best features:\", best_features)\n",
    "else:\n",
    "    print(\"No model found.\")   "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9a5fd897-2def-4753-840d-68b84e2ad5ec",
   "metadata": {},
   "source": [
    "# regression-FvsP on result\n",
    "\n",
    "X = df['FvsP']\n",
    "y = df2['result']\n",
    "\n",
    "\n",
    "# Add a constant term to the regression\n",
    "X_wContant = sm.add_constant(X)\n",
    "Model_all_index = sm.OLS(endog=y, exog=X_wContant).fit(maxiter = 5000)\n",
    "\n",
    "print(Model_all_index.summary())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "82a2827c-9ca2-47cd-b50f-db261d4fe6ec",
   "metadata": {},
   "source": [
    "# regression-PvsN on result\n",
    "\n",
    "X = df['PvsN']\n",
    "y = df2['result']\n",
    "\n",
    "\n",
    "# Add a constant term to the regression\n",
    "X_wContant = sm.add_constant(X)\n",
    "Model_all_index = sm.OLS(endog=y, exog=X_wContant).fit(maxiter = 5000)\n",
    "\n",
    "print(Model_all_index.summary())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2fb84f1a-0b35-443f-9c5c-dbb5859cb815",
   "metadata": {},
   "source": [
    "# regression-FvsP + PvsN + control variabels on result\n",
    "\n",
    "\n",
    "X = df[['FvsP','PvsN']]\n",
    "C = df[['nasdq', 'market_value', 'btm', 'pre_alpha', 'pre_rmse', 'InstOwn_Perc', 'log_share']]\n",
    "y = df2['result']\n",
    "\n",
    "best_model = None\n",
    "best_features = None\n",
    "best_aic = float('inf')\n",
    "\n",
    "# 逐步选择自变量\n",
    "for x in combinations(X, 2):  # 从X中选择2个变量的组合\n",
    "    for c in combinations(C, 7):\n",
    "        # 构建自变量\n",
    "        X = df[list(x)+list(c)]\n",
    "        X = sm.add_constant(X)\n",
    "        # 拟合模型\n",
    "        model = sm.OLS(y, X).fit()\n",
    "        # 计算AIC\n",
    "        aic = model.aic\n",
    "        # 保存最佳模型\n",
    "        if aic < best_aic:\n",
    "            best_model = model\n",
    "            best_features = list(x) + list(c)\n",
    "            best_aic = aic\n",
    "\n",
    "# 输出最佳模型结果\n",
    "if best_model is not None:\n",
    "    print(best_model.summary())\n",
    "    print(\"Best features:\", best_features)\n",
    "else:\n",
    "    print(\"No model found.\")   "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bd033253-60dc-47c1-848b-98b92ea2fd27",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8f57344c-3f35-4a3f-9ad7-3e7649b18a3b",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fca9def7-8a76-48af-85e3-351da514f3f0",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cb4220ab-3e80-43bf-ab50-f86b63f152c3",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
