{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of the laboratory exercise, I will demonstrate the application of the word list method for gauging the sentiment within movie reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Simple Illustration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:37:16.865733Z",
     "start_time": "2024-04-11T08:37:16.849734Z"
    }
   },
   "source": [
    "sentence = \"'Glorious new chapter for Hong Kong swimming': top officials congratulate Siobhan Haughey for Olympic win\"\n",
    "\n",
    "# Convert all text to lower case \n",
    "sentence = sentence.lower()\n",
    "\n",
    "# Remove special characters\n",
    "sentence = sentence.replace(\"'\", '')\n",
    "sentence = sentence.replace(\":\", '')\n",
    "\n",
    "# Simple tokenization\n",
    "words= sentence.split(' ')\n",
    "\n",
    "print(words)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['glorious', 'new', 'chapter', 'for', 'hong', 'kong', 'swimming', 'top', 'officials', 'congratulate', 'siobhan', 'haughey', 'for', 'olympic', 'win']\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:37:16.880732Z",
     "start_time": "2024-04-11T08:37:16.867732Z"
    }
   },
   "source": [
    "positive_words = ['awesome', 'glorious', 'nice', 'super', 'win', 'delightful', 'congratulate']\n",
    "negative_words = ['awful', 'lame', 'horrible', 'bad', 'scare']"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:37:16.895584Z",
     "start_time": "2024-04-11T08:37:16.883615Z"
    }
   },
   "source": [
    "set(words) - set(positive_words)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chapter',\n",
       " 'for',\n",
       " 'haughey',\n",
       " 'hong',\n",
       " 'kong',\n",
       " 'new',\n",
       " 'officials',\n",
       " 'olympic',\n",
       " 'siobhan',\n",
       " 'swimming',\n",
       " 'top'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:37:16.988612Z",
     "start_time": "2024-04-11T08:37:16.969459Z"
    }
   },
   "source": [
    "set(words).intersection(set(positive_words))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'congratulate', 'glorious', 'win'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:37:17.192236Z",
     "start_time": "2024-04-11T08:37:17.177943Z"
    }
   },
   "source": [
    "pos = len(set(words).intersection(set(positive_words))) / len(words)\n",
    "print('Positive sentiment score: {}'.format(pos))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive sentiment score: 0.2\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:37:17.381939Z",
     "start_time": "2024-04-11T08:37:17.367432Z"
    }
   },
   "source": [
    "neg = len(set(words).intersection(set(negative_words))) / len(words)\n",
    "print('Negative sentiment score: {}'.format(neg))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative sentiment score: 0.0\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie Review Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMDB enables users to rate movies on a scale from 1 to 10. In categorizing these reviews, the data curator assigned a rating of 4 stars or less to signify a negative reaction, **indicating a poor reception or unfavorable view of the movie**. Conversely, ratings of 7 stars or higher were marked as positive, **reflecting a favorable reaction or approval of the film**. Reviews with ratings of 5 or 6 stars were omitted from this classification scheme. These assigned labels will act as the reference standard for subsequent comparisons."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:37:17.537556Z",
     "start_time": "2024-04-11T08:37:17.527801Z"
    }
   },
   "source": [
    "# pip install nltk"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:37:19.068879Z",
     "start_time": "2024-04-11T08:37:17.632515Z"
    }
   },
   "source": [
    "# Import useful libraries used for data management\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:37:19.806100Z",
     "start_time": "2024-04-11T08:37:19.070880Z"
    }
   },
   "source": [
    "dataset = pd.read_csv('IMDB.csv')\n",
    "dataset"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:37:19.821206Z",
     "start_time": "2024-04-11T08:37:19.808341Z"
    }
   },
   "source": [
    "dataset['sentiment'].value_counts()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    25000\n",
       "negative    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:37:19.852674Z",
     "start_time": "2024-04-11T08:37:19.824364Z"
    }
   },
   "source": [
    "# Sample 1000 reviews from each sentiment\n",
    "dataset = dataset.groupby('sentiment').apply(lambda x: x.sample(1000)).reset_index(drop = True)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\AppData\\Local\\Temp\\ipykernel_14352\\3500470110.py:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  dataset = dataset.groupby('sentiment').apply(lambda x: x.sample(1000)).reset_index(drop = True)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:37:19.868109Z",
     "start_time": "2024-04-11T08:37:19.855039Z"
    }
   },
   "source": [
    "# Convert the 'label' column into a numeric variable; 'negative' as 0, 'positive' as 1\n",
    "dataset['label'] = dataset['sentiment'].map({'negative':0, 'positive':1})\n",
    "dataset"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                 review sentiment  label\n",
       "0     This is one of the worst anime series I have e...  negative      0\n",
       "1     Black and White film. Good photography. Believ...  negative      0\n",
       "2     I might have given this movie a higher rating ...  negative      0\n",
       "3     I don't really know where to start. The acting...  negative      0\n",
       "4     Bad acting? Yes, but it was not a surprise. St...  negative      0\n",
       "...                                                 ...       ...    ...\n",
       "1995  I saw this in the early 70s (when I was 16), a...  positive      1\n",
       "1996  This is a great example of a rather simple Fil...  positive      1\n",
       "1997  Opening the film with a Bach Toccata is an aur...  positive      1\n",
       "1998  I am a huge Rupert Everett fan. I adore Kathy ...  positive      1\n",
       "1999  Perhaps I'm one of the only avid horror fans w...  positive      1\n",
       "\n",
       "[2000 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is one of the worst anime series I have e...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Black and White film. Good photography. Believ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I might have given this movie a higher rating ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I don't really know where to start. The acting...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bad acting? Yes, but it was not a surprise. St...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>I saw this in the early 70s (when I was 16), a...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>This is a great example of a rather simple Fil...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>Opening the film with a Bach Toccata is an aur...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>I am a huge Rupert Everett fan. I adore Kathy ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>Perhaps I'm one of the only avid horror fans w...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:37:20.459652Z",
     "start_time": "2024-04-11T08:37:19.869108Z"
    }
   },
   "source": [
    "# Convert to list\n",
    "review = dataset.review.values.tolist()\n",
    "\n",
    "# Remove all html tags\n",
    "review = [re.sub(\"<.*?>\", \" \", i) for i in review]\n",
    "\n",
    "# Remove unnecessary characters\n",
    "review = [re.sub(\"[^A-Za-z0-9]+\", \" \", i) for i in review]\n",
    "\n",
    "# Change to lower case\n",
    "review = [i.lower() for i in review]\n",
    "\n",
    "# Define functions for stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in doc.split(' ') if word not in stop_words] for doc in texts]\n",
    "\n",
    "# Remove Stop Words\n",
    "review = remove_stopwords(review)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\David\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:37:20.475658Z",
     "start_time": "2024-04-11T08:37:20.463647Z"
    }
   },
   "source": [
    "print(dataset.review[0])\n",
    "print('\\n')\n",
    "print(review[0])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is one of the worst anime series I have ever seen. When I watched the manga review in a magazine, I thought it was maybe interesting, but when I got the chapters I realized it was a complete stupidity.<br /><br />OK, the first 2 or 3 chapters are OK, and the series have an standard. But as the plot advances, it becomes totally incoherent. The series tries to show some mystical based upon the Christian mythologies, but it's a total stupidity. It features some demons and stigmata scenes... Totally nonsense. It seems the series tries to seem deeply-thought, complex or mythologically reviewed, but a watcher with a bit of brain and cultural references, will realize soon all those elements used don't have a real sense: THEY ARE PUT THERE ONLY TO IMPRESS THE IGNORANT WATCHERS!!<br /><br />The final chapters are full of totally nonsense elements: battles with cat-eared demons, references to a supposed fight between demons, and demons who controls time (with no apparent reason). The final is totally nonsense; an ignorant watcher will see on it a floating final that gives them a place to meditate; but the truth is this: THE FINAL AND THE COMPLETE SERIES IS A TOTAL INCOHERENT AND INCONGRUENT NONSENSE.\n",
      "\n",
      "\n",
      "['one', 'worst', 'anime', 'series', 'ever', 'seen', 'watched', 'manga', 'review', 'magazine', 'thought', 'maybe', 'interesting', 'got', 'chapters', 'realized', 'complete', 'stupidity', 'ok', 'first', '2', '3', 'chapters', 'ok', 'series', 'standard', 'plot', 'advances', 'becomes', 'totally', 'incoherent', 'series', 'tries', 'show', 'mystical', 'based', 'upon', 'christian', 'mythologies', 'total', 'stupidity', 'features', 'demons', 'stigmata', 'scenes', 'totally', 'nonsense', 'seems', 'series', 'tries', 'seem', 'deeply', 'thought', 'complex', 'mythologically', 'reviewed', 'watcher', 'bit', 'brain', 'cultural', 'references', 'realize', 'soon', 'elements', 'used', 'real', 'sense', 'put', 'impress', 'ignorant', 'watchers', 'final', 'chapters', 'full', 'totally', 'nonsense', 'elements', 'battles', 'cat', 'eared', 'demons', 'references', 'supposed', 'fight', 'demons', 'demons', 'controls', 'time', 'apparent', 'reason', 'final', 'totally', 'nonsense', 'ignorant', 'watcher', 'see', 'floating', 'final', 'gives', 'place', 'meditate', 'truth', 'final', 'complete', 'series', 'total', 'incoherent', 'incongruent', 'nonsense', '']\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load External Emotion Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our analysis will incorporate the NRC Word-Emotion Association Lexicon, which encompasses associations of English words with eight fundamental emotions — anger, fear, anticipation, trust, surprise, sadness, joy, and disgust — as well as two overarching sentiments, namely negative and positive.\n",
    "\n",
    "For more information, please visit: https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm#:~:text=The%20NRC%20Emotion%20Lexicon%20is,sentiments%20(negative%20and%20positive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "   <img src=\"img/summary.png\" width=\"1200\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:37:20.600963Z",
     "start_time": "2024-04-11T08:37:20.476652Z"
    }
   },
   "source": [
    "lexicon = pd.read_csv('NRC-Emotion-Lexicon.txt')\n",
    "lexicon.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          aback\\tanger\\t0\n",
       "0  aback\\tanticipation\\t0\n",
       "1       aback\\tdisgust\\t0\n",
       "2          aback\\tfear\\t0\n",
       "3           aback\\tjoy\\t0\n",
       "4      aback\\tnegative\\t0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aback\\tanger\\t0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aback\\tanticipation\\t0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aback\\tdisgust\\t0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aback\\tfear\\t0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aback\\tjoy\\t0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aback\\tnegative\\t0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the file is in **.txt**, we need to change the delimiter from **comma** (',') to **tab** ('\\t'). Besides, we need to manually set the column name as the file does not define the columns."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:37:20.695215Z",
     "start_time": "2024-04-11T08:37:20.602963Z"
    }
   },
   "source": [
    "lexicon = pd.read_csv('NRC-Emotion-Lexicon.txt', sep = '\\t', names = ['term', 'category', 'associated'])\n",
    "lexicon.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    term      category  associated\n",
       "0  aback         anger           0\n",
       "1  aback  anticipation           0\n",
       "2  aback       disgust           0\n",
       "3  aback          fear           0\n",
       "4  aback           joy           0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>category</th>\n",
       "      <th>associated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aback</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aback</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aback</td>\n",
       "      <td>disgust</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aback</td>\n",
       "      <td>fear</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aback</td>\n",
       "      <td>joy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Sample 10 Positive Words"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:37:20.727097Z",
     "start_time": "2024-04-11T08:37:20.699506Z"
    }
   },
   "source": [
    "(lexicon['category'] == 'positive')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         False\n",
       "1         False\n",
       "2         False\n",
       "3         False\n",
       "4         False\n",
       "          ...  \n",
       "141565    False\n",
       "141566     True\n",
       "141567    False\n",
       "141568    False\n",
       "141569    False\n",
       "Name: category, Length: 141570, dtype: bool"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:37:20.742758Z",
     "start_time": "2024-04-11T08:37:20.729099Z"
    }
   },
   "source": [
    "(lexicon['associated'] == 1)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         False\n",
       "1         False\n",
       "2         False\n",
       "3         False\n",
       "4         False\n",
       "          ...  \n",
       "141565    False\n",
       "141566    False\n",
       "141567    False\n",
       "141568    False\n",
       "141569    False\n",
       "Name: associated, Length: 141570, dtype: bool"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:37:20.773501Z",
     "start_time": "2024-04-11T08:37:20.744774Z"
    }
   },
   "source": [
    "lexicon[(lexicon['category'] == 'positive') & (lexicon['associated'] == 1)]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                  term  category  associated\n",
       "76                abba  positive           1\n",
       "206            ability  positive           1\n",
       "366     abovementioned  positive           1\n",
       "486           absolute  positive           1\n",
       "496         absolution  positive           1\n",
       "...                ...       ...         ...\n",
       "141216        yearning  positive           1\n",
       "141396           youth  positive           1\n",
       "141426            zeal  positive           1\n",
       "141446         zealous  positive           1\n",
       "141496            zest  positive           1\n",
       "\n",
       "[2308 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>category</th>\n",
       "      <th>associated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>abba</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>ability</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>abovementioned</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>absolute</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>absolution</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141216</th>\n",
       "      <td>yearning</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141396</th>\n",
       "      <td>youth</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141426</th>\n",
       "      <td>zeal</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141446</th>\n",
       "      <td>zealous</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141496</th>\n",
       "      <td>zest</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2308 rows × 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:37:20.805266Z",
     "start_time": "2024-04-11T08:37:20.776888Z"
    }
   },
   "source": [
    "list(lexicon[(lexicon['category'] == 'positive') & (lexicon['associated'] == 1)].term.sample(10))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brilliant',\n",
       " 'balanced',\n",
       " 'equity',\n",
       " 'boldness',\n",
       " 'cleanse',\n",
       " 'inspired',\n",
       " 'completion',\n",
       " 'adept',\n",
       " 'satisfactorily',\n",
       " 'improved']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Sample 10 Negative Words"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:37:20.835860Z",
     "start_time": "2024-04-11T08:37:20.807849Z"
    }
   },
   "source": [
    "list(lexicon[(lexicon['category'] == 'negative') & (lexicon['associated'] == 1)].term.sample(10))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['denied',\n",
       " 'shady',\n",
       " 'sponge',\n",
       " 'unpredictable',\n",
       " 'mumble',\n",
       " 'sarcoma',\n",
       " 'obligor',\n",
       " 'oversight',\n",
       " 'blister',\n",
       " 'slaughtering']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Sentiment of Movie Reviews"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:37:20.867198Z",
     "start_time": "2024-04-11T08:37:20.837861Z"
    }
   },
   "source": [
    "pos_list = list(lexicon[(lexicon['category'] == 'positive') & (lexicon['associated'] == 1)].term)\n",
    "neg_list = list(lexicon[(lexicon['category'] == 'negative') & (lexicon['associated'] == 1)].term)"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:37:20.883198Z",
     "start_time": "2024-04-11T08:37:20.869199Z"
    }
   },
   "source": [
    "print(len(pos_list))\n",
    "print(len(neg_list))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2308\n",
      "3318\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:37:20.899199Z",
     "start_time": "2024-04-11T08:37:20.885199Z"
    }
   },
   "source": [
    "# A Function to Construct a Sentiment Variable Using a Lexicon-Based Approach\n",
    "\n",
    "def sentiment_score(text, sen_list):\n",
    "    temp_list = []\n",
    "    for t in text:\n",
    "        temp = 0\n",
    "        for w in sen_list:\n",
    "            temp += t.count(w)\n",
    "        temp_list.append(temp/len(t))\n",
    "    return temp_list"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T08:37:47.479202Z",
     "start_time": "2024-04-11T08:37:47.466319Z"
    }
   },
   "cell_type": "code",
   "source": "review[0]",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one',\n",
       " 'worst',\n",
       " 'anime',\n",
       " 'series',\n",
       " 'ever',\n",
       " 'seen',\n",
       " 'watched',\n",
       " 'manga',\n",
       " 'review',\n",
       " 'magazine',\n",
       " 'thought',\n",
       " 'maybe',\n",
       " 'interesting',\n",
       " 'got',\n",
       " 'chapters',\n",
       " 'realized',\n",
       " 'complete',\n",
       " 'stupidity',\n",
       " 'ok',\n",
       " 'first',\n",
       " '2',\n",
       " '3',\n",
       " 'chapters',\n",
       " 'ok',\n",
       " 'series',\n",
       " 'standard',\n",
       " 'plot',\n",
       " 'advances',\n",
       " 'becomes',\n",
       " 'totally',\n",
       " 'incoherent',\n",
       " 'series',\n",
       " 'tries',\n",
       " 'show',\n",
       " 'mystical',\n",
       " 'based',\n",
       " 'upon',\n",
       " 'christian',\n",
       " 'mythologies',\n",
       " 'total',\n",
       " 'stupidity',\n",
       " 'features',\n",
       " 'demons',\n",
       " 'stigmata',\n",
       " 'scenes',\n",
       " 'totally',\n",
       " 'nonsense',\n",
       " 'seems',\n",
       " 'series',\n",
       " 'tries',\n",
       " 'seem',\n",
       " 'deeply',\n",
       " 'thought',\n",
       " 'complex',\n",
       " 'mythologically',\n",
       " 'reviewed',\n",
       " 'watcher',\n",
       " 'bit',\n",
       " 'brain',\n",
       " 'cultural',\n",
       " 'references',\n",
       " 'realize',\n",
       " 'soon',\n",
       " 'elements',\n",
       " 'used',\n",
       " 'real',\n",
       " 'sense',\n",
       " 'put',\n",
       " 'impress',\n",
       " 'ignorant',\n",
       " 'watchers',\n",
       " 'final',\n",
       " 'chapters',\n",
       " 'full',\n",
       " 'totally',\n",
       " 'nonsense',\n",
       " 'elements',\n",
       " 'battles',\n",
       " 'cat',\n",
       " 'eared',\n",
       " 'demons',\n",
       " 'references',\n",
       " 'supposed',\n",
       " 'fight',\n",
       " 'demons',\n",
       " 'demons',\n",
       " 'controls',\n",
       " 'time',\n",
       " 'apparent',\n",
       " 'reason',\n",
       " 'final',\n",
       " 'totally',\n",
       " 'nonsense',\n",
       " 'ignorant',\n",
       " 'watcher',\n",
       " 'see',\n",
       " 'floating',\n",
       " 'final',\n",
       " 'gives',\n",
       " 'place',\n",
       " 'meditate',\n",
       " 'truth',\n",
       " 'final',\n",
       " 'complete',\n",
       " 'series',\n",
       " 'total',\n",
       " 'incoherent',\n",
       " 'incongruent',\n",
       " 'nonsense',\n",
       " '']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T08:59:15.020480Z",
     "start_time": "2024-04-03T08:59:06.265737Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                              review sentiment  label  \\\n0  I remember when \"The Love Machine\" was first r...  negative      0   \n1  Nothing more than a soccer knock-off of The Mi...  negative      0   \n2  Academy Awarding actor Sidney Poitier of \"Lili...  negative      0   \n3  I completely disagree with the previous review...  negative      0   \n4  I didn't want to write this movie off on the r...  negative      0   \n\n    Pos_Dic   Neg_Dic  Sentiment_Dic  \n0  0.078853  0.107527      -0.028674  \n1  0.116279  0.023256       0.093023  \n2  0.096774  0.068311       0.028463  \n3  0.139535  0.139535       0.000000  \n4  0.108527  0.077519       0.031008  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n      <th>label</th>\n      <th>Pos_Dic</th>\n      <th>Neg_Dic</th>\n      <th>Sentiment_Dic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I remember when \"The Love Machine\" was first r...</td>\n      <td>negative</td>\n      <td>0</td>\n      <td>0.078853</td>\n      <td>0.107527</td>\n      <td>-0.028674</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Nothing more than a soccer knock-off of The Mi...</td>\n      <td>negative</td>\n      <td>0</td>\n      <td>0.116279</td>\n      <td>0.023256</td>\n      <td>0.093023</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Academy Awarding actor Sidney Poitier of \"Lili...</td>\n      <td>negative</td>\n      <td>0</td>\n      <td>0.096774</td>\n      <td>0.068311</td>\n      <td>0.028463</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I completely disagree with the previous review...</td>\n      <td>negative</td>\n      <td>0</td>\n      <td>0.139535</td>\n      <td>0.139535</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I didn't want to write this movie off on the r...</td>\n      <td>negative</td>\n      <td>0</td>\n      <td>0.108527</td>\n      <td>0.077519</td>\n      <td>0.031008</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Pos_Dic'] = sentiment_score(review, pos_list)\n",
    "dataset['Neg_Dic'] = sentiment_score(review, neg_list)\n",
    "\n",
    "# Calculate polarity = positive - negative\n",
    "dataset['Sentiment_Dic'] = dataset['Pos_Dic'] - dataset['Neg_Dic']\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T08:59:22.440505Z",
     "start_time": "2024-04-03T08:59:22.430538Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                  label  Sentiment_Dic\nlabel          1.000000       0.435127\nSentiment_Dic  0.435127       1.000000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>Sentiment_Dic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>label</th>\n      <td>1.000000</td>\n      <td>0.435127</td>\n    </tr>\n    <tr>\n      <th>Sentiment_Dic</th>\n      <td>0.435127</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[['label', 'Sentiment_Dic']].corr(method ='spearman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "   <img src=\"img/example.png\" width=\"600\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Text Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second part of the laboratory exercise, I will demonstrate the application of various regression models to identify confirmed instances of COVID-19-related misinformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake News Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset on COVID-19 misinformation was sourced from a repository known as \"ReCOVery,\" as documented by Zhou et al. (2020). This dataset comprises news items specifically related to COVID-19, which have been corroborated through two independent verification platforms: NewsGuard and Media Bias/Fact Check.\n",
    "\n",
    "The identification of pertinent news articles was conducted using a predefined set of keywords including **\"SARS-CoV-2,\"** **\"COVID-19,\"** and **\"Coronavirus.\"**\n",
    "\n",
    "For comprehensive information, please refer to the following document: https://arxiv.org/pdf/2006.05557.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-03T08:59:48.913409Z",
     "start_time": "2024-04-03T08:59:41.233183Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Obtaining dependency information for spacy from https://files.pythonhosted.org/packages/7a/f9/85ad3071616e5abb738e229aa1fa728e26241605274a862251675ee35e27/spacy-3.7.4-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading spacy-3.7.4-cp310-cp310-win_amd64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Obtaining dependency information for spacy-legacy<3.1.0,>=3.0.11 from https://files.pythonhosted.org/packages/c3/55/12e842c70ff8828e34e543a2c7176dac4da006ca6901c9e8b43efab8bc6b/spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Obtaining dependency information for spacy-loggers<2.0.0,>=1.0.0 from https://files.pythonhosted.org/packages/33/78/d1a1a026ef3af911159398c939b1509d5c36fe524c7b644f34a5146c4e16/spacy_loggers-1.0.5-py3-none-any.whl.metadata\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Obtaining dependency information for murmurhash<1.1.0,>=0.28.0 from https://files.pythonhosted.org/packages/ed/9d/d62d12e3ecc6f99eddea6289413669a905d2ebb15cf9fe075336ca6cceaa/murmurhash-1.0.10-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading murmurhash-1.0.10-cp310-cp310-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Obtaining dependency information for cymem<2.1.0,>=2.0.2 from https://files.pythonhosted.org/packages/51/12/4aa9eec680c6d12b2275d479e159c3d063d7c757175063dd45386e15b39d/cymem-2.0.8-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading cymem-2.0.8-cp310-cp310-win_amd64.whl.metadata (8.6 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Obtaining dependency information for preshed<3.1.0,>=3.0.2 from https://files.pythonhosted.org/packages/f3/72/108426ca3b6e7f16db30b3b9396e3fa45a3fd5a76f6532ab04beada2e4e3/preshed-3.0.9-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading preshed-3.0.9-cp310-cp310-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy)\n",
      "  Obtaining dependency information for thinc<8.3.0,>=8.2.2 from https://files.pythonhosted.org/packages/aa/39/7e2aa8b46b430c4d19676a4ff9f36b329788723c05e6187539daca209da2/thinc-8.2.3-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading thinc-8.2.3-cp310-cp310-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Obtaining dependency information for wasabi<1.2.0,>=0.9.1 from https://files.pythonhosted.org/packages/8f/69/26cbf0bad11703241cb84d5324d868097f7a8faf2f1888354dac8883f3fc/wasabi-1.1.2-py3-none-any.whl.metadata\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Obtaining dependency information for srsly<3.0.0,>=2.4.3 from https://files.pythonhosted.org/packages/0a/ed/d2c37221fe1975f4b6e8e3cf200d25b905b77e18f6a660b3dc149ade6192/srsly-2.4.8-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading srsly-2.4.8-cp310-cp310-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Obtaining dependency information for catalogue<2.1.0,>=2.0.6 from https://files.pythonhosted.org/packages/9e/96/d32b941a501ab566a16358d68b6eb4e4acc373fab3c3c4d7d9e649f7b4bb/catalogue-2.0.10-py3-none-any.whl.metadata\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.4.0,>=0.1.0 (from spacy)\n",
      "  Obtaining dependency information for weasel<0.4.0,>=0.1.0 from https://files.pythonhosted.org/packages/d5/e5/b63b8e255d89ba4155972990d42523251d4d1368c4906c646597f63870e2/weasel-0.3.4-py3-none-any.whl.metadata\n",
      "  Downloading weasel-0.3.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0 (from spacy)\n",
      "  Obtaining dependency information for typer<0.10.0,>=0.3.0 from https://files.pythonhosted.org/packages/62/39/82c9d3e10979851847361d922a373bdfef4091020da7f893acfaf07c0225/typer-0.9.4-py3-none-any.whl.metadata\n",
      "  Downloading typer-0.9.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<7.0.0,>=5.2.1 (from spacy)\n",
      "  Obtaining dependency information for smart-open<7.0.0,>=5.2.1 from https://files.pythonhosted.org/packages/fc/d9/d97f1db64b09278aba64e8c81b5d322d436132df5741c518f3823824fae0/smart_open-6.4.0-py3-none-any.whl.metadata\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\ca.ca-pc\\ideaprojects\\japjc_export\\mm5427\\lib\\site-packages (from spacy) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\ca.ca-pc\\ideaprojects\\japjc_export\\mm5427\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Obtaining dependency information for pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 from https://files.pythonhosted.org/packages/e5/f3/8296f550276194a58c5500d55b19a27ae0a5a3a51ffef66710c58544b32d/pydantic-2.6.4-py3-none-any.whl.metadata\n",
      "  Downloading pydantic-2.6.4-py3-none-any.whl.metadata (85 kB)\n",
      "     ---------------------------------------- 0.0/85.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 85.1/85.1 kB ? eta 0:00:00\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ca.ca-pc\\ideaprojects\\japjc_export\\mm5427\\lib\\site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ca.ca-pc\\ideaprojects\\japjc_export\\mm5427\\lib\\site-packages (from spacy) (68.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ca.ca-pc\\ideaprojects\\japjc_export\\mm5427\\lib\\site-packages (from spacy) (24.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Obtaining dependency information for langcodes<4.0.0,>=3.2.0 from https://files.pythonhosted.org/packages/fe/c3/0d04d248624a181e57c2870127dfa8d371973561caf54333c85e8f9133a2/langcodes-3.3.0-py3-none-any.whl.metadata\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\ca.ca-pc\\ideaprojects\\japjc_export\\mm5427\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Obtaining dependency information for annotated-types>=0.4.0 from https://files.pythonhosted.org/packages/28/78/d31230046e58c207284c6b2c4e8d96e6d3cb4e52354721b944d3e1ee4aa5/annotated_types-0.6.0-py3-none-any.whl.metadata\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.16.3 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Obtaining dependency information for pydantic-core==2.16.3 from https://files.pythonhosted.org/packages/ec/e8/49d65816802781451af7e758bdf9ff9d976a6b3959e1aab843da9931e89f/pydantic_core-2.16.3-cp310-none-win_amd64.whl.metadata\n",
      "  Downloading pydantic_core-2.16.3-cp310-none-win_amd64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\ca.ca-pc\\ideaprojects\\japjc_export\\mm5427\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ca.ca-pc\\ideaprojects\\japjc_export\\mm5427\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ca.ca-pc\\ideaprojects\\japjc_export\\mm5427\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ca.ca-pc\\ideaprojects\\japjc_export\\mm5427\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ca.ca-pc\\ideaprojects\\japjc_export\\mm5427\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Obtaining dependency information for blis<0.8.0,>=0.7.8 from https://files.pythonhosted.org/packages/ad/65/d9fd07e11499e0a3162c6d61ae430172125e5c340c89c40504189d5299b9/blis-0.7.11-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading blis-0.7.11-cp310-cp310-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Obtaining dependency information for confection<1.0.0,>=0.0.1 from https://files.pythonhosted.org/packages/39/78/f9d18da7b979a2e6007bfcea2f3c8cc02ed210538ae1ce7e69092aed7b18/confection-0.1.4-py3-none-any.whl.metadata\n",
      "  Downloading confection-0.1.4-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\ca.ca-pc\\ideaprojects\\japjc_export\\mm5427\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\ca.ca-pc\\ideaprojects\\japjc_export\\mm5427\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\n",
      "  Obtaining dependency information for cloudpathlib<0.17.0,>=0.7.0 from https://files.pythonhosted.org/packages/0f/6e/45b57a7d4573d85d0b0a39d99673dc1f5eea9d92a1a4603b35e968fbf89a/cloudpathlib-0.16.0-py3-none-any.whl.metadata\n",
      "  Downloading cloudpathlib-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ca.ca-pc\\ideaprojects\\japjc_export\\mm5427\\lib\\site-packages (from jinja2->spacy) (2.1.5)\n",
      "Downloading spacy-3.7.4-cp310-cp310-win_amd64.whl (12.1 MB)\n",
      "   ---------------------------------------- 0.0/12.1 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.3/12.1 MB 28.1 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 4.0/12.1 MB 42.0 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.9/12.1 MB 49.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.3/12.1 MB 54.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.1 MB 65.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.1/12.1 MB 54.7 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.8-cp310-cp310-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "   ---------------------------------------- 0.0/181.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 181.6/181.6 kB 10.7 MB/s eta 0:00:00\n",
      "Downloading murmurhash-1.0.10-cp310-cp310-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp310-cp310-win_amd64.whl (122 kB)\n",
      "   ---------------------------------------- 0.0/122.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 122.2/122.2 kB ? eta 0:00:00\n",
      "Downloading pydantic-2.6.4-py3-none-any.whl (394 kB)\n",
      "   ---------------------------------------- 0.0/394.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 394.9/394.9 kB 24.0 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.16.3-cp310-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.9/1.9 MB 60.4 MB/s eta 0:00:00\n",
      "Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "   ---------------------------------------- 0.0/57.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 57.0/57.0 kB ? eta 0:00:00\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp310-cp310-win_amd64.whl (481 kB)\n",
      "   ---------------------------------------- 0.0/481.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 481.9/481.9 kB ? eta 0:00:00\n",
      "Downloading thinc-8.2.3-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 47.5 MB/s eta 0:00:00\n",
      "Downloading typer-0.9.4-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/46.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 46.0/46.0 kB ? eta 0:00:00\n",
      "Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.1/50.1 kB ? eta 0:00:00\n",
      "Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading blis-0.7.11-cp310-cp310-win_amd64.whl (6.6 MB)\n",
      "   ---------------------------------------- 0.0/6.6 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 2.7/6.6 MB 58.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.2/6.6 MB 66.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.6/6.6 MB 60.6 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/45.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 45.0/45.0 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, smart-open, pydantic-core, murmurhash, langcodes, cloudpathlib, catalogue, blis, annotated-types, typer, srsly, pydantic, preshed, confection, weasel, thinc, spacy\n",
      "Successfully installed annotated-types-0.6.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 confection-0.1.4 cymem-2.0.8 langcodes-3.3.0 murmurhash-1.0.10 preshed-3.0.9 pydantic-2.6.4 pydantic-core-2.16.3 smart-open-6.4.0 spacy-3.7.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.3 typer-0.9.4 wasabi-1.1.2 weasel-0.3.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T09:04:10.360927Z",
     "start_time": "2024-04-03T09:04:07.700426Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\CA.CA-\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from spacy.lang.en import English\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T09:01:05.817372Z",
     "start_time": "2024-04-03T09:01:05.694784Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   news_id                                                url  \\\n0        0  https://www.nytimes.com/article/what-is-corona...   \n1        1  https://www.npr.org/2020/01/22/798392172/chine...   \n2        2  https://www.theverge.com/2020/1/23/21078457/co...   \n3        3  https://www.worldhealth.net/news/novel-coronav...   \n4        4  https://www.theverge.com/2020/1/24/21080845/co...   \n\n                     publisher publish_date  \\\n0           The New York Times   2020-01-21   \n1  National Public Radio (NPR)   2020-01-22   \n2                    The Verge   2020-01-23   \n3              WorldHealth.Net   2020-01-24   \n4                    The Verge   2020-01-24   \n\n                                              author  \\\n0               ['Knvul Sheikh', 'Roni Caryn Rabin']   \n1                                     ['Emily Feng']   \n2                                 ['Nicole Wetsman']   \n3                                                 []   \n4  ['Nicole Wetsman', 'Zoe Schiffer', 'Jay Peters...   \n\n                                               title  \\\n0  The Coronavirus: What Scientists Have Learned ...   \n1  Chinese Health Officials: More Die From Newly ...   \n2  Everything you need to know about the coronavirus   \n3  Novel Coronavirus Cases Confirmed To Be Spreading   \n4  Coronavirus disrupts the world: updates on the...   \n\n                                               image  \\\n0  https://static01.nyt.com/images/2020/03/12/sci...   \n1  https://media.npr.org/include/images/facebook-...   \n2  https://cdn.vox-cdn.com/thumbor/a9_Oz7cvSBKyal...   \n3  https://www.worldhealth.net/media/original_ima...   \n4  https://cdn.vox-cdn.com/thumbor/t2gt1SmEni4Mcr...   \n\n                                           body_text  news_guard_score  \\\n0  \\r\\nA novel respiratory virus that originated ...             100.0   \n1  Chinese Health Officials: More Die From Newly ...             100.0   \n2  Public health experts around the globe are scr...             100.0   \n3  The first two coronavirus cases in Europe have...              30.0   \n4  A new coronavirus appeared in Wuhan, China, at...             100.0   \n\n  mbfc_level political_bias country  reliability  \n0       High           Left     USA            1  \n1  Very high         Center     USA            1  \n2       High    Left-center     USA            1  \n3        Low            NaN     USA            0  \n4       High    Left-center     USA            1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>news_id</th>\n      <th>url</th>\n      <th>publisher</th>\n      <th>publish_date</th>\n      <th>author</th>\n      <th>title</th>\n      <th>image</th>\n      <th>body_text</th>\n      <th>news_guard_score</th>\n      <th>mbfc_level</th>\n      <th>political_bias</th>\n      <th>country</th>\n      <th>reliability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>https://www.nytimes.com/article/what-is-corona...</td>\n      <td>The New York Times</td>\n      <td>2020-01-21</td>\n      <td>['Knvul Sheikh', 'Roni Caryn Rabin']</td>\n      <td>The Coronavirus: What Scientists Have Learned ...</td>\n      <td>https://static01.nyt.com/images/2020/03/12/sci...</td>\n      <td>\\r\\nA novel respiratory virus that originated ...</td>\n      <td>100.0</td>\n      <td>High</td>\n      <td>Left</td>\n      <td>USA</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>https://www.npr.org/2020/01/22/798392172/chine...</td>\n      <td>National Public Radio (NPR)</td>\n      <td>2020-01-22</td>\n      <td>['Emily Feng']</td>\n      <td>Chinese Health Officials: More Die From Newly ...</td>\n      <td>https://media.npr.org/include/images/facebook-...</td>\n      <td>Chinese Health Officials: More Die From Newly ...</td>\n      <td>100.0</td>\n      <td>Very high</td>\n      <td>Center</td>\n      <td>USA</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>https://www.theverge.com/2020/1/23/21078457/co...</td>\n      <td>The Verge</td>\n      <td>2020-01-23</td>\n      <td>['Nicole Wetsman']</td>\n      <td>Everything you need to know about the coronavirus</td>\n      <td>https://cdn.vox-cdn.com/thumbor/a9_Oz7cvSBKyal...</td>\n      <td>Public health experts around the globe are scr...</td>\n      <td>100.0</td>\n      <td>High</td>\n      <td>Left-center</td>\n      <td>USA</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>https://www.worldhealth.net/news/novel-coronav...</td>\n      <td>WorldHealth.Net</td>\n      <td>2020-01-24</td>\n      <td>[]</td>\n      <td>Novel Coronavirus Cases Confirmed To Be Spreading</td>\n      <td>https://www.worldhealth.net/media/original_ima...</td>\n      <td>The first two coronavirus cases in Europe have...</td>\n      <td>30.0</td>\n      <td>Low</td>\n      <td>NaN</td>\n      <td>USA</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>https://www.theverge.com/2020/1/24/21080845/co...</td>\n      <td>The Verge</td>\n      <td>2020-01-24</td>\n      <td>['Nicole Wetsman', 'Zoe Schiffer', 'Jay Peters...</td>\n      <td>Coronavirus disrupts the world: updates on the...</td>\n      <td>https://cdn.vox-cdn.com/thumbor/t2gt1SmEni4Mcr...</td>\n      <td>A new coronavirus appeared in Wuhan, China, at...</td>\n      <td>100.0</td>\n      <td>High</td>\n      <td>Left-center</td>\n      <td>USA</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fakeNews = pd.read_csv(\"fake_news.csv\")\n",
    "fakeNews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T09:06:40.838143Z",
     "start_time": "2024-04-03T09:04:20.237145Z"
    }
   },
   "outputs": [],
   "source": [
    "# Simple preprocessing by removing extra lines and lowercase all text\n",
    "fakeNews['body_text'] = fakeNews['body_text'].replace('\\n','', regex=True)\n",
    "fakeNews['body_text'] = fakeNews['body_text'].replace('\\r','', regex=True)\n",
    "fakeNews['body_text'] = [x.lower() for x in fakeNews['body_text']]\n",
    "\n",
    "\n",
    "# Further preprocessing by removing all stopwords and lemmatizing all text\n",
    "documents = []\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for text in fakeNews['body_text']:\n",
    "    # Load English tokenizer, tagger, parser, NER and word vectors\n",
    "    nlp = English()\n",
    "\n",
    "    #  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "    my_doc = nlp(text)\n",
    "\n",
    "    # Create list of word tokens\n",
    "    token_list = []\n",
    "    for token in my_doc:\n",
    "        token_list.append(token.text)\n",
    "\n",
    "    # Create list of word tokens after removing stopwords\n",
    "    filtered_sentence =[] \n",
    "\n",
    "    for word in token_list:\n",
    "        lexeme = nlp.vocab[word]\n",
    "        if not lexeme.is_stop:\n",
    "            filtered_sentence.append(word) \n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in filtered_sentence]\n",
    "    document = ' '.join(document)\n",
    "\n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T09:08:11.561816Z",
     "start_time": "2024-04-03T09:08:11.537896Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   fake                                  body_text_process\n0     0  novel respiratory virus originated wuhan , chi...\n1     0  chinese health official : die newly identified...\n2     0  public health expert globe scrambling understa...\n3     1  coronavirus case europe detected france , seco...\n4     0  new coronavirus appeared wuhan , china , start...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fake</th>\n      <th>body_text_process</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>novel respiratory virus originated wuhan , chi...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>chinese health official : die newly identified...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>public health expert globe scrambling understa...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>coronavirus case europe detected france , seco...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>new coronavirus appeared wuhan , china , start...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fakeNews['body_text_process'] = documents\n",
    "fakeNews['fake'] = 1 - fakeNews['reliability']\n",
    "fakeNews = fakeNews[['fake', 'body_text_process']]\n",
    "fakeNews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T09:09:39.102066Z",
     "start_time": "2024-04-03T09:09:38.774164Z"
    }
   },
   "outputs": [],
   "source": [
    "# import cross validation and other evaluation tool\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T09:09:43.206023Z",
     "start_time": "2024-04-03T09:09:43.198049Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "News = fakeNews['body_text_process'].values\n",
    "y = fakeNews['fake'].values\n",
    "\n",
    "News_train, News_test, y_train, y_test = train_test_split(News, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T09:09:50.583629Z",
     "start_time": "2024-04-03T09:09:49.121517Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df = 0.5, min_df = 0.02)\n",
    "vectorizer.fit(News_train)\n",
    "\n",
    "X = vectorizer.transform(News).toarray()\n",
    "X_train = vectorizer.transform(News_train).toarray()\n",
    "X_test = vectorizer.transform(News_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T09:09:54.302959Z",
     "start_time": "2024-04-03T09:09:54.286015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2029, 2789)\n",
      "(1623, 2789)\n",
      "(406, 2789)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T09:13:23.146709Z",
     "start_time": "2024-04-03T09:13:23.133753Z"
    }
   },
   "outputs": [],
   "source": [
    "# import Logistic Regression from sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# define model to be logistic regression\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "# 'saga' is the algorithm to use in the optimization problem (finding the optimal coefficient values)\n",
    "lr = LogisticRegression(penalty=None, random_state=0, solver='saga')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T09:13:50.975817Z",
     "start_time": "2024-04-03T09:13:24.364363Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.8689021118860655"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_cv = cross_val_score(lr, X, y, cv=10)\n",
    "score_cv.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T09:13:18.388111Z",
     "start_time": "2024-04-03T09:12:40.384131Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 0, 0, ..., 0, 0, 1], dtype=int64)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#predict value of target based on cross validation\n",
    "pred_y = cross_val_predict(lr, X, y, cv=10)\n",
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1272   92]\n",
      " [ 174  491]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.91      1364\n",
      "           1       0.84      0.74      0.79       665\n",
      "\n",
      "    accuracy                           0.87      2029\n",
      "   macro avg       0.86      0.84      0.85      2029\n",
      "weighted avg       0.87      0.87      0.87      2029\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y, pred_y))\n",
    "print(classification_report(y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(penalty=&#x27;none&#x27;, random_state=0, solver=&#x27;saga&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(penalty=&#x27;none&#x27;, random_state=0, solver=&#x27;saga&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(penalty='none', random_state=0, solver='saga')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model using training dataset\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.fit\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.45053734])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the intercept of the trained model (Theta_0)\n",
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>-5.323127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>05</th>\n",
       "      <td>-1.945742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-5.236894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>3.331025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-3.232239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>younger</th>\n",
       "      <td>-1.000704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>youth</th>\n",
       "      <td>3.315562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>youtube</th>\n",
       "      <td>0.462407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zero</th>\n",
       "      <td>5.306109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zoom</th>\n",
       "      <td>-5.565747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2789 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Coefficient\n",
       "000        -5.323127\n",
       "05         -1.945742\n",
       "10         -5.236894\n",
       "100         3.331025\n",
       "11         -3.232239\n",
       "...              ...\n",
       "younger    -1.000704\n",
       "youth       3.315562\n",
       "youtube     0.462407\n",
       "zero        5.306109\n",
       "zoom       -5.565747\n",
       "\n",
       "[2789 rows x 1 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the coefficients of independent attributes\n",
    "# the reason that we use the function .flatten() here is to convert the 8X1 array to 1X8 array\n",
    "coeff_df = pd.DataFrame(lr.coef_.flatten(), vectorizer.get_feature_names_out(), columns=['Coefficient'])  \n",
    "coeff_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Benefit Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probabilities for each prediction\n",
    "proba_y = cross_val_predict(lr, X, y, cv=10, method='predict_proba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.90672087e-01, 9.32791268e-03],\n",
       "       [9.97789908e-01, 2.21009174e-03],\n",
       "       [9.99504084e-01, 4.95916169e-04],\n",
       "       ...,\n",
       "       [9.99432189e-01, 5.67810700e-04],\n",
       "       [9.59587203e-01, 4.04127975e-02],\n",
       "       [1.64482713e-04, 9.99835517e-01]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proba_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "   <img src=\"img/Confusion_Matrix.png\" width=\"700\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_y_1 = proba_y[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the default threshold from 0.5 to 0.2\n",
    "pred_y_1 = [0 if i < 0.2 else 1 for i in proba_y_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1191  173]\n",
      " [ 112  553]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.87      0.89      1364\n",
      "           1       0.76      0.83      0.80       665\n",
      "\n",
      "    accuracy                           0.86      2029\n",
      "   macro avg       0.84      0.85      0.84      2029\n",
      "weighted avg       0.86      0.86      0.86      2029\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y, pred_y_1))\n",
    "print(classification_report(y, pred_y_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penalized Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "   <img src=\"img/L1L2.png\" width=\"700\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L1 Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# penalty='l1' means L1 regularization (recall LASSO regression); default is penalty='L2' (L2 regularization). C=1.0 is inverse of regularization strength; must be a positive float.\n",
    "lr_L1 = LogisticRegression(penalty='l1', C=1.0, random_state=0, solver='saga')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8279763936984832"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_cv = cross_val_score(lr_L1, X, y, cv=10)\n",
    "score_cv.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict value of target based on cross validation\n",
    "pred_y = cross_val_predict(lr_L1, X, y, cv=10)\n",
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1301   63]\n",
      " [ 286  379]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.95      0.88      1364\n",
      "           1       0.86      0.57      0.68       665\n",
      "\n",
      "    accuracy                           0.83      2029\n",
      "   macro avg       0.84      0.76      0.78      2029\n",
      "weighted avg       0.83      0.83      0.82      2029\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y, pred_y))\n",
    "print(classification_report(y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "      <th>Coefficient_L1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>-5.323127</td>\n",
       "      <td>-0.671078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>05</th>\n",
       "      <td>-1.945742</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-5.236894</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>3.331025</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-3.232239</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>younger</th>\n",
       "      <td>-1.000704</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>youth</th>\n",
       "      <td>3.315562</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>youtube</th>\n",
       "      <td>0.462407</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zero</th>\n",
       "      <td>5.306109</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zoom</th>\n",
       "      <td>-5.565747</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2789 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Coefficient  Coefficient_L1\n",
       "000        -5.323127       -0.671078\n",
       "05         -1.945742        0.000000\n",
       "10         -5.236894        0.000000\n",
       "100         3.331025        0.000000\n",
       "11         -3.232239        0.000000\n",
       "...              ...             ...\n",
       "younger    -1.000704        0.000000\n",
       "youth       3.315562        0.000000\n",
       "youtube     0.462407        0.000000\n",
       "zero        5.306109        0.000000\n",
       "zoom       -5.565747        0.000000\n",
       "\n",
       "[2789 rows x 2 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model using training dataset\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.fit\n",
    "lr_L1.fit(X_train, y_train)\n",
    "\n",
    "# show the coefficients of independent attributes\n",
    "# the reason that we use the function .flatten() here is to convert the 8X1 array to 1X8 array\n",
    "coeff_df['Coefficient_L1'] = lr_L1.coef_.flatten()\n",
    "coeff_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L2 Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.843269277666683"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# penalty='l1' means L1 regularization (recall LASSO regression); default is penalty='L2' (L2 regularization). C=1.0 is inverse of regularization strength; must be a positive float.\n",
    "lr_L2 = LogisticRegression(penalty='l2', C=1.0, random_state=0, solver='saga')\n",
    "\n",
    "score_cv = cross_val_score(lr_L2, X, y, cv=10)\n",
    "score_cv.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1311   53]\n",
      " [ 265  400]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.96      0.89      1364\n",
      "           1       0.88      0.60      0.72       665\n",
      "\n",
      "    accuracy                           0.84      2029\n",
      "   macro avg       0.86      0.78      0.80      2029\n",
      "weighted avg       0.85      0.84      0.83      2029\n"
     ]
    }
   ],
   "source": [
    "#predict value of target based on cross validation\n",
    "pred_y = cross_val_predict(lr_L2, X, y, cv=10)\n",
    "\n",
    "print(confusion_matrix(y, pred_y))\n",
    "print(classification_report(y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "      <th>Coefficient_L1</th>\n",
       "      <th>Coefficient_L2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>-5.323127</td>\n",
       "      <td>-0.671078</td>\n",
       "      <td>-0.882948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>05</th>\n",
       "      <td>-1.945742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.131950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-5.236894</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.461151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>3.331025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-3.232239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.226032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>younger</th>\n",
       "      <td>-1.000704</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.065336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>youth</th>\n",
       "      <td>3.315562</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>youtube</th>\n",
       "      <td>0.462407</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zero</th>\n",
       "      <td>5.306109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.395460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zoom</th>\n",
       "      <td>-5.565747</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.445634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2789 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Coefficient  Coefficient_L1  Coefficient_L2\n",
       "000        -5.323127       -0.671078       -0.882948\n",
       "05         -1.945742        0.000000       -0.131950\n",
       "10         -5.236894        0.000000       -0.461151\n",
       "100         3.331025        0.000000        0.187767\n",
       "11         -3.232239        0.000000       -0.226032\n",
       "...              ...             ...             ...\n",
       "younger    -1.000704        0.000000       -0.065336\n",
       "youth       3.315562        0.000000        0.024792\n",
       "youtube     0.462407        0.000000        0.139082\n",
       "zero        5.306109        0.000000        0.395460\n",
       "zoom       -5.565747        0.000000       -0.445634\n",
       "\n",
       "[2789 rows x 3 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model using training dataset\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.fit\n",
    "lr_L2.fit(X_train, y_train)\n",
    "\n",
    "# show the coefficients of independent attributes\n",
    "# the reason that we use the function .flatten() here is to convert the 8X1 array to 1X8 array\n",
    "coeff_df['Coefficient_L2'] = lr_L2.coef_.flatten()\n",
    "coeff_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Components Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "   <img src=\"img/PCA.png\" width=\"900\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>PCA(n_components=100)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA(n_components=100)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "PCA(n_components=100)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_100 = PCA(n_components=100)\n",
    "pca_100.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02318243 0.0156609  0.01471188 0.01081118 0.01072263 0.00951713\n",
      " 0.0090249  0.00845664 0.00794627 0.00768835 0.00702376 0.00675158\n",
      " 0.00640983 0.0056671  0.00561131 0.00546869 0.00537474 0.00495723\n",
      " 0.00484049 0.0045945  0.00444083 0.00435471 0.00430546 0.00422697\n",
      " 0.00409899 0.0040267  0.00394021 0.0039076  0.0037929  0.00376998\n",
      " 0.00366975 0.00358502 0.00345716 0.00342659 0.00332768 0.00329247\n",
      " 0.00323686 0.00317153 0.00311429 0.00308895 0.0030058  0.00296875\n",
      " 0.00293283 0.00288956 0.00284374 0.00282723 0.00275749 0.00269043\n",
      " 0.00265216 0.00262501 0.0025905  0.0025369  0.00253041 0.0024983\n",
      " 0.00248354 0.00245157 0.00242757 0.00239511 0.00237723 0.00235289\n",
      " 0.0023407  0.00229023 0.00228098 0.00222812 0.00221858 0.00220445\n",
      " 0.00217676 0.00217086 0.00215828 0.00213469 0.00210882 0.00208723\n",
      " 0.00206406 0.00204568 0.00202753 0.00202187 0.00201185 0.00198141\n",
      " 0.00197984 0.0019546  0.00193933 0.0019118  0.00189821 0.00189127\n",
      " 0.00188629 0.00187496 0.00183082 0.00182613 0.00181128 0.00180555\n",
      " 0.00180029 0.00177476 0.00176626 0.00176045 0.00174544 0.00172694\n",
      " 0.001718   0.00170524 0.00168648 0.00168291]\n",
      "0.38002222843092787\n"
     ]
    }
   ],
   "source": [
    "print(pca_100.explained_variance_ratio_)\n",
    "print(sum(pca_100.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "   <img src=\"img/PCR.png\" width=\"900\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_PCA_100 = pca_100.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8501755840608691"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_cv = cross_val_score(lr, X_PCA_100, y, cv=10)\n",
    "score_cv.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict value of target based on cross validation\n",
    "pred_y = cross_val_predict(lr, X_PCA_100, y, cv=10)\n",
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1241  123]\n",
      " [ 181  484]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.91      0.89      1364\n",
      "           1       0.80      0.73      0.76       665\n",
      "\n",
      "    accuracy                           0.85      2029\n",
      "   macro avg       0.84      0.82      0.83      2029\n",
      "weighted avg       0.85      0.85      0.85      2029\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y, pred_y))\n",
    "print(classification_report(y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>PCA(n_components=50)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA(n_components=50)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "PCA(n_components=50)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_50 = PCA(n_components=50)\n",
    "pca_50.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02318243 0.0156609  0.01471188 0.01081118 0.01072263 0.00951713\n",
      " 0.0090249  0.00845664 0.00794627 0.00768835 0.00702376 0.00675158\n",
      " 0.00640983 0.00566709 0.0056113  0.00546868 0.00537474 0.00495721\n",
      " 0.00484047 0.00459432 0.00444073 0.00435459 0.00430536 0.00422649\n",
      " 0.00409808 0.0040264  0.00393869 0.00390563 0.00379204 0.00376837\n",
      " 0.00366382 0.0035793  0.00345534 0.00342236 0.00332419 0.00328634\n",
      " 0.00322137 0.00316734 0.0031029  0.0030766  0.00298259 0.00293986\n",
      " 0.00291884 0.00286185 0.00280201 0.00277176 0.00269067 0.00264068\n",
      " 0.00261734 0.00254743]\n",
      "0.2763502767928231\n"
     ]
    }
   ],
   "source": [
    "print(pca_50.explained_variance_ratio_)\n",
    "print(sum(pca_50.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8393235136321513"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_PCA_50 = pca_50.fit_transform(X)\n",
    "\n",
    "score_cv = cross_val_score(lr, X_PCA_50, y, cv=10)\n",
    "score_cv.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict value of target based on cross validation\n",
    "pred_y = cross_val_predict(lr, X_PCA_50, y, cv=10)\n",
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1232  132]\n",
      " [ 194  471]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.90      0.88      1364\n",
      "           1       0.78      0.71      0.74       665\n",
      "\n",
      "    accuracy                           0.84      2029\n",
      "   macro avg       0.82      0.81      0.81      2029\n",
      "weighted avg       0.84      0.84      0.84      2029\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y, pred_y))\n",
    "print(classification_report(y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X_PCA_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8560869141101302"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_cv = cross_val_score(lr, X_poly, y, cv=10)\n",
    "score_cv.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict value of target based on cross validation\n",
    "pred_y = cross_val_predict(lr, X_poly, y, cv=10)\n",
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1247  117]\n",
      " [ 175  490]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.91      0.90      1364\n",
      "           1       0.81      0.74      0.77       665\n",
      "\n",
      "    accuracy                           0.86      2029\n",
      "   macro avg       0.84      0.83      0.83      2029\n",
      "weighted avg       0.85      0.86      0.85      2029\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y, pred_y))\n",
    "print(classification_report(y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "   <img src=\"img/SVM.png\" width=\"600\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svm = svm.SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8629883431692923"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_cv = cross_val_score(model_svm, X, y, cv=10)\n",
    "score_cv.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict value of target based on cross validation\n",
    "pred_y = cross_val_predict(model_svm, X, y, cv=10)\n",
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1311   53]\n",
      " [ 225  440]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.90      1364\n",
      "           1       0.89      0.66      0.76       665\n",
      "\n",
      "    accuracy                           0.86      2029\n",
      "   macro avg       0.87      0.81      0.83      2029\n",
      "weighted avg       0.87      0.86      0.86      2029\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y, pred_y))\n",
    "print(classification_report(y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01331069, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.01002866, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.02679912, 0.        , 0.        , ..., 0.0579478 , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_svm.fit(X, y)\n",
    "\n",
    "# get support vectors\n",
    "model_svm.support_vectors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    1,    4, ..., 2025, 2026, 2027], dtype=int32)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get indices of support vectors\n",
    "model_svm.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([975, 575], dtype=int32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get number of support vectors for each class\n",
    "model_svm.n_support_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "   <img src=\"img/decisiontree.png\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our model by using the default value\n",
    "model_dt = DecisionTreeClassifier(criterion='entropy', max_leaf_nodes = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7343486319075259"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_cv = cross_val_score(model_dt, X, y, cv=10)\n",
    "score_cv.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict value of target based on cross validation\n",
    "pred_y = cross_val_predict(model_dt, X, y, cv=10)\n",
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1073  291]\n",
      " [ 257  408]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.79      0.80      1364\n",
      "           1       0.58      0.61      0.60       665\n",
      "\n",
      "    accuracy                           0.73      2029\n",
      "   macro avg       0.70      0.70      0.70      2029\n",
      "weighted avg       0.73      0.73      0.73      2029\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y, pred_y))\n",
    "print(classification_report(y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "   <img src=\"img/NN.png\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2029, 2789)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MLP = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(1000, 500, 100,), random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "model_MLP.fit(X_train, y_train)\n",
    "\n",
    "# test model\n",
    "pred_y = model_MLP.predict(X_test)\n",
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8694581280788177\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.94      0.91       271\n",
      "           1       0.86      0.73      0.79       135\n",
      "\n",
      "    accuracy                           0.87       406\n",
      "   macro avg       0.87      0.83      0.85       406\n",
      "weighted avg       0.87      0.87      0.87       406\n",
      "\n",
      "Confusion Matrix: [[255  16]\n",
      " [ 37  98]]\n"
     ]
    }
   ],
   "source": [
    "# evaluate result \n",
    "print(\"Accuracy:\", accuracy_score(y_test, pred_y, normalize=True, sample_weight=None))\n",
    "print(\"Classification Report:\", classification_report(y_test, pred_y))\n",
    "print(\"Confusion Matrix:\", confusion_matrix(y_test, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modify the Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MLP2 = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(2000, 500,), random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 1, 0, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "model_MLP2.fit(X_train, y_train)\n",
    "\n",
    "# test model\n",
    "pred_y = model_MLP2.predict(X_test)\n",
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8669950738916257\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.90       271\n",
      "           1       0.84      0.74      0.79       135\n",
      "\n",
      "    accuracy                           0.87       406\n",
      "   macro avg       0.86      0.84      0.85       406\n",
      "weighted avg       0.87      0.87      0.86       406\n",
      "\n",
      "Confusion Matrix: [[252  19]\n",
      " [ 35 100]]\n"
     ]
    }
   ],
   "source": [
    "# evaluate result \n",
    "print(\"Accuracy:\", accuracy_score(y_test, pred_y, normalize=True, sample_weight=None))\n",
    "print(\"Classification Report:\", classification_report(y_test, pred_y))\n",
    "print(\"Confusion Matrix:\", confusion_matrix(y_test, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "   <img src=\"img/activationfunction.png\" width=\"600\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default activation function is **ReLU**.\n",
    "\n",
    "Check this link: https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "   <img src=\"img/ensemble.png\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary of our models\n",
    "estimators=[('lr', lr_L1), ('svm', model_svm), ('dt', model_dt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create our voting classifier, inputting our models\n",
    "model_ensemble = VotingClassifier(estimators, voting='hard')\n",
    "\n",
    "# If ‘hard’, uses predicted class labels for majority rule voting. Else if ‘soft’, predicts the class label based on the argmax of the sums of the predicted probabilities, which is recommended for an ensemble of well-calibrated classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8506657562307955"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_cv = cross_val_score(model_ensemble, X, y, cv=10)\n",
    "score_cv.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict value of target based on cross validation\n",
    "pred_y = cross_val_predict(model_ensemble, X, y, cv=10)\n",
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1309   55]\n",
      " [ 252  413]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.96      0.90      1364\n",
      "           1       0.88      0.62      0.73       665\n",
      "\n",
      "    accuracy                           0.85      2029\n",
      "   macro avg       0.86      0.79      0.81      2029\n",
      "weighted avg       0.85      0.85      0.84      2029\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y, pred_y))\n",
    "print(classification_report(y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Bonus: Random Forest**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "   <img src=\"img/randomforest.png\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_rf = RandomForestClassifier(n_estimators = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8299614690533093"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_cv = cross_val_score(model_rf, X, y, cv=10)\n",
    "score_cv.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict value of target based on cross validation\n",
    "pred_y = cross_val_predict(model_rf, X, y, cv=10)\n",
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1318   46]\n",
      " [ 283  382]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.97      0.89      1364\n",
      "           1       0.89      0.57      0.70       665\n",
      "\n",
      "    accuracy                           0.84      2029\n",
      "   macro avg       0.86      0.77      0.79      2029\n",
      "weighted avg       0.85      0.84      0.83      2029\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y, pred_y))\n",
    "print(classification_report(y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
