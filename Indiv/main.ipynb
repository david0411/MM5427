{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68b1891d-e12f-4cfb-afd4-71a880210c51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:24:29.599752Z",
     "start_time": "2024-04-17T09:24:22.599939Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.9/site-packages (3.8.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk) (2024.4.16)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk) (8.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75bddccb-a807-4107-8173-f68be6bcafa5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:24:29.615018Z",
     "start_time": "2024-04-17T09:24:29.604744Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import string\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95cb7d608483103",
   "metadata": {},
   "source": [
    "Load data file with header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "536c9850-70eb-45db-8824-54f73d150532",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:24:30.657521Z",
     "start_time": "2024-04-17T09:24:29.618628Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('MM5427_COVID-19_Tweets_2.csv', encoding='latin-1', header=0)\n",
    "dataset.text = [row.encode('latin-1').decode('utf-8', 'ignore') for row in dataset.text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a77c0947878639b",
   "metadata": {},
   "source": [
    "# 1.I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9461aaa2a3afda10",
   "metadata": {},
   "source": [
    "Construct a list acc_tag that contain all account tags for each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c05ca9c7-b46d-49c3-8cd2-4dbf51abbfe2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:24:33.356482Z",
     "start_time": "2024-04-17T09:24:30.661951Z"
    }
   },
   "outputs": [],
   "source": [
    "acc_tag = []\n",
    "for text in dataset['text']:\n",
    "    # split the text by whitespace\n",
    "    text_content = re.split(r'\\s|[(),.;!?/\\'\"]', text)\n",
    "    # filter the list of items that starts with \"@\" and append to the acc_tag list\n",
    "    acc_tag.append(list(filter(lambda x: x.startswith('@') and len(x) > 1, text_content)))\n",
    "# add the acc_tag as the new column\n",
    "dataset['acc_tag'] = acc_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96da6df1400df35",
   "metadata": {},
   "source": [
    "Do the same for hashtag and URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae5619b2-75c5-462e-975c-02102013d223",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:24:34.627745Z",
     "start_time": "2024-04-17T09:24:33.358073Z"
    }
   },
   "outputs": [],
   "source": [
    "hashtag = []\n",
    "for text in dataset['text']:\n",
    "    text_content = re.split(r'\\s|[(),.;!?/\\'\"]', text)\n",
    "    hashtag.append(list(filter(lambda x: x.startswith('#') and len(x) > 1, text_content)))\n",
    "dataset['hashtag'] = hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "338be5c6-24d1-4bf4-b3a0-ee3129c6d6fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:24:35.542287Z",
     "start_time": "2024-04-17T09:24:34.629034Z"
    }
   },
   "outputs": [],
   "source": [
    "URL = []\n",
    "for text in dataset['text']:\n",
    "    text_content = re.split(r'\\s', text)\n",
    "    URL.append(list(filter(lambda x: x.startswith('http'), text_content)))\n",
    "dataset['URL'] = URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3d7568f-6cb3-4481-8f5f-45d000672a8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:24:35.605601Z",
     "start_time": "2024-04-17T09:24:35.545458Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_follower_count</th>\n",
       "      <th>user_like_count</th>\n",
       "      <th>user_friend_count</th>\n",
       "      <th>user_media_count</th>\n",
       "      <th>user_post_count</th>\n",
       "      <th>user_list_count</th>\n",
       "      <th>user_verified</th>\n",
       "      <th>...</th>\n",
       "      <th>user_account_type</th>\n",
       "      <th>user_account_age</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>text</th>\n",
       "      <th>acc_tag</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/1/2020</td>\n",
       "      <td>10:05:36</td>\n",
       "      <td>1.201200e+18</td>\n",
       "      <td>524</td>\n",
       "      <td>889</td>\n",
       "      <td>425</td>\n",
       "      <td>441</td>\n",
       "      <td>1644</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>736</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>@AlwayACritic @DariusVolket @ZubSpike @AusMaze...</td>\n",
       "      <td>[@AlwayACritic, @DariusVolket, @ZubSpike, @Aus...</td>\n",
       "      <td>[#China, #SARS, #chinesevirus]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/1/2020</td>\n",
       "      <td>2:02:52</td>\n",
       "      <td>1.380602e+09</td>\n",
       "      <td>102</td>\n",
       "      <td>495</td>\n",
       "      <td>864</td>\n",
       "      <td>10</td>\n",
       "      <td>136</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>58587</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Can‚Äôt drink can‚Äôt smoke wonderful way to start...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[#flu]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/1/2020</td>\n",
       "      <td>2:13:03</td>\n",
       "      <td>8.246220e+17</td>\n",
       "      <td>3269</td>\n",
       "      <td>2131</td>\n",
       "      <td>5001</td>\n",
       "      <td>130</td>\n",
       "      <td>1933</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>25668</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Great use of medical #science in the drama nar...</td>\n",
       "      <td>[@bbcradio4]</td>\n",
       "      <td>[#science, #Flu]</td>\n",
       "      <td>[https://t.co/AXHQi7wwgi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/1/2020</td>\n",
       "      <td>16:11:05</td>\n",
       "      <td>2.749415e+09</td>\n",
       "      <td>767</td>\n",
       "      <td>3079</td>\n",
       "      <td>83</td>\n",
       "      <td>156</td>\n",
       "      <td>11499</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>47040</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>SARS is back! In one of the most heavily traff...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[#Wuhan, #China, #SARS, #HongKong]</td>\n",
       "      <td>[https://t.co/iF4sS1WxMf]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/1/2020</td>\n",
       "      <td>19:01:09</td>\n",
       "      <td>4.330358e+08</td>\n",
       "      <td>7645</td>\n",
       "      <td>9290</td>\n",
       "      <td>984</td>\n",
       "      <td>950</td>\n",
       "      <td>37606</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>70673</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Tim and I spent #NYE playing #Pandemic togethe...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[#NYE, #Pandemic]</td>\n",
       "      <td>[https://t.co/mNHuuvXA9Q]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       date      time       user_id  user_follower_count  user_like_count  \\\n",
       "0  1/1/2020  10:05:36  1.201200e+18                  524              889   \n",
       "1  1/1/2020   2:02:52  1.380602e+09                  102              495   \n",
       "2  1/1/2020   2:13:03  8.246220e+17                 3269             2131   \n",
       "3  1/1/2020  16:11:05  2.749415e+09                  767             3079   \n",
       "4  1/1/2020  19:01:09  4.330358e+08                 7645             9290   \n",
       "\n",
       "   user_friend_count  user_media_count  user_post_count  user_list_count  \\\n",
       "0                425               441             1644                0   \n",
       "1                864                10              136                0   \n",
       "2               5001               130             1933               20   \n",
       "3                 83               156            11499               13   \n",
       "4                984               950            37606               21   \n",
       "\n",
       "   user_verified  ...  user_account_type  user_account_age  reply_count  \\\n",
       "0              0  ...                  1               736            1   \n",
       "1              0  ...                  0             58587            0   \n",
       "2              0  ...                  1             25668            0   \n",
       "3              0  ...                  1             47040            0   \n",
       "4              0  ...                  1             70673            0   \n",
       "\n",
       "   like_count  retweet_count  quote_count  \\\n",
       "0           6              8            3   \n",
       "1           0              0            0   \n",
       "2           1              0            0   \n",
       "3           0              1            0   \n",
       "4           2              0            0   \n",
       "\n",
       "                                                text  \\\n",
       "0  @AlwayACritic @DariusVolket @ZubSpike @AusMaze...   \n",
       "1  Can‚Äôt drink can‚Äôt smoke wonderful way to start...   \n",
       "2  Great use of medical #science in the drama nar...   \n",
       "3  SARS is back! In one of the most heavily traff...   \n",
       "4  Tim and I spent #NYE playing #Pandemic togethe...   \n",
       "\n",
       "                                             acc_tag  \\\n",
       "0  [@AlwayACritic, @DariusVolket, @ZubSpike, @Aus...   \n",
       "1                                                 []   \n",
       "2                                       [@bbcradio4]   \n",
       "3                                                 []   \n",
       "4                                                 []   \n",
       "\n",
       "                              hashtag                        URL  \n",
       "0      [#China, #SARS, #chinesevirus]                         []  \n",
       "1                              [#flu]                         []  \n",
       "2                    [#science, #Flu]  [https://t.co/AXHQi7wwgi]  \n",
       "3  [#Wuhan, #China, #SARS, #HongKong]  [https://t.co/iF4sS1WxMf]  \n",
       "4                   [#NYE, #Pandemic]  [https://t.co/mNHuuvXA9Q]  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7297ea80f37f2d5b",
   "metadata": {},
   "source": [
    "# 1.II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6365a240-41e0-4455-b303-f7f3dd4902f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:24:35.650628Z",
     "start_time": "2024-04-17T09:24:35.610701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('@realDonaldTrump', 812), ('@WHO', 747), ('@CDCgov', 321), ('@narendramodi', 269), ('@DrTedros', 153), ('@BorisJohnson', 144), ('@POTUS', 143), ('@PMOIndia', 140), ('@MoHFW_INDIA', 114), ('@CNN', 97)]\n"
     ]
    }
   ],
   "source": [
    "# put the acc_tag sublist into one single list\n",
    "acc_tag_single_list = [item for sublist in acc_tag for item in sublist]\n",
    "# count the number of each acc in the list\n",
    "acc_counter = Counter(acc_tag_single_list)\n",
    "# get the top 10 countered acc\n",
    "top_10_acc = acc_counter.most_common(10)\n",
    "print(top_10_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5309a302d0d60b56",
   "metadata": {},
   "source": [
    "The result shows that country leaders and international health organizations are the most tagged.\n",
    "This may reflect people seeking information, express opinion or emotion towards authority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a36dddd-31d1-4c98-8d3b-9fba8166ed60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:24:35.727557Z",
     "start_time": "2024-04-17T09:24:35.652739Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#coronavirus', 16760), ('#COVID19', 14314), ('#Coronavirus', 3922), ('#CoronavirusOutbreak', 3359), ('#COVID2019', 2938), ('#CoronavirusPandemic', 2108), ('#covid19', 1594), ('#CoronaVirus', 1490), ('#CoronaVirusUpdate', 1477), ('#Corona', 1337)]\n"
     ]
    }
   ],
   "source": [
    "# Do the same for hashtag and URL\n",
    "hashtag_single_list = [item for sublist in hashtag for item in sublist]\n",
    "hash_counter = Counter(hashtag_single_list)\n",
    "top_10_hash = hash_counter.most_common(10)\n",
    "print(top_10_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2142ff25ce549793",
   "metadata": {},
   "source": [
    "All 10 hashtags are related to COVID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a51381aa-69cf-47ba-aafa-85feb3d2cfb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:24:35.774687Z",
     "start_time": "2024-04-17T09:24:35.733679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('https://t.co/Fbzw6mR9Q5', 12), ('http', 12), ('https://t.co/nFY1lZJJ2I', 12), ('https:/', 11), ('https://t.c', 11), ('https://t.co/vY4fVgAjuk', 10), ('https://t.', 9), ('https://t.co/huLTzc781F', 7), ('https://t.co/', 7), ('https://t.co', 6)]\n"
     ]
    }
   ],
   "source": [
    "URL_single_list = [item for sublist in URL for item in sublist]\n",
    "URL_counter = Counter(URL_single_list)\n",
    "top_10_URL = URL_counter.most_common(10)\n",
    "print(top_10_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74939b3f76b89ca",
   "metadata": {},
   "source": [
    "Those URL are mainly news related. Showing people wants to spread the information in the social media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af327d8a-a992-4714-947b-ec17c740b416",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:24:35.789779Z",
     "start_time": "2024-04-17T09:24:35.778778Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a function for removing punctuation\n",
    "def remove_punctuation(input_string):\n",
    "    # Create a translation table mapping punctuation characters to empty string\n",
    "    translator = str.maketrans('', '', string.punctuation + '‚Äò‚Äô‚Äú‚Äù‚Äì‚Ä¢„Éª‚ùù‚ùû')\n",
    "    # Transform the full-width characters to half-with characters\n",
    "    normalized_text = unicodedata.normalize('NFKC', input_string)\n",
    "    # Remove punctuation using the translation table\n",
    "    no_punct = normalized_text.translate(translator)\n",
    "    return no_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f349e81f-ae8b-41fc-a8cf-1924230f342a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:24:51.852262Z",
     "start_time": "2024-04-17T09:24:35.793076Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Use the result in 1.I to do the removal\n",
    "processed_text = []\n",
    "delimiter = ' '\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nltk.download('punkt')\n",
    "for i in range(dataset.shape[0]):\n",
    "    # get the data of the row\n",
    "    text = dataset['text'][i]\n",
    "    acc_tag = dataset['acc_tag'][i]\n",
    "    hashtag = dataset['hashtag'][i]\n",
    "    URL = dataset['URL'][i]\n",
    "    # remove account tags\n",
    "    for tag1 in acc_tag:\n",
    "        text = text.replace(tag1, '')\n",
    "    # remove hashtag\n",
    "    for tag2 in hashtag:\n",
    "        text = text.replace(tag2, '')\n",
    "    # remove URL\n",
    "    for tag3 in URL:\n",
    "        text = text.replace(tag3, '')\n",
    "    # remove punctuations by above function\n",
    "    text = remove_punctuation(text)\n",
    "    # tokenize the text, remove the stop words and join the tokens to sentence again\n",
    "    tokens = word_tokenize(text)\n",
    "    processed_text.append(delimiter.join([w for w in tokens if not w in stop_words]))\n",
    "dataset['processed_text'] = processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9b1fae5-f3a9-447b-bc25-c17d6b407556",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:24:51.914726Z",
     "start_time": "2024-04-17T09:24:51.856723Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_follower_count</th>\n",
       "      <th>user_like_count</th>\n",
       "      <th>user_friend_count</th>\n",
       "      <th>user_media_count</th>\n",
       "      <th>user_post_count</th>\n",
       "      <th>user_list_count</th>\n",
       "      <th>user_verified</th>\n",
       "      <th>...</th>\n",
       "      <th>user_account_age</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>text</th>\n",
       "      <th>acc_tag</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>URL</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/1/2020</td>\n",
       "      <td>10:05:36</td>\n",
       "      <td>1.201200e+18</td>\n",
       "      <td>524</td>\n",
       "      <td>889</td>\n",
       "      <td>425</td>\n",
       "      <td>441</td>\n",
       "      <td>1644</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>736</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>@AlwayACritic @DariusVolket @ZubSpike @AusMaze...</td>\n",
       "      <td>[@AlwayACritic, @DariusVolket, @ZubSpike, @Aus...</td>\n",
       "      <td>[#China, #SARS, #chinesevirus]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Lets welcome new year decade exciting Im tryin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/1/2020</td>\n",
       "      <td>2:02:52</td>\n",
       "      <td>1.380602e+09</td>\n",
       "      <td>102</td>\n",
       "      <td>495</td>\n",
       "      <td>864</td>\n",
       "      <td>10</td>\n",
       "      <td>136</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>58587</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Can‚Äôt drink can‚Äôt smoke wonderful way to start...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[#flu]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Cant drink cant smoke wonderful way start 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/1/2020</td>\n",
       "      <td>2:13:03</td>\n",
       "      <td>8.246220e+17</td>\n",
       "      <td>3269</td>\n",
       "      <td>2131</td>\n",
       "      <td>5001</td>\n",
       "      <td>130</td>\n",
       "      <td>1933</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>25668</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Great use of medical #science in the drama nar...</td>\n",
       "      <td>[@bbcradio4]</td>\n",
       "      <td>[#science, #Flu]</td>\n",
       "      <td>[https://t.co/AXHQi7wwgi]</td>\n",
       "      <td>Great use medical drama narrative 15 Minute Dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/1/2020</td>\n",
       "      <td>16:11:05</td>\n",
       "      <td>2.749415e+09</td>\n",
       "      <td>767</td>\n",
       "      <td>3079</td>\n",
       "      <td>83</td>\n",
       "      <td>156</td>\n",
       "      <td>11499</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>47040</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>SARS is back! In one of the most heavily traff...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[#Wuhan, #China, #SARS, #HongKong]</td>\n",
       "      <td>[https://t.co/iF4sS1WxMf]</td>\n",
       "      <td>SARS back In one heavily trafficked airports w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/1/2020</td>\n",
       "      <td>19:01:09</td>\n",
       "      <td>4.330358e+08</td>\n",
       "      <td>7645</td>\n",
       "      <td>9290</td>\n",
       "      <td>984</td>\n",
       "      <td>950</td>\n",
       "      <td>37606</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>70673</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Tim and I spent #NYE playing #Pandemic togethe...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[#NYE, #Pandemic]</td>\n",
       "      <td>[https://t.co/mNHuuvXA9Q]</td>\n",
       "      <td>Tim I spent playing together FaceTime We one t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       date      time       user_id  user_follower_count  user_like_count  \\\n",
       "0  1/1/2020  10:05:36  1.201200e+18                  524              889   \n",
       "1  1/1/2020   2:02:52  1.380602e+09                  102              495   \n",
       "2  1/1/2020   2:13:03  8.246220e+17                 3269             2131   \n",
       "3  1/1/2020  16:11:05  2.749415e+09                  767             3079   \n",
       "4  1/1/2020  19:01:09  4.330358e+08                 7645             9290   \n",
       "\n",
       "   user_friend_count  user_media_count  user_post_count  user_list_count  \\\n",
       "0                425               441             1644                0   \n",
       "1                864                10              136                0   \n",
       "2               5001               130             1933               20   \n",
       "3                 83               156            11499               13   \n",
       "4                984               950            37606               21   \n",
       "\n",
       "   user_verified  ...  user_account_age  reply_count  like_count  \\\n",
       "0              0  ...               736            1           6   \n",
       "1              0  ...             58587            0           0   \n",
       "2              0  ...             25668            0           1   \n",
       "3              0  ...             47040            0           0   \n",
       "4              0  ...             70673            0           2   \n",
       "\n",
       "   retweet_count  quote_count  \\\n",
       "0              8            3   \n",
       "1              0            0   \n",
       "2              0            0   \n",
       "3              1            0   \n",
       "4              0            0   \n",
       "\n",
       "                                                text  \\\n",
       "0  @AlwayACritic @DariusVolket @ZubSpike @AusMaze...   \n",
       "1  Can‚Äôt drink can‚Äôt smoke wonderful way to start...   \n",
       "2  Great use of medical #science in the drama nar...   \n",
       "3  SARS is back! In one of the most heavily traff...   \n",
       "4  Tim and I spent #NYE playing #Pandemic togethe...   \n",
       "\n",
       "                                             acc_tag  \\\n",
       "0  [@AlwayACritic, @DariusVolket, @ZubSpike, @Aus...   \n",
       "1                                                 []   \n",
       "2                                       [@bbcradio4]   \n",
       "3                                                 []   \n",
       "4                                                 []   \n",
       "\n",
       "                              hashtag                        URL  \\\n",
       "0      [#China, #SARS, #chinesevirus]                         []   \n",
       "1                              [#flu]                         []   \n",
       "2                    [#science, #Flu]  [https://t.co/AXHQi7wwgi]   \n",
       "3  [#Wuhan, #China, #SARS, #HongKong]  [https://t.co/iF4sS1WxMf]   \n",
       "4                   [#NYE, #Pandemic]  [https://t.co/mNHuuvXA9Q]   \n",
       "\n",
       "                                      processed_text  \n",
       "0  Lets welcome new year decade exciting Im tryin...  \n",
       "1     Cant drink cant smoke wonderful way start 2020  \n",
       "2  Great use medical drama narrative 15 Minute Dr...  \n",
       "3  SARS back In one heavily trafficked airports w...  \n",
       "4  Tim I spent playing together FaceTime We one t...  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcef0cce6a8a1666",
   "metadata": {},
   "source": [
    "# 1.IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92acc148-10a2-4db6-aae0-155a9cba62b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:24:53.417473Z",
     "start_time": "2024-04-17T09:24:51.918194Z"
    }
   },
   "outputs": [],
   "source": [
    "lower_text = []\n",
    "for i in range(dataset.shape[0]):\n",
    "    # apply lowercase for each row\n",
    "    lower_text.append(dataset['processed_text'][i].lower())\n",
    "dataset['processed_text'] = lower_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27b1a87e2be9ddb",
   "metadata": {},
   "source": [
    "# 1.V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "978523fc-b402-4d9a-9ef4-a02d05820c2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:25:25.537016Z",
     "start_time": "2024-04-17T09:24:53.420834Z"
    }
   },
   "outputs": [],
   "source": [
    "stem_text = []\n",
    "delimiter = ' '\n",
    "stemmer = PorterStemmer()\n",
    "for i in range(dataset.shape[0]):\n",
    "    stem_token = []\n",
    "    # tokenize the text and apply stemming for each token\n",
    "    for tokens in word_tokenize(dataset['processed_text'][i]):\n",
    "        stem_token.append(stemmer.stem(tokens))\n",
    "    stem_text.append(delimiter.join(stem_token))\n",
    "dataset['stemmed_text'] = stem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5de0a926-6e2e-443e-8248-8aa3fe495800",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:25:25.583567Z",
     "start_time": "2024-04-17T09:25:25.539530Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_follower_count</th>\n",
       "      <th>user_like_count</th>\n",
       "      <th>user_friend_count</th>\n",
       "      <th>user_media_count</th>\n",
       "      <th>user_post_count</th>\n",
       "      <th>user_list_count</th>\n",
       "      <th>user_verified</th>\n",
       "      <th>...</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>text</th>\n",
       "      <th>acc_tag</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>URL</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>stemmed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/1/2020</td>\n",
       "      <td>10:05:36</td>\n",
       "      <td>1.201200e+18</td>\n",
       "      <td>524</td>\n",
       "      <td>889</td>\n",
       "      <td>425</td>\n",
       "      <td>441</td>\n",
       "      <td>1644</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>@AlwayACritic @DariusVolket @ZubSpike @AusMaze...</td>\n",
       "      <td>[@AlwayACritic, @DariusVolket, @ZubSpike, @Aus...</td>\n",
       "      <td>[#China, #SARS, #chinesevirus]</td>\n",
       "      <td>[]</td>\n",
       "      <td>lets welcome new year decade exciting im tryin...</td>\n",
       "      <td>let welcom new year decad excit im tri upbeat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/1/2020</td>\n",
       "      <td>2:02:52</td>\n",
       "      <td>1.380602e+09</td>\n",
       "      <td>102</td>\n",
       "      <td>495</td>\n",
       "      <td>864</td>\n",
       "      <td>10</td>\n",
       "      <td>136</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Can‚Äôt drink can‚Äôt smoke wonderful way to start...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[#flu]</td>\n",
       "      <td>[]</td>\n",
       "      <td>cant drink cant smoke wonderful way start 2020</td>\n",
       "      <td>cant drink cant smoke wonder way start 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/1/2020</td>\n",
       "      <td>2:13:03</td>\n",
       "      <td>8.246220e+17</td>\n",
       "      <td>3269</td>\n",
       "      <td>2131</td>\n",
       "      <td>5001</td>\n",
       "      <td>130</td>\n",
       "      <td>1933</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Great use of medical #science in the drama nar...</td>\n",
       "      <td>[@bbcradio4]</td>\n",
       "      <td>[#science, #Flu]</td>\n",
       "      <td>[https://t.co/AXHQi7wwgi]</td>\n",
       "      <td>great use medical drama narrative 15 minute dr...</td>\n",
       "      <td>great use medic drama narr 15 minut drama my l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/1/2020</td>\n",
       "      <td>16:11:05</td>\n",
       "      <td>2.749415e+09</td>\n",
       "      <td>767</td>\n",
       "      <td>3079</td>\n",
       "      <td>83</td>\n",
       "      <td>156</td>\n",
       "      <td>11499</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>SARS is back! In one of the most heavily traff...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[#Wuhan, #China, #SARS, #HongKong]</td>\n",
       "      <td>[https://t.co/iF4sS1WxMf]</td>\n",
       "      <td>sars back in one heavily trafficked airports w...</td>\n",
       "      <td>sar back in one heavili traffick airport world...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/1/2020</td>\n",
       "      <td>19:01:09</td>\n",
       "      <td>4.330358e+08</td>\n",
       "      <td>7645</td>\n",
       "      <td>9290</td>\n",
       "      <td>984</td>\n",
       "      <td>950</td>\n",
       "      <td>37606</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Tim and I spent #NYE playing #Pandemic togethe...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[#NYE, #Pandemic]</td>\n",
       "      <td>[https://t.co/mNHuuvXA9Q]</td>\n",
       "      <td>tim i spent playing together facetime we one t...</td>\n",
       "      <td>tim i spent play togeth facetim we one turn sp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       date      time       user_id  user_follower_count  user_like_count  \\\n",
       "0  1/1/2020  10:05:36  1.201200e+18                  524              889   \n",
       "1  1/1/2020   2:02:52  1.380602e+09                  102              495   \n",
       "2  1/1/2020   2:13:03  8.246220e+17                 3269             2131   \n",
       "3  1/1/2020  16:11:05  2.749415e+09                  767             3079   \n",
       "4  1/1/2020  19:01:09  4.330358e+08                 7645             9290   \n",
       "\n",
       "   user_friend_count  user_media_count  user_post_count  user_list_count  \\\n",
       "0                425               441             1644                0   \n",
       "1                864                10              136                0   \n",
       "2               5001               130             1933               20   \n",
       "3                 83               156            11499               13   \n",
       "4                984               950            37606               21   \n",
       "\n",
       "   user_verified  ...  reply_count  like_count  retweet_count  quote_count  \\\n",
       "0              0  ...            1           6              8            3   \n",
       "1              0  ...            0           0              0            0   \n",
       "2              0  ...            0           1              0            0   \n",
       "3              0  ...            0           0              1            0   \n",
       "4              0  ...            0           2              0            0   \n",
       "\n",
       "                                                text  \\\n",
       "0  @AlwayACritic @DariusVolket @ZubSpike @AusMaze...   \n",
       "1  Can‚Äôt drink can‚Äôt smoke wonderful way to start...   \n",
       "2  Great use of medical #science in the drama nar...   \n",
       "3  SARS is back! In one of the most heavily traff...   \n",
       "4  Tim and I spent #NYE playing #Pandemic togethe...   \n",
       "\n",
       "                                             acc_tag  \\\n",
       "0  [@AlwayACritic, @DariusVolket, @ZubSpike, @Aus...   \n",
       "1                                                 []   \n",
       "2                                       [@bbcradio4]   \n",
       "3                                                 []   \n",
       "4                                                 []   \n",
       "\n",
       "                              hashtag                        URL  \\\n",
       "0      [#China, #SARS, #chinesevirus]                         []   \n",
       "1                              [#flu]                         []   \n",
       "2                    [#science, #Flu]  [https://t.co/AXHQi7wwgi]   \n",
       "3  [#Wuhan, #China, #SARS, #HongKong]  [https://t.co/iF4sS1WxMf]   \n",
       "4                   [#NYE, #Pandemic]  [https://t.co/mNHuuvXA9Q]   \n",
       "\n",
       "                                      processed_text  \\\n",
       "0  lets welcome new year decade exciting im tryin...   \n",
       "1     cant drink cant smoke wonderful way start 2020   \n",
       "2  great use medical drama narrative 15 minute dr...   \n",
       "3  sars back in one heavily trafficked airports w...   \n",
       "4  tim i spent playing together facetime we one t...   \n",
       "\n",
       "                                        stemmed_text  \n",
       "0  let welcom new year decad excit im tri upbeat ...  \n",
       "1        cant drink cant smoke wonder way start 2020  \n",
       "2  great use medic drama narr 15 minut drama my l...  \n",
       "3  sar back in one heavili traffick airport world...  \n",
       "4  tim i spent play togeth facetim we one turn sp...  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda2efb1712f0aba",
   "metadata": {},
   "source": [
    "# 1.VI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b977d4de1bd9a4f",
   "metadata": {},
   "source": [
    "Finding the emoji. I notice there are some emoji stuck together. \n",
    "\n",
    "I tried both considering it to be one emoji and multiple emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44f78e8f-e373-4a4e-9255-aa833df67890",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:25:27.955784Z",
     "start_time": "2024-04-17T09:25:25.586168Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('üôè', 560), ('üò∑', 346), ('üòÇ', 242)]\n",
      "[('üôè', 757), ('üòÇ', 602), ('üò∑', 462)]\n"
     ]
    }
   ],
   "source": [
    "emoticons1 = []\n",
    "emoticons2 = []\n",
    "for i in range(dataset.shape[0]):\n",
    "    text = dataset['stemmed_text'][i]\n",
    "    # Consider consecutive emoji as one emoji\n",
    "    emoticons1.append(re.findall('[\\U0001F600-\\U0001F64F]+', text))\n",
    "    # Consider consecutive emoji as multiple emoji\n",
    "    emoticons_multi = re.finditer('[\\U0001F600-\\U0001F64F]', text)\n",
    "    for emoji in emoticons_multi:\n",
    "        emoticons2.append(emoji.group())\n",
    "# create single list of emoji for counting        \n",
    "emoticons_single_list1 = [item for sublist in emoticons1 for item in sublist]\n",
    "emoticons_counter1 = Counter(emoticons_single_list1)\n",
    "top_3_emoticons1 = emoticons_counter1.most_common(3)\n",
    "print(top_3_emoticons1)\n",
    "\n",
    "emoticons_counter2 = Counter(emoticons2)\n",
    "top_3_emoticons2 = emoticons_counter2.most_common(3)\n",
    "print(top_3_emoticons2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5014556320f1e5",
   "metadata": {},
   "source": [
    "The top 3 results are the same but in different order.\n",
    "\n",
    "It shows people are likely to use multiple üòÇ to express themselves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3d482524b52e5",
   "metadata": {},
   "source": [
    "# 2.I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361eb15802a8579a",
   "metadata": {},
   "source": [
    "Two models for Count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a8de9ba7fe14935",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:25:27.971347Z",
     "start_time": "2024-04-17T09:25:27.958799Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer_count_reply = CountVectorizer(max_df=0.5, min_df=0.02)\n",
    "vectorizer_count_like = CountVectorizer(max_df=0.5, min_df=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb140e76e0e4061",
   "metadata": {},
   "source": [
    "# 2.IIa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cd5a05cfe78fc14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:25:27.986208Z",
     "start_time": "2024-04-17T09:25:27.973861Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebf9894a918aeebe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:25:28.002298Z",
     "start_time": "2024-04-17T09:25:27.987944Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    51221.000000\n",
       "mean         8.971984\n",
       "std        103.724886\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          1.000000\n",
       "75%          4.000000\n",
       "max       8221.000000\n",
       "Name: reply_count, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['reply_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e5d219436d9db03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:25:28.018385Z",
     "start_time": "2024-04-17T09:25:28.004586Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reply_count\n",
       "0       20276\n",
       "1        9713\n",
       "2        5263\n",
       "3        3112\n",
       "4        2138\n",
       "        ...  \n",
       "5550        1\n",
       "6935        1\n",
       "7141        1\n",
       "7649        1\n",
       "8221        1\n",
       "Length: 405, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.groupby('reply_count').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a490f3c26cbb02b5",
   "metadata": {},
   "source": [
    "As the data have long tail, I try to keep 99% of the data and exclude the remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bab17b378d54d786",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:25:44.384854Z",
     "start_time": "2024-04-17T09:25:28.020380Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 108.31\n",
      "Training R square: 0.010701\n",
      "Testing MSE: 110.75\n",
      "Testing R square: 0.012406\n"
     ]
    }
   ],
   "source": [
    "dataset2 = dataset[dataset['reply_count'] <= dataset['reply_count'].quantile(0.99)]\n",
    "content = dataset2['stemmed_text'].values\n",
    "y = dataset2['reply_count'].values\n",
    "\n",
    "content_train, content_test, y_train, y_test = train_test_split(content, y, test_size=0.2, random_state=99)\n",
    "\n",
    "vectorizer_count_reply.fit(content)\n",
    "\n",
    "X = vectorizer_count_reply.transform(content).toarray()\n",
    "X_train = vectorizer_count_reply.transform(content_train).toarray()\n",
    "X_test = vectorizer_count_reply.transform(content_test).toarray()\n",
    "\n",
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(X_train, y_train)\n",
    "\n",
    "mse_cv = -cross_val_score(linear_reg, X_train, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "score_cv = cross_val_score(linear_reg, X_train, y_train, cv=10)\n",
    "\n",
    "print(\"Training MSE: %.2f\" % mse_cv.mean())\n",
    "print(\"Training R square: %.6f\" % score_cv.mean())\n",
    "print(\"Testing MSE: %.2f\" % mean_squared_error(y_test, linear_reg.predict(X_test)))\n",
    "print(\"Testing R square: %.6f\" % linear_reg.score(X_test, y_test, sample_weight=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41bb990bf8b9704",
   "metadata": {},
   "source": [
    "# 2.IIb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7449896e69cb71cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:25:44.415040Z",
     "start_time": "2024-04-17T09:25:44.388407Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     51221.000000\n",
       "mean        147.968099\n",
       "std        4008.755911\n",
       "min           0.000000\n",
       "25%           5.000000\n",
       "50%          16.000000\n",
       "75%          45.000000\n",
       "max      717503.000000\n",
       "Name: like_count, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['like_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "961550aff6639719",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:25:44.430234Z",
     "start_time": "2024-04-17T09:25:44.417023Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "like_count\n",
       "0         3856\n",
       "1         2806\n",
       "2         2193\n",
       "3         1608\n",
       "4         1554\n",
       "          ... \n",
       "81190        1\n",
       "152986       1\n",
       "196017       1\n",
       "428200       1\n",
       "717503       1\n",
       "Length: 1596, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.groupby('like_count').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a610c969e8caa02",
   "metadata": {},
   "source": [
    "As the data have long tail, I try to keep 99% of the data and exclude the remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7611a0816c08008e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:25:58.243666Z",
     "start_time": "2024-04-17T09:25:44.432606Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 20440.03\n",
      "Training R square: 0.008071\n",
      "Testing MSE: 20454.69\n",
      "Testing R square: 0.009841\n"
     ]
    }
   ],
   "source": [
    "dataset3 = dataset[dataset['like_count'] <= dataset['like_count'].quantile(0.99)]\n",
    "content = dataset3['stemmed_text'].values\n",
    "y = dataset3['like_count'].values\n",
    "\n",
    "content_train, content_test, y_train, y_test = train_test_split(content, y, test_size=0.2, random_state=99)\n",
    "\n",
    "vectorizer_count_reply.fit(content)\n",
    "\n",
    "X = vectorizer_count_reply.transform(content).toarray()\n",
    "X_train = vectorizer_count_reply.transform(content_train).toarray()\n",
    "X_test = vectorizer_count_reply.transform(content_test).toarray()\n",
    "\n",
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(X_train, y_train)\n",
    "\n",
    "mse_cv = -cross_val_score(linear_reg, X_train, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "score_cv = cross_val_score(linear_reg, X_train, y_train, cv=10)\n",
    "\n",
    "print(\"Training MSE: %.2f\" % mse_cv.mean())\n",
    "print(\"Training R square: %.6f\" % score_cv.mean())\n",
    "print(\"Testing MSE: %.2f\" % mean_squared_error(y_test, linear_reg.predict(X_test)))\n",
    "print(\"Testing R square: %.6f\" % linear_reg.score(X_test, y_test, sample_weight=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6c94afc5c7eae2",
   "metadata": {},
   "source": [
    "# 2.III"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683618f69ca928c4",
   "metadata": {},
   "source": [
    "The MSE of \"reply\" prediction is significantly lower than the MSE of \"like\" prediction. Meaning the people are more predictable to reply the tweet with specific words. While people may randomly give like regardless of the content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b451098450d6ec8",
   "metadata": {},
   "source": [
    "# 2.IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19eec1d6e94579d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:25:58.275028Z",
     "start_time": "2024-04-17T09:25:58.247091Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Two models for Tf-IDf vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer_tfidf_reply = TfidfVectorizer(max_df = 0.5, min_df = 0.02)\n",
    "vectorizer_tfidf_like = TfidfVectorizer(max_df = 0.5, min_df = 0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcc92a9f91bc5e5",
   "metadata": {},
   "source": [
    "# 2.Va"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f87ee8b641ca7b6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:26:11.096543Z",
     "start_time": "2024-04-17T09:25:58.277704Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 108.53\n",
      "Training R square: 0.008544\n",
      "Testing MSE: 110.95\n",
      "Testing R square: 0.010605\n"
     ]
    }
   ],
   "source": [
    "content = dataset2['stemmed_text'].values\n",
    "y = dataset2['reply_count'].values\n",
    "\n",
    "vectorizer_tfidf_reply.fit(content)\n",
    "\n",
    "content_train, content_test, y_train, y_test = train_test_split(content, y, test_size=0.2, random_state=99)\n",
    "X = vectorizer_tfidf_reply.transform(content).toarray()\n",
    "X_train = vectorizer_tfidf_reply.transform(content_train).toarray()\n",
    "X_test = vectorizer_tfidf_reply.transform(content_test).toarray()\n",
    "\n",
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(X_train, y_train)\n",
    "\n",
    "mse_cv = -cross_val_score(linear_reg, X_train, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "score_cv = cross_val_score(linear_reg, X_train, y_train, cv=10)\n",
    "\n",
    "print(\"Training MSE: %.2f\" % mse_cv.mean())\n",
    "print(\"Training R square: %.6f\" % score_cv.mean())\n",
    "print(\"Testing MSE: %.2f\" % mean_squared_error(y_test, linear_reg.predict(X_test)))\n",
    "print(\"Testing R square: %.6f\" % linear_reg.score(X_test, y_test, sample_weight=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bebeb651caafd6",
   "metadata": {},
   "source": [
    "# 2.Vb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eff8a3eac475f361",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:26:24.691084Z",
     "start_time": "2024-04-17T09:26:11.103479Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 20474.87\n",
      "Training R square: 0.006368\n",
      "Testing MSE: 20501.33\n",
      "Testing R square: 0.007583\n"
     ]
    }
   ],
   "source": [
    "content = dataset3['stemmed_text'].values\n",
    "y = dataset3['like_count'].values\n",
    "\n",
    "vectorizer_tfidf_like.fit(content)\n",
    "\n",
    "content_train, content_test, y_train, y_test = train_test_split(content, y, test_size=0.2, random_state=99)\n",
    "X = vectorizer_tfidf_like.transform(content).toarray()\n",
    "X_train = vectorizer_tfidf_like.transform(content_train).toarray()\n",
    "X_test = vectorizer_tfidf_like.transform(content_test).toarray()\n",
    "\n",
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(X_train, y_train)\n",
    "\n",
    "mse_cv = -cross_val_score(linear_reg, X_train, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "score_cv = cross_val_score(linear_reg, X_train, y_train, cv=10)\n",
    "\n",
    "print(\"Training MSE: %.2f\" % mse_cv.mean())\n",
    "print(\"Training R square: %.6f\" % score_cv.mean())\n",
    "print(\"Testing MSE: %.2f\" % mean_squared_error(y_test, linear_reg.predict(X_test)))\n",
    "print(\"Testing R square: %.6f\" % linear_reg.score(X_test, y_test, sample_weight=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a43dc0-f4af-4adc-9da2-f0887a9746c5",
   "metadata": {},
   "source": [
    "\n",
    "The MSE of \"reply\" is again significantly lower than MSE of \"like\" prediction. \n",
    "\n",
    "The result of Count and TF-IDF vectorizer are very similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f61fa8-af33-45b5-9cee-bdd4767e8d97",
   "metadata": {},
   "source": [
    "# 3.I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db3ddbb386baa274",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:26:24.767700Z",
     "start_time": "2024-04-17T09:26:24.692375Z"
    }
   },
   "outputs": [],
   "source": [
    "# load NRC-Emotion-lexicon\n",
    "lexicon = pd.read_csv('NRC-Emotion-Lexicon.txt', sep = '\\t', names = ['term', 'category', 'associated'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e045cafdab0ce169",
   "metadata": {},
   "source": [
    "# 3.II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "490490921a68524d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:26:24.813955Z",
     "start_time": "2024-04-17T09:26:24.770970Z"
    }
   },
   "outputs": [],
   "source": [
    "# positive and negative sentiment word list\n",
    "pos_list = list(lexicon[(lexicon['category'] == 'positive') & (lexicon['associated'] == 1)].term)\n",
    "neg_list = list(lexicon[(lexicon['category'] == 'negative') & (lexicon['associated'] == 1)].term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d16797c01798fdf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:26:24.829351Z",
     "start_time": "2024-04-17T09:26:24.815465Z"
    }
   },
   "outputs": [],
   "source": [
    "def sentiment_score(text_list, sen_list):\n",
    "    temp_list = []\n",
    "    check_list = []\n",
    "    for t in text_list:\n",
    "        if len(t) > 0:\n",
    "            tokenized_text = word_tokenize(t)\n",
    "            temp = 0\n",
    "            word1 = []\n",
    "            for w in sen_list:\n",
    "                if tokenized_text.count(w) > 0:\n",
    "                    word1.append(w)\n",
    "                temp += tokenized_text.count(w)\n",
    "            temp_list.append(temp/len(tokenized_text))\n",
    "            check_list.append(word1)\n",
    "        else:\n",
    "            temp_list.append(0)\n",
    "            check_list.append([])\n",
    "    return temp_list, check_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "910891be7a090f9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:26:34.001214Z",
     "start_time": "2024-04-17T09:26:24.831360Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "# Lemmatize text before applying lexicon\n",
    "lemmatized_text = []\n",
    "delimiter = ' '\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for text in dataset['processed_text']:\n",
    "    token = []\n",
    "    # tokenize the text and apply lemmatization for each token\n",
    "    for tokens in word_tokenize(text):\n",
    "        token.append(lemmatizer.lemmatize(tokens))\n",
    "    lemmatized_text.append(delimiter.join(token))\n",
    "# replace the text\n",
    "dataset['lemmatized_text'] = lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b8b0c6f52baef289",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:29:40.010652Z",
     "start_time": "2024-04-17T09:26:34.003163Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_follower_count</th>\n",
       "      <th>user_like_count</th>\n",
       "      <th>user_friend_count</th>\n",
       "      <th>user_media_count</th>\n",
       "      <th>user_post_count</th>\n",
       "      <th>user_list_count</th>\n",
       "      <th>user_verified</th>\n",
       "      <th>...</th>\n",
       "      <th>acc_tag</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>URL</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>stemmed_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>pos_score</th>\n",
       "      <th>pos_word_list</th>\n",
       "      <th>neg_score</th>\n",
       "      <th>neg_word_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/1/2020</td>\n",
       "      <td>10:05:36</td>\n",
       "      <td>1.201200e+18</td>\n",
       "      <td>524</td>\n",
       "      <td>889</td>\n",
       "      <td>425</td>\n",
       "      <td>441</td>\n",
       "      <td>1644</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>[@AlwayACritic, @DariusVolket, @ZubSpike, @Aus...</td>\n",
       "      <td>[#China, #SARS, #chinesevirus]</td>\n",
       "      <td>[]</td>\n",
       "      <td>lets welcome new year decade exciting im tryin...</td>\n",
       "      <td>let welcom new year decad excit im tri upbeat ...</td>\n",
       "      <td>let welcome new year decade exciting im trying...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>[exciting, launch]</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>[pneumonia, rumor]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/1/2020</td>\n",
       "      <td>2:02:52</td>\n",
       "      <td>1.380602e+09</td>\n",
       "      <td>102</td>\n",
       "      <td>495</td>\n",
       "      <td>864</td>\n",
       "      <td>10</td>\n",
       "      <td>136</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[#flu]</td>\n",
       "      <td>[]</td>\n",
       "      <td>cant drink cant smoke wonderful way start 2020</td>\n",
       "      <td>cant drink cant smoke wonder way start 2020</td>\n",
       "      <td>cant drink cant smoke wonderful way start 2020</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>[wonderful]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/1/2020</td>\n",
       "      <td>2:13:03</td>\n",
       "      <td>8.246220e+17</td>\n",
       "      <td>3269</td>\n",
       "      <td>2131</td>\n",
       "      <td>5001</td>\n",
       "      <td>130</td>\n",
       "      <td>1933</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>[@bbcradio4]</td>\n",
       "      <td>[#science, #Flu]</td>\n",
       "      <td>[https://t.co/AXHQi7wwgi]</td>\n",
       "      <td>great use medical drama narrative 15 minute dr...</td>\n",
       "      <td>great use medic drama narr 15 minut drama my l...</td>\n",
       "      <td>great use medical drama narrative 15 minute dr...</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>[medical]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/1/2020</td>\n",
       "      <td>16:11:05</td>\n",
       "      <td>2.749415e+09</td>\n",
       "      <td>767</td>\n",
       "      <td>3079</td>\n",
       "      <td>83</td>\n",
       "      <td>156</td>\n",
       "      <td>11499</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[#Wuhan, #China, #SARS, #HongKong]</td>\n",
       "      <td>[https://t.co/iF4sS1WxMf]</td>\n",
       "      <td>sars back in one heavily trafficked airports w...</td>\n",
       "      <td>sar back in one heavili traffick airport world...</td>\n",
       "      <td>sars back in one heavily trafficked airport wo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>[heavily]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/1/2020</td>\n",
       "      <td>19:01:09</td>\n",
       "      <td>4.330358e+08</td>\n",
       "      <td>7645</td>\n",
       "      <td>9290</td>\n",
       "      <td>984</td>\n",
       "      <td>950</td>\n",
       "      <td>37606</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[#NYE, #Pandemic]</td>\n",
       "      <td>[https://t.co/mNHuuvXA9Q]</td>\n",
       "      <td>tim i spent playing together facetime we one t...</td>\n",
       "      <td>tim i spent play togeth facetim we one turn sp...</td>\n",
       "      <td>tim i spent playing together facetime we one t...</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>[love]</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>[spent]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       date      time       user_id  user_follower_count  user_like_count  \\\n",
       "0  1/1/2020  10:05:36  1.201200e+18                  524              889   \n",
       "1  1/1/2020   2:02:52  1.380602e+09                  102              495   \n",
       "2  1/1/2020   2:13:03  8.246220e+17                 3269             2131   \n",
       "3  1/1/2020  16:11:05  2.749415e+09                  767             3079   \n",
       "4  1/1/2020  19:01:09  4.330358e+08                 7645             9290   \n",
       "\n",
       "   user_friend_count  user_media_count  user_post_count  user_list_count  \\\n",
       "0                425               441             1644                0   \n",
       "1                864                10              136                0   \n",
       "2               5001               130             1933               20   \n",
       "3                 83               156            11499               13   \n",
       "4                984               950            37606               21   \n",
       "\n",
       "   user_verified  ...                                            acc_tag  \\\n",
       "0              0  ...  [@AlwayACritic, @DariusVolket, @ZubSpike, @Aus...   \n",
       "1              0  ...                                                 []   \n",
       "2              0  ...                                       [@bbcradio4]   \n",
       "3              0  ...                                                 []   \n",
       "4              0  ...                                                 []   \n",
       "\n",
       "                              hashtag                        URL  \\\n",
       "0      [#China, #SARS, #chinesevirus]                         []   \n",
       "1                              [#flu]                         []   \n",
       "2                    [#science, #Flu]  [https://t.co/AXHQi7wwgi]   \n",
       "3  [#Wuhan, #China, #SARS, #HongKong]  [https://t.co/iF4sS1WxMf]   \n",
       "4                   [#NYE, #Pandemic]  [https://t.co/mNHuuvXA9Q]   \n",
       "\n",
       "                                      processed_text  \\\n",
       "0  lets welcome new year decade exciting im tryin...   \n",
       "1     cant drink cant smoke wonderful way start 2020   \n",
       "2  great use medical drama narrative 15 minute dr...   \n",
       "3  sars back in one heavily trafficked airports w...   \n",
       "4  tim i spent playing together facetime we one t...   \n",
       "\n",
       "                                        stemmed_text  \\\n",
       "0  let welcom new year decad excit im tri upbeat ...   \n",
       "1        cant drink cant smoke wonder way start 2020   \n",
       "2  great use medic drama narr 15 minut drama my l...   \n",
       "3  sar back in one heavili traffick airport world...   \n",
       "4  tim i spent play togeth facetim we one turn sp...   \n",
       "\n",
       "                                     lemmatized_text  pos_score  \\\n",
       "0  let welcome new year decade exciting im trying...   0.100000   \n",
       "1     cant drink cant smoke wonderful way start 2020   0.125000   \n",
       "2  great use medical drama narrative 15 minute dr...   0.083333   \n",
       "3  sars back in one heavily trafficked airport wo...   0.000000   \n",
       "4  tim i spent playing together facetime we one t...   0.076923   \n",
       "\n",
       "        pos_word_list neg_score       neg_word_list  \n",
       "0  [exciting, launch]  0.100000  [pneumonia, rumor]  \n",
       "1         [wonderful]  0.000000                  []  \n",
       "2           [medical]  0.000000                  []  \n",
       "3                  []  0.055556           [heavily]  \n",
       "4              [love]  0.076923             [spent]  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['pos_score'] ,dataset['pos_word_list'] = sentiment_score(dataset['lemmatized_text'], pos_list)\n",
    "dataset['neg_score'] ,dataset['neg_word_list'] = sentiment_score(dataset['lemmatized_text'], neg_list)\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be2fccd8430b6cf",
   "metadata": {},
   "source": [
    "# 3.III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f44ddce547194cb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:29:40.073815Z",
     "start_time": "2024-04-17T09:29:40.013652Z"
    }
   },
   "outputs": [],
   "source": [
    "# 4 emotion word list\n",
    "fear_list = list(lexicon[(lexicon['category'] == 'fear') & (lexicon['associated'] == 1)].term)\n",
    "anger_list = list(lexicon[(lexicon['category'] == 'anger') & (lexicon['associated'] == 1)].term)\n",
    "sadness_list = list(lexicon[(lexicon['category'] == 'sadness') & (lexicon['associated'] == 1)].term)\n",
    "joy_list = list(lexicon[(lexicon['category'] == 'joy') & (lexicon['associated'] == 1)].term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a4aad1fe26de7f9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:32:52.807772Z",
     "start_time": "2024-04-17T09:29:40.076338Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset['fear_score'] ,dataset['fear_word_list'] = sentiment_score(dataset['lemmatized_text'], fear_list)\n",
    "dataset['anger_score'] ,dataset['anger_word_list'] = sentiment_score(dataset['lemmatized_text'], anger_list)\n",
    "dataset['sadness_score'] ,dataset['sadness_word_list'] = sentiment_score(dataset['lemmatized_text'], sadness_list)\n",
    "dataset['joy_score'] ,dataset['joy_word_list'] = sentiment_score(dataset['lemmatized_text'], joy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "441af66ded498b7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:32:52.870539Z",
     "start_time": "2024-04-17T09:32:52.809847Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_follower_count</th>\n",
       "      <th>user_like_count</th>\n",
       "      <th>user_friend_count</th>\n",
       "      <th>user_media_count</th>\n",
       "      <th>user_post_count</th>\n",
       "      <th>user_list_count</th>\n",
       "      <th>user_verified</th>\n",
       "      <th>...</th>\n",
       "      <th>neg_score</th>\n",
       "      <th>neg_word_list</th>\n",
       "      <th>fear_score</th>\n",
       "      <th>fear_word_list</th>\n",
       "      <th>anger_score</th>\n",
       "      <th>anger_word_list</th>\n",
       "      <th>sadness_score</th>\n",
       "      <th>sadness_word_list</th>\n",
       "      <th>joy_score</th>\n",
       "      <th>joy_word_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/1/2020</td>\n",
       "      <td>10:05:36</td>\n",
       "      <td>1.201200e+18</td>\n",
       "      <td>524</td>\n",
       "      <td>889</td>\n",
       "      <td>425</td>\n",
       "      <td>441</td>\n",
       "      <td>1644</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>[pneumonia, rumor]</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>[mysterious, pneumonia]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.05</td>\n",
       "      <td>[rumor]</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>[exciting]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/1/2020</td>\n",
       "      <td>2:02:52</td>\n",
       "      <td>1.380602e+09</td>\n",
       "      <td>102</td>\n",
       "      <td>495</td>\n",
       "      <td>864</td>\n",
       "      <td>10</td>\n",
       "      <td>136</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>[wonderful]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/1/2020</td>\n",
       "      <td>2:13:03</td>\n",
       "      <td>8.246220e+17</td>\n",
       "      <td>3269</td>\n",
       "      <td>2131</td>\n",
       "      <td>5001</td>\n",
       "      <td>130</td>\n",
       "      <td>1933</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>[medical]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/1/2020</td>\n",
       "      <td>16:11:05</td>\n",
       "      <td>2.749415e+09</td>\n",
       "      <td>767</td>\n",
       "      <td>3079</td>\n",
       "      <td>83</td>\n",
       "      <td>156</td>\n",
       "      <td>11499</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>[heavily]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/1/2020</td>\n",
       "      <td>19:01:09</td>\n",
       "      <td>4.330358e+08</td>\n",
       "      <td>7645</td>\n",
       "      <td>9290</td>\n",
       "      <td>984</td>\n",
       "      <td>950</td>\n",
       "      <td>37606</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>[spent]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>[love]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       date      time       user_id  user_follower_count  user_like_count  \\\n",
       "0  1/1/2020  10:05:36  1.201200e+18                  524              889   \n",
       "1  1/1/2020   2:02:52  1.380602e+09                  102              495   \n",
       "2  1/1/2020   2:13:03  8.246220e+17                 3269             2131   \n",
       "3  1/1/2020  16:11:05  2.749415e+09                  767             3079   \n",
       "4  1/1/2020  19:01:09  4.330358e+08                 7645             9290   \n",
       "\n",
       "   user_friend_count  user_media_count  user_post_count  user_list_count  \\\n",
       "0                425               441             1644                0   \n",
       "1                864                10              136                0   \n",
       "2               5001               130             1933               20   \n",
       "3                 83               156            11499               13   \n",
       "4                984               950            37606               21   \n",
       "\n",
       "   user_verified  ...  neg_score       neg_word_list  fear_score  \\\n",
       "0              0  ...   0.100000  [pneumonia, rumor]    0.100000   \n",
       "1              0  ...   0.000000                  []    0.000000   \n",
       "2              0  ...   0.000000                  []    0.083333   \n",
       "3              0  ...   0.055556           [heavily]    0.000000   \n",
       "4              0  ...   0.076923             [spent]    0.000000   \n",
       "\n",
       "            fear_word_list  anger_score  anger_word_list  sadness_score  \\\n",
       "0  [mysterious, pneumonia]          0.0               []           0.05   \n",
       "1                       []          0.0               []           0.00   \n",
       "2                [medical]          0.0               []           0.00   \n",
       "3                       []          0.0               []           0.00   \n",
       "4                       []          0.0               []           0.00   \n",
       "\n",
       "  sadness_word_list joy_score joy_word_list  \n",
       "0           [rumor]  0.050000    [exciting]  \n",
       "1                []  0.125000   [wonderful]  \n",
       "2                []  0.000000            []  \n",
       "3                []  0.000000            []  \n",
       "4                []  0.076923        [love]  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f602f25f0ec65835",
   "metadata": {},
   "source": [
    "# 3.IVa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de52388f1e34f9cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:32:53.551721Z",
     "start_time": "2024-04-17T09:32:52.872961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 109.46\n",
      "Training R square: 0.000076\n",
      "Testing MSE: 112.19\n",
      "Testing R square: -0.000479\n"
     ]
    }
   ],
   "source": [
    "dataset2 = dataset[dataset['reply_count'] <= dataset['reply_count'].quantile(0.99)]\n",
    "X = dataset2[['pos_score', 'neg_score', 'fear_score', 'anger_score', 'fear_score', 'sadness_score']]\n",
    "y = dataset2['reply_count'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=99)\n",
    "\n",
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(X_train, y_train)\n",
    "\n",
    "mse_cv = -cross_val_score(linear_reg, X_train, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "score_cv = cross_val_score(linear_reg, X_train, y_train, cv=10)\n",
    "\n",
    "print(\"Training MSE: %.2f\" % mse_cv.mean())\n",
    "print(\"Training R square: %.6f\" % score_cv.mean())\n",
    "print(\"Testing MSE: %.2f\" % mean_squared_error(y_test, linear_reg.predict(X_test)))\n",
    "print(\"Testing R square: %.6f\" % linear_reg.score(X_test, y_test, sample_weight=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cddd17e8c55e74d",
   "metadata": {},
   "source": [
    "# 3.IVb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2129b22d513fdeb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:32:54.447377Z",
     "start_time": "2024-04-17T09:32:53.557685Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 20611.66\n",
      "Training R square: -0.000350\n",
      "Testing MSE: 20657.51\n",
      "Testing R square: 0.000022\n"
     ]
    }
   ],
   "source": [
    "dataset3 = dataset[dataset['like_count'] <= dataset['like_count'].quantile(0.99)]\n",
    "X = dataset3[['pos_score', 'neg_score', 'fear_score', 'anger_score', 'fear_score', 'sadness_score']]\n",
    "y = dataset3['like_count'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=99)\n",
    "\n",
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(X_train, y_train)\n",
    "\n",
    "mse_cv = -cross_val_score(linear_reg, X_train, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "score_cv = cross_val_score(linear_reg, X_train, y_train, cv=10)\n",
    "\n",
    "print(\"Training MSE: %.2f\" % mse_cv.mean())\n",
    "print(\"Training R square: %.6f\" % score_cv.mean())\n",
    "print(\"Testing MSE: %.2f\" % mean_squared_error(y_test, linear_reg.predict(X_test)))\n",
    "print(\"Testing R square: %.6f\" % linear_reg.score(X_test, y_test, sample_weight=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dfc9de642034eb",
   "metadata": {},
   "source": [
    "# 3.V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8f488a24343d83",
   "metadata": {},
   "source": [
    "When comparing between the MSE of \"reply\" and \"like\" using the 2 sentiment and 4 emotion value, the result still shows \"reply\" is more predictable than \"like\"\n",
    "\n",
    "However, when MSE between word vector method and sentiment-emotion method, I find that the word vector result is better on both \"reply\" and \"like\" prediction. Which shows that the sentiment and emotion alone does not have good prediction power.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eabc54fd072621",
   "metadata": {},
   "source": [
    "# 3.VI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e86d857f3040f007",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:32:55.940042Z",
     "start_time": "2024-04-17T09:32:54.450777Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 103.46\n",
      "Training R square: 0.055022\n",
      "Testing MSE: 104.65\n",
      "Testing R square: 0.066774\n"
     ]
    }
   ],
   "source": [
    "X = dataset2[['pos_score', 'neg_score', 'fear_score', 'anger_score', 'fear_score', 'sadness_score',\n",
    "              'user_follower_count','user_like_count', 'user_friend_count', 'user_media_count',\n",
    "              'user_post_count', 'user_list_count' , 'user_verified', 'user_default_profile', \n",
    "              'user_account_type', 'user_account_age']]\n",
    "y = dataset2['reply_count'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=99)\n",
    "\n",
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(X_train, y_train)\n",
    "\n",
    "mse_cv = -cross_val_score(linear_reg, X_train, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "score_cv = cross_val_score(linear_reg, X_train, y_train, cv=10)\n",
    "\n",
    "print(\"Training MSE: %.2f\" % mse_cv.mean())\n",
    "print(\"Training R square: %.6f\" % score_cv.mean())\n",
    "print(\"Testing MSE: %.2f\" % mean_squared_error(y_test, linear_reg.predict(X_test)))\n",
    "print(\"Testing R square: %.6f\" % linear_reg.score(X_test, y_test, sample_weight=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5490634bb04ef201",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T09:32:57.770146Z",
     "start_time": "2024-04-17T09:32:55.943229Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 19524.48\n",
      "Training R square: 0.052670\n",
      "Testing MSE: 19363.70\n",
      "Testing R square: 0.062653\n"
     ]
    }
   ],
   "source": [
    "X = dataset3[['pos_score', 'neg_score', 'fear_score', 'anger_score', 'fear_score', 'sadness_score',\n",
    "              'user_follower_count','user_like_count', 'user_friend_count', 'user_media_count',\n",
    "              'user_post_count', 'user_list_count' , 'user_verified', 'user_default_profile',\n",
    "              'user_account_type', 'user_account_age']]\n",
    "y = dataset3['like_count'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=99)\n",
    "\n",
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(X_train, y_train)\n",
    "\n",
    "mse_cv = -cross_val_score(linear_reg, X_train, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "score_cv = cross_val_score(linear_reg, X_train, y_train, cv=10)\n",
    "\n",
    "print(\"Training MSE: %.2f\" % mse_cv.mean())\n",
    "print(\"Training R square: %.6f\" % score_cv.mean())\n",
    "print(\"Testing MSE: %.2f\" % mean_squared_error(y_test, linear_reg.predict(X_test)))\n",
    "print(\"Testing R square: %.6f\" % linear_reg.score(X_test, y_test, sample_weight=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b61fe337f66a087",
   "metadata": {},
   "source": [
    "# 3.VII"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf72e2a575e708e1",
   "metadata": {},
   "source": [
    "The MSE of using sentiment, emotion and user characteristics is the lowest among all 3 methods. Although the \"reply\" prediction is still much lower than \"like\" prediction, this method achieved the best result.\n",
    "The R square is also much better than the other 2 methods (word vector and sentiment-emotion)\n",
    "\n",
    "Sentiment and emotion do have value in prediction because it achieved simular result and word vector, but it is only part of the factor. We need to add in some other factors, like user characteristics, to get more accurate result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
