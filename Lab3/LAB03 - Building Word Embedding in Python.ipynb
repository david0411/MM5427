{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statistics** involves drawing inferences about a population from a sample, while **machine learning** identifies predictive patterns that generalize well to new data.\n",
    "\n",
    "**Statistical inference** constructs mathematical models to describe the data generation process, thereby formalizing understanding or testing hypotheses about the system's behavior.\n",
    "\n",
    "**Prediction** focuses on forecasting unobserved outcomes or future behaviors, such as determining whether a mouse with a specific gene expression pattern is likely to have a disease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "   <img src=\"img/compare.png\" width=\"700\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Statsmodels` is a Python library designed for statistical modeling, testing, estimation, and performing various statistical analyses. It is built on top of the popular NumPy, SciPy, and pandas libraries, making it a powerful tool for statistical computation and data manipulation within the Python ecosystem.\n",
    "\n",
    "**Key Features of Statsmodels:**\n",
    "\n",
    "1. **Statistical Models**: Supports a wide range of statistical models, including linear regression, generalized linear models (GLMs), robust linear models, and many others. Each model can be fitted to data using different statistical methods, such as ordinary least squares (OLS), maximum likelihood estimation, and others.\n",
    "\n",
    "2. **Time Series Analysis**: Provides extensive tools for time series analysis, which include ARIMA models, vector autoregressive (VAR) models, seasonal decompositions, and tools for dealing with cointegration.\n",
    "\n",
    "3. **Nonparametric Methods**: Includes nonparametric estimation techniques and kernel density estimation, which are useful for making inferences without assuming a specific parametric model form.\n",
    "\n",
    "4. **Hypothesis Tests**: Offers a variety of hypothesis tests and procedures for statistical testing, such as t-tests, F-tests, chi-squared tests, and ANOVA.\n",
    "\n",
    "5. **Regression Analysis**: Facilitates detailed regression analysis, providing extensive output for diagnostic measures, including residuals analysis, influence and leverage diagnostics, and goodness-of-fit metrics.\n",
    "\n",
    "6. **Plotting Functions**: Integrates with Matplotlib to provide a range of plotting functions for visual analysis of data relationships and model diagnostics.\n",
    "\n",
    "7. **Robust Statistical Methods**: Includes methods that are robust to outliers and other anomalies in data, which are crucial for practical data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "   <img src=\"img/statsmodels.png\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T18:30:30.562506Z",
     "start_time": "2024-04-25T18:30:29.861276Z"
    }
   },
   "source": [
    "import statsmodels.api as sm"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-25T18:30:30.578507Z",
     "start_time": "2024-04-25T18:30:30.563506Z"
    }
   },
   "source": [
    "#!pip install nltk\n",
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T18:30:32.274449Z",
     "start_time": "2024-04-25T18:30:30.579506Z"
    }
   },
   "source": [
    "# Import useful libraries used for data management\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "import spacy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\CA.CA-\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Review Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMDB allows users to rate movies on a scale from 1 to 10. For labeling purposes, the curator of the data classified reviews with ratings of 4 stars or less as negative and those with 7 stars or more as positive. Reviews rated 5 or 6 stars were excluded. These labels will serve as the benchmark for comparisons."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T18:30:50.815438Z",
     "start_time": "2024-04-25T18:30:50.338513Z"
    }
   },
   "source": [
    "dataset = pd.read_csv('IMDB.csv')\n",
    "dataset"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T18:30:54.830554Z",
     "start_time": "2024-04-25T18:30:54.820554Z"
    }
   },
   "source": [
    "dataset['sentiment'].value_counts()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    25000\n",
       "negative    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T18:30:55.560560Z",
     "start_time": "2024-04-25T18:30:55.545560Z"
    }
   },
   "source": [
    "# Sample 1000 reviews from each sentiment\n",
    "dataset = dataset.groupby('sentiment').apply(lambda x: x.sample(1000)).reset_index(drop = True)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T18:30:57.074427Z",
     "start_time": "2024-04-25T18:30:57.061427Z"
    }
   },
   "source": [
    "# Convert the 'label' column into a numeric variable; 'negative' as 0, 'positive' as 1\n",
    "dataset['label'] = dataset['sentiment'].map({'negative':0, 'positive':1})\n",
    "dataset"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                 review sentiment  label\n",
       "0     Martin Lawrence is not a funny man i Runteldat...  negative      0\n",
       "1     As soon as it hits a screen, it destroys all i...  negative      0\n",
       "2     You have to acknowledge Cimino's contribution ...  negative      0\n",
       "3     Knights was just a beginning of a series, a pi...  negative      0\n",
       "4     Well, I must say, I initially found this short...  negative      0\n",
       "...                                                 ...       ...    ...\n",
       "1995  This movie takes the psychological thriller to...  positive      1\n",
       "1996  If you find yourself in need of an escape, som...  positive      1\n",
       "1997  This eloquent, simple film makes a remarkably ...  positive      1\n",
       "1998  What a real treat and quite unexpected. This i...  positive      1\n",
       "1999  Although it has been off the air for 6 years n...  positive      1\n",
       "\n",
       "[2000 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Martin Lawrence is not a funny man i Runteldat...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>As soon as it hits a screen, it destroys all i...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You have to acknowledge Cimino's contribution ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Knights was just a beginning of a series, a pi...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well, I must say, I initially found this short...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>This movie takes the psychological thriller to...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>If you find yourself in need of an escape, som...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>This eloquent, simple film makes a remarkably ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>What a real treat and quite unexpected. This i...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>Although it has been off the air for 6 years n...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling - Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling and Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topic modeling** is a type of statistical modeling for discovering abstract topics that occur in a collection of documents. It is a frequently used text-mining tool for the discovery of hidden semantic structures in a text body. Topic models are built around the idea that the semantics of our document are actually being governed by hidden, or \"latent,\" topics that can be inferred from the words in the documents.\n",
    "\n",
    "**Latent Dirichlet Allocation (LDA)** is one of the most popular algorithms for topic modeling. It was introduced by David Blei, Andrew Ng, and Michael I. Jordan in 2003. LDA is a generative probabilistic model, meaning it assumes that each document is a mixture of a small number of topics and that each word's creation is attributable to one of the document's topics. LDA represents documents as mixtures of topics that spit out words with certain probabilities.\n",
    "\n",
    "**How LDA Works**\n",
    "\n",
    "1. **Assumptions**:\n",
    "    - Documents are represented as random mixtures over latent topics.\n",
    "    - Each topic is characterized by a distribution over words.\n",
    "\n",
    "2. **Process**:\n",
    "    - LDA starts with a fixed number of topics. Each topic is modeled as a distribution over words, and each document is modeled as a distribution over topics.\n",
    "    - Though the true topic distribution is unknown, LDA attempts to backtrack from the documents to discover what topics would create those documents in the first place.\n",
    "    - LDA uses two Dirichlet distributions (hence the name): one for the topics in documents and one for the words in topics. Dirichlet distributions are used because they are conjugate to multinomial distributions, which simplifies the computation.\n",
    "    \n",
    "3. **Mathematical Model**:\n",
    "    - Each document is assumed to be generated by first picking a distribution over topics.\n",
    "    - For each word in the document, a topic is chosen from this distribution, and then the word is chosen from the corresponding distribution over words in the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim and LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gensim** is a popular open-source library in Python designed for unsupervised semantic modeling from plain text. It is particularly useful for applications such as topic modeling and similarity detection and primarily focuses on vector space modeling and topic modeling.\n",
    "\n",
    "Gensim’s LDA model allows users to model text data and extract topic distribution of documents and word distribution of topics. It provides an efficient implementation of the LDA algorithm that can scale to large datasets and can handle sparse data efficiently.\n",
    "\n",
    "**Using Gensim for LDA**\n",
    "\n",
    "Here is how you might typically use Gensim to perform LDA topic modeling:\n",
    "\n",
    "1. **Preprocessing**:\n",
    "\n",
    "    - Tokenization: Splitting the text into sentences and sentences into words.\n",
    "    - Removing stopwords: Frequently occurring words such as 'and', 'the', etc., are removed.\n",
    "    - Making bigrams or trigrams: Depending on the context, some words might need to be processed together ('New York', 'financial crisis').\n",
    "    - Lemmatization: Words are reduced to their root form.\n",
    "\n",
    "2. **Creating the Dictionary and Corpus**:\n",
    "\n",
    "    - Gensim’s Dictionary object converts words into unique ids.\n",
    "    - Texts are converted to a bag-of-words format using this dictionary (a list of (word_id, word_frequency) tuples).\n",
    "\n",
    "3. **Running LDA**:\n",
    "\n",
    "    - Using `gensim.models.ldamodel.LdaModel` to train the LDA model on the corpus.\n",
    "    - Parameters such as the number of topics need to be specified."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T18:31:28.179629Z",
     "start_time": "2024-04-25T18:31:28.162629Z"
    }
   },
   "source": [
    "#!pip install gensim --user"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T18:31:29.067186Z",
     "start_time": "2024-04-25T18:31:29.023683Z"
    }
   },
   "source": [
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "   <img src=\"img/lda.png\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-25T18:31:30.394352Z",
     "start_time": "2024-04-25T18:31:30.377353Z"
    }
   },
   "source": [
    "dataset = dataset.drop(columns=['sentiment'])\n",
    "dataset"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                 review  label\n",
       "0     Martin Lawrence is not a funny man i Runteldat...      0\n",
       "1     As soon as it hits a screen, it destroys all i...      0\n",
       "2     You have to acknowledge Cimino's contribution ...      0\n",
       "3     Knights was just a beginning of a series, a pi...      0\n",
       "4     Well, I must say, I initially found this short...      0\n",
       "...                                                 ...    ...\n",
       "1995  This movie takes the psychological thriller to...      1\n",
       "1996  If you find yourself in need of an escape, som...      1\n",
       "1997  This eloquent, simple film makes a remarkably ...      1\n",
       "1998  What a real treat and quite unexpected. This i...      1\n",
       "1999  Although it has been off the air for 6 years n...      1\n",
       "\n",
       "[2000 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Martin Lawrence is not a funny man i Runteldat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>As soon as it hits a screen, it destroys all i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You have to acknowledge Cimino's contribution ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Knights was just a beginning of a series, a pi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well, I must say, I initially found this short...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>This movie takes the psychological thriller to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>If you find yourself in need of an escape, som...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>This eloquent, simple film makes a remarkably ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>What a real treat and quite unexpected. This i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>Although it has been off the air for 6 years n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Unnecessary Characters"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T18:31:33.667775Z",
     "start_time": "2024-04-25T18:31:33.560776Z"
    }
   },
   "source": [
    "# Convert to list\n",
    "review = dataset.review.values.tolist()\n",
    "\n",
    "# Remove all html tags\n",
    "review = [re.sub(\"<.*?>\", \" \", i) for i in review]\n",
    "\n",
    "# Remove unnecessary characters\n",
    "review = [re.sub(\"[^A-Za-z0-9]+\", \" \", i) for i in review]\n",
    "\n",
    "# Change to lower case\n",
    "review = [i.lower() for i in review]\n",
    "\n",
    "print(dataset['review'][0])\n",
    "print('\\n')\n",
    "print(review[:1])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Martin Lawrence is not a funny man i Runteldat. He just has too much on his mind and he is too mad which trips his puns pretty early in the game. He tries to make fun of critics, which boils down to \"f*** them\". Then he goes on to rather primitive sexual jokes on smokers with throat cancer and it just goes downhill from there. 3/10\n",
      "\n",
      "\n",
      "['martin lawrence is not a funny man i runteldat he just has too much on his mind and he is too mad which trips his puns pretty early in the game he tries to make fun of critics which boils down to f them then he goes on to rather primitive sexual jokes on smokers with throat cancer and it just goes downhill from there 3 10']\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T18:31:40.187833Z",
     "start_time": "2024-04-25T18:31:39.722833Z"
    }
   },
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield gensim.utils.simple_preprocess(str(sentence), deacc=True)  # deacc=True removes punctuations\n",
    "\n",
    "review_words = list(sent_to_words(review))\n",
    "\n",
    "print(review_words[:3])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['martin', 'lawrence', 'is', 'not', 'funny', 'man', 'runteldat', 'he', 'just', 'has', 'too', 'much', 'on', 'his', 'mind', 'and', 'he', 'is', 'too', 'mad', 'which', 'trips', 'his', 'puns', 'pretty', 'early', 'in', 'the', 'game', 'he', 'tries', 'to', 'make', 'fun', 'of', 'critics', 'which', 'boils', 'down', 'to', 'them', 'then', 'he', 'goes', 'on', 'to', 'rather', 'primitive', 'sexual', 'jokes', 'on', 'smokers', 'with', 'throat', 'cancer', 'and', 'it', 'just', 'goes', 'downhill', 'from', 'there'], ['as', 'soon', 'as', 'it', 'hits', 'screen', 'it', 'destroys', 'all', 'intelligent', 'life', 'forms', 'around', 'but', 'on', 'behalf', 'of', 'its', 'producers', 'must', 'say', 'it', 'doesn', 'fall', 'into', 'any', 'known', 'movie', 'category', 'it', 'deserves', 'brand', 'new', 'denomination', 'of', 'its', 'own', 'it', 'neurological', 'drama', 'it', 'saddens', 'and', 'depresses', 'every', 'single', 'neuron', 'inside', 'person', 'brain', 'it', 'the', 'closest', 'thing', 'one', 'will', 'ever', 'get', 'to', 'stroke', 'without', 'actually', 'suffering', 'one', 'it', 'drives', 'you', 'speechless', 'all', 'you', 'members', 'go', 'numb', 'your', 'mouth', 'falls', 'open', 'and', 'remains', 'so', 'and', 'the', 'most', 'strange', 'symptom', 'of', 'all', 'is', 'that', 'you', 'get', 'yourself', 'wishing', 'to', 'go', 'blind', 'and', 'deaf', 'no', 'small', 'feat', 'for', 'such', 'sort', 'of', 'movie', 'the', 'only', 'word', 'that', 'comes', 'to', 'my', 'mind', 'just', 'having', 'finished', 'my', 'ordeal', 'is', 'outrage'], ['you', 'have', 'to', 'acknowledge', 'cimino', 'contribution', 'to', 'cinema', 'he', 'gave', 'us', 'both', 'the', 'most', 'over', 'rated', 'film', 'in', 'history', 'the', 'deer', 'hunter', 'and', 'the', 'worst', 'film', 'in', 'history', 'heaven', 'gate', 'and', 'before', 'you', 'start', 'with', 'the', 'it', 'bad', 'but', 'not', 'the', 'worst', 'ever', 'let', 'me', 'explain', 'for', 'years', 'listened', 'to', 'the', 'critics', 'and', 'avoided', 'heaven', 'gate', 'actually', 'this', 'was', 'not', 'hard', 'because', 'you', 'are', 'hardly', 'bombarded', 'with', 'opportunities', 'to', 'view', 'this', 'film', 'then', 'few', 'days', 'after', 'seeing', 'the', 'final', 'cut', 'the', 'making', 'and', 'unmaking', 'of', 'heaven', 'gate', 'documentary', 'stumbled', 'on', 'used', 'dvd', 'of', 'the', 'long', 'version', 'my', 'advice', 'after', 'minutes', 'is', 'to', 'seek', 'out', 'the', 'most', 'negative', 'review', 'ever', 'written', 'about', 'this', 'film', 'you', 'will', 'find', 'wide', 'selection', 'and', 'imagine', 'that', 'the', 'reviewer', 'is', 'cimino', 'devoted', 'mother', 'and', 'that', 'she', 'is', 'doing', 'everything', 'she', 'can', 'to', 'put', 'positive', 'slant', 'on', 'her', 'dear', 'son', 'movie', 'then', 'you', 'will', 'have', 'an', 'idea', 'of', 'just', 'how', 'big', 'mess', 'cimino', 'made', 'while', 'pretty', 'much', 'everything', 'is', 'wrong', 'with', 'this', 'film', 'what', 'ultimately', 'tips', 'the', 'scale', 'to', 'make', 'it', 'the', 'worse', 'ever', 'and', 'classic', 'less', 'than', 'zero', 'example', 'is', 'its', 'shameless', 'distortion', 'of', 'history', 'although', 'the', 'cattlemen', 'association', 'did', 'send', 'group', 'of', 'regulators', 'gunmen', 'to', 'johnson', 'county', 'and', 'did', 'have', 'list', 'of', 'targeted', 'names', 'the', 'actual', 'facts', 'of', 'an', 'interesting', 'historical', 'event', 'are', 'hopelessly', 'exaggerated', 'on', 'the', 'morning', 'of', 'april', 'nick', 'ray', 'and', 'nate', 'champion', 'were', 'besieged', 'and', 'eventually', 'killed', 'by', 'an', 'army', 'of', 'about', 'cattlemen', 'and', 'texas', 'hired', 'guns', 'who', 'had', 'come', 'to', 'johnson', 'county', 'to', 'clean', 'out', 'rustlers', 'the', 'citizens', 'of', 'the', 'county', 'then', 'besieged', 'the', 'regulators', 'who', 'finally', 'were', 'arrested', 'or', 'rescued', 'by', 'the', 'army', 'women', 'did', 'not', 'actively', 'participate', 'in', 'the', 'fighting', 'and', 'aside', 'from', 'ray', 'and', 'champion', 'there', 'were', 'minimal', 'casualties', 'after', 'all', 'these', 'were', 'sieges', 'not', 'assaults', 'and', 'there', 'were', 'not', 'wagons', 'of', 'immigrants', 'riding', 'in', 'circles', 'around', 'the', 'encampment', 'of', 'regulations', 'early', 'westerns', 'to', 'the', 'contrary', 'this', 'was', 'film', 'making', 'device', 'and', 'not', 'an', 'actual', 'tactic', 'of', 'the', 'indians', 'and', 'weeks', 'prior', 'to', 'the', 'arrival', 'of', 'the', 'regulators', 'number', 'of', 'johnson', 'county', 'residents', 'were', 'hanged', 'without', 'trial', 'including', 'jim', 'averell', 'the', 'keeper', 'of', 'modest', 'road', 'ranch', 'and', 'his', 'wife', 'ella', 'watson', 'who', 'cimino', 'resurrects', 'as', 'his', 'two', 'leads', 'and', 'he', 'even', 'shows', 'averell', 'living', 'to', 'ripe', 'old', 'age', 'there', 'is', 'no', 'movie', 'making', 'sin', 'greater', 'than', 'fictionalizing', 'history', 'if', 'you', 'are', 'going', 'to', 'play', 'fast', 'and', 'loose', 'with', 'historical', 'facts', 'then', 'change', 'the', 'names', 'and', 'locations', 'to', 'protect', 'the', 'unsuspecting', 'audience', 'members', 'who', 'might', 'go', 'away', 'from', 'film', 'believing', 'what', 'they', 'saw', 'actually', 'happened', 'fortunately', 'so', 'few', 'people', 'saw', 'this', 'film', 'that', 'the', 'damage', 'was', 'minimal', 'perhaps', 'it', 'is', 'harsh', 'to', 'blame', 'cimino', 'for', 'his', 'distortion', 'of', 'history', 'he', 'could', 'probably', 'escape', 'blame', 'anyway', 'with', 'an', 'insanity', 'defense', 'the', 'film', 'provides', 'plenty', 'of', 'support', 'if', 'cimino', 'was', 'insane', 'during', 'the', 'production', 'of', 'heaven', 'gate', 'it', 'would', 'explain', 'lot', 'of', 'things', 'but', 'my', 'vote', 'goes', 'to', 'lack', 'of', 'directing', 'talent', 'instead', 'of', 'insanity', 'there', 'are', 'some', 'good', 'things', 'about', 'heaven', 'gate', 'you', 'can', 'actually', 'see', 'on', 'the', 'screen', 'where', 'some', 'of', 'the', 'huge', 'budget', 'went', 'expensive', 'sets', 'beautiful', 'epic', 'camera', 'shots', 'artful', 'dance', 'sequences', 'isabelle', 'huppert', 'strange', 'casting', 'choice', 'that', 'actually', 'worked', 'gives', 'an', 'agreeable', 'and', 'likable', 'performance', 'although', 'most', 'of', 'her', 'scenes', 'are', 'extremely', 'boring', 'that', 'tends', 'to', 'happen', 'when', 'the', 'director', 'forgets', 'to', 'give', 'the', 'viewer', 'any', 'reason', 'to', 'care', 'about', 'the', 'characters', 'the', 'dialogue', 'is', 'generally', 'solid', 'if', 'rather', 'ordinary', 'but', 'don', 'fall', 'for', 'the', 'crap', 'that', 'this', 'film', 'experiments', 'with', 'storytelling', 'by', 'intermixing', 'carefully', 'crafted', 'moments', 'of', 'character', 'interaction', 'with', 'textured', 'pageant', 'like', 'explosions', 'of', 'communal', 'action', 'this', 'implies', 'that', 'there', 'was', 'method', 'to', 'cimino', 'madness', 'experiments', 'is', 'another', 'word', 'for', 'when', 'filmmaker', 'gets', 'so', 'lost', 'in', 'his', 'project', 'that', 'coherent', 'story', 'is', 'no', 'longer', 'possible', 'the', 'simple', 'fact', 'is', 'that', 'there', 'is', 'no', 'evidence', 'cimino', 'storyboarded', 'single', 'scene', 'or', 'made', 'any', 'attempt', 'at', 'control', 'or', 'organization', 'what', 'it', 'looks', 'like', 'is', 'that', 'he', 'just', 'turned', 'his', 'dp', 'loose', 'to', 'stage', 'action', 'and', 'to', 'get', 'an', 'endless', 'selection', 'of', 'colorful', 'shots', 'million', 'feet', 'of', 'loosely', 'staged', 'scenes', 'then', 'he', 'tried', 'without', 'success', 'to', 'pare', 'this', 'down', 'and', 'fit', 'everything', 'together', 'in', 'post', 'production', 'the', 'final', 'battle', 'scene', 'is', 'genuinely', 'hilarious', 'as', 'babushka', 'wearing', 'townswomen', 'perhaps', 'borrowed', 'from', 'fiddler', 'on', 'the', 'roof', 'touring', 'company', 'throw', 'countless', 'sticks', 'of', 'dynamite', 'at', 'the', 'regulators', 'unfortunately', 'each', 'explosive', 'falls', 'just', 'short', 'of', 'the', 'target', 'and', 'explodes', 'harmlessly', 'after', 'you', 'see', 'this', 'happen', 'times', 'you', 'can', 'relate', 'to', 'the', 'woman', 'the', 'one', 'who', 'looks', 'like', 'something', 'out', 'of', 'the', 'grapes', 'of', 'wrath', 'who', 'puts', 'huge', 'gun', 'in', 'her', 'mouth', 'and', 'pulls', 'the', 'trigger', 'this', 'is', 'probably', 'what', 'cimino', 'mother', 'did', 'after', 'writing', 'that', 'review', 'so', 'believe', 'what', 'you', 'have', 'been', 'hearing', 'about', 'this', 'film', 'since', 'it', 'is', 'sloppy', 'disconnected', 'poorly', 'paced', 'and', 'historically', 'distorted', 'mess', 'of', 'value', 'only', 'as', 'how', 'not', 'to', 'make', 'film', 'example', 'for', 'film', 'historians', 'and', 'as', 'source', 'of', 'amusement', 'to', 'those', 'knowledgeable', 'about', 'the', 'actual', 'history', 'of', 'the', 'american', 'west']]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Bigram and Trigram Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "   <img src=\"img/n-gram.png\" width=\"700\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-25T18:31:43.354449Z",
     "start_time": "2024-04-25T18:31:43.345450Z"
    }
   },
   "source": [
    "?gensim.models.Phrases"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T18:31:46.253086Z",
     "start_time": "2024-04-25T18:31:44.690084Z"
    }
   },
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(review_words, min_count=5, threshold=10) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[review_words], threshold=10)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[review_words[0]]])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['martin', 'lawrence', 'is', 'not', 'funny', 'man', 'runteldat', 'he', 'just', 'has', 'too_much', 'on', 'his', 'mind', 'and', 'he', 'is', 'too', 'mad', 'which', 'trips', 'his', 'puns', 'pretty', 'early', 'in', 'the', 'game', 'he', 'tries_to_make', 'fun', 'of', 'critics', 'which', 'boils', 'down', 'to', 'them', 'then', 'he', 'goes_on', 'to', 'rather', 'primitive', 'sexual', 'jokes', 'on', 'smokers', 'with', 'throat', 'cancer', 'and', 'it', 'just', 'goes', 'downhill', 'from', 'there']\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T18:31:48.487776Z",
     "start_time": "2024-04-25T18:31:48.483776Z"
    }
   },
   "source": [
    "print(trigram_mod[bigram_mod[review_words[9]]])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this_film', 'is', 'just', 'as', 'bad', 'as', 'the', 'birdman', 'of', 'alcatraz', 'do_not', 'refer', 'to', 'the', 'acting', 'but', 'rather', 'the', 'premise', 'of', 'both', 'films', 'which', 'try_to', 'portray', 'psychopathic', 'criminals', 'as', 'heroic', 'figures', 'moreover', 'it', 'disturbs', 'me', 'when', 'well', 'respected', 'revered', 'actors', 'like', 'alan', 'alda', 'and', 'burt', 'lancaster', 'play', 'such', 'roles', 'because', 'their', 'status', 'tends_to', 'lend', 'credibility', 'to', 'the', 'director', 'intent', 'to', 'elevate', 'the', 'film', 'subject', 'societal', 'outcast', 'was', 'in', 'junior', 'high_school', 'during', 'the', 'last_years', 'of', 'caryl', 'chessman', 'life', 'and', 'his', 'death', 'penalty', 'appeals', 'and', 'books', 'were', 'very_much', 'in', 'the', 'news', 'remember', 'the', 'groundswell', 'of', 'opinion', 'that', 'the', 'death', 'penalty', 'was', 'wrong', 'and', 'chessman', 'was', 'the', 'victim', 'get', 'grip', 'people', 'read', 'the', 'history', 'chessman', 'was', 'criminal', 'and', 'sexual', 'predator', 'he', 'drove', 'around', 'the', 'la', 'streets', 'at_night', 'with', 'stolen', 'police', 'light', 'in', 'his', 'vehicle', 'he', 'stopped', 'cars', 'with', 'attractive', 'women', 'inside', 'under', 'the', 'ruse', 'of', 'making', 'traffic', 'arrest', 'then', 'abducted', 'and', 'raped', 'the', 'women', 'rape', 'is', 'the', 'worst', 'trauma', 'woman', 'can', 'experience', 'and', 'many', 'victims', 'say', 'they', 'would', 'prefer', 'death', 'to', 'its', 'horror', 'and', 'humiliation', 'chessman', 'got', 'exactly_what', 'he', 'deserved', 'it', 'just', 'took', 'decade', 'too_long', 'no', 'sympathy_for', 'the', 'devil', 'here']\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T18:31:54.376366Z",
     "start_time": "2024-04-25T18:31:54.368367Z"
    }
   },
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=None):\n",
    "    if allowed_postags is None:\n",
    "        allowed_postags = ['NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***We remove stop words before applying the n-gram model!***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T18:31:57.829814Z",
     "start_time": "2024-04-25T18:31:57.361772Z"
    }
   },
   "source": [
    "# Remove Stop Words\n",
    "review_words_nostops = remove_stopwords(review_words)\n",
    "\n",
    "# Form Bigrams. You can try Trigrams at home!\n",
    "review_words_bigrams = make_bigrams(review_words_nostops)"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T18:32:09.831595Z",
     "start_time": "2024-04-25T18:31:58.464769Z"
    }
   },
   "source": [
    "# Initialize spacy 'en_core_web_sm' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "review_lemmatized = lemmatization(review_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(review_lemmatized[:1])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['man', 'runteldat', 'much', 'mind', 'mad', 'trip', 'pun', 'pretty', 'early', 'game', 'try', 'make', 'fun', 'critic', 'boil', 'go', 'rather', 'primitive', 'sexual', 'joke', 'smoker', 'throat', 'cancer', 'go', 'downhill']]\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Dictionary and Corpus needed for Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T18:32:19.137686Z",
     "start_time": "2024-04-25T18:32:18.866687Z"
    }
   },
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(review_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = review_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1]) # Return a mapping of (word_id, word_frequency)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 2), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1)]]\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T18:32:20.740826Z",
     "start_time": "2024-04-25T18:32:20.732826Z"
    }
   },
   "source": [
    "id2word[123]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'contrary'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-25T18:32:22.502001Z",
     "start_time": "2024-04-25T18:32:22.487001Z"
    }
   },
   "source": [
    "# Human-readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('boil', 1),\n",
       "  ('cancer', 1),\n",
       "  ('critic', 1),\n",
       "  ('downhill', 1),\n",
       "  ('early', 1),\n",
       "  ('fun', 1),\n",
       "  ('game', 1),\n",
       "  ('go', 2),\n",
       "  ('joke', 1),\n",
       "  ('mad', 1),\n",
       "  ('make', 1),\n",
       "  ('man', 1),\n",
       "  ('mind', 1),\n",
       "  ('much', 1),\n",
       "  ('pretty', 1),\n",
       "  ('primitive', 1),\n",
       "  ('pun', 1),\n",
       "  ('rather', 1),\n",
       "  ('runteldat', 1),\n",
       "  ('sexual', 1),\n",
       "  ('smoker', 1),\n",
       "  ('throat', 1),\n",
       "  ('trip', 1),\n",
       "  ('try', 1)]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-25T18:32:25.945651Z",
     "start_time": "2024-04-25T18:32:25.931652Z"
    }
   },
   "source": [
    "?gensim.models.ldamodel.LdaModel"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T18:32:32.692911Z",
     "start_time": "2024-04-25T18:32:27.387111Z"
    }
   },
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=9, \n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T18:32:41.254667Z",
     "start_time": "2024-04-25T18:32:41.241666Z"
    }
   },
   "source": [
    "# Print the Keyword in the 9 topics\n",
    "lda_model.print_topics()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"companion\" + 0.004*\"salesman\" + 0.003*\"corman\" + 0.001*\"jill\" + 0.000*\"babysit\" + 0.000*\"cloak\" + 0.000*\"demme\" + 0.000*\"blouse\" + 0.000*\"woodstock\" + 0.000*\"interval\"'),\n",
       " (1,\n",
       "  '0.014*\"small\" + 0.012*\"soon\" + 0.011*\"catch\" + 0.008*\"tale\" + 0.008*\"discover\" + 0.008*\"game\" + 0.007*\"class\" + 0.007*\"wait\" + 0.007*\"town\" + 0.007*\"sport\"'),\n",
       " (2,\n",
       "  '0.030*\"film\" + 0.011*\"man\" + 0.010*\"life\" + 0.007*\"story\" + 0.006*\"live\" + 0.006*\"woman\" + 0.006*\"performance\" + 0.005*\"work\" + 0.005*\"role\" + 0.005*\"family\"'),\n",
       " (3,\n",
       "  '0.035*\"movie\" + 0.019*\"film\" + 0.016*\"see\" + 0.015*\"make\" + 0.014*\"good\" + 0.012*\"get\" + 0.010*\"great\" + 0.010*\"well\" + 0.010*\"watch\" + 0.009*\"show\"'),\n",
       " (4,\n",
       "  '0.011*\"western\" + 0.010*\"land\" + 0.010*\"battle\" + 0.010*\"reveal\" + 0.009*\"fight\" + 0.009*\"career\" + 0.007*\"horse\" + 0.007*\"fly\" + 0.007*\"destroy\" + 0.006*\"send\"'),\n",
       " (5,\n",
       "  '0.031*\"generation\" + 0.029*\"english\" + 0.020*\"hitchcock\" + 0.018*\"tragedy\" + 0.007*\"sera\" + 0.003*\"que\" + 0.002*\"richard_gere\" + 0.001*\"whistle\" + 0.001*\"perkin\" + 0.001*\"orphanage\"'),\n",
       " (6,\n",
       "  '0.011*\"comparison\" + 0.009*\"distinct\" + 0.007*\"savage\" + 0.006*\"louise\" + 0.006*\"quote\" + 0.006*\"nude\" + 0.005*\"table\" + 0.005*\"homage\" + 0.005*\"deny\" + 0.005*\"pool\"'),\n",
       " (7,\n",
       "  '0.018*\"spirit\" + 0.016*\"supporting_cast\" + 0.013*\"evening\" + 0.011*\"mirror\" + 0.010*\"praise\" + 0.009*\"cameo\" + 0.006*\"critical\" + 0.005*\"belt\" + 0.005*\"lonesome\" + 0.005*\"presentation\"'),\n",
       " (8,\n",
       "  '0.016*\"screenplay\" + 0.008*\"river\" + 0.005*\"proceeding\" + 0.005*\"godfather\" + 0.004*\"astronaut\" + 0.004*\"done\" + 0.003*\"metaphor\" + 0.003*\"good_idea\" + 0.002*\"doppleganger\" + 0.002*\"intentional\"')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating LDA Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perplexity** is a statistical measure used to evaluate how well a probability model predicts a sample. In the context of LDA, perplexity measures the model's performance by comparing the predicted word distributions across topics with the actual distribution of words in your documents. To use perplexity, you first estimate the LDA model for a given number of topics, then analyze how closely the model's topic distributions align with the observed data.\n",
    "\n",
    "Although perplexity can provide valuable insights, it is not very informative on its own. Its true utility lies in comparing the perplexity scores across different models, each configured with varying numbers of topics. Generally, the model with the lowest perplexity is considered the best, as a lower score indicates a better fit to the data.\n",
    "\n",
    "**Coherence** assesses the semantic similarity between high scoring words within the same topic. This metric, which ranges from 0 to 1, helps determine how coherent the words in each topic are, with higher scores indicating better coherence.\n",
    "\n",
    "Together, these two metrics, perplexity and coherence, are essential for determining the **optimal number of topics** in a dataset. They help balance the statistical fit of the model (perplexity) with the interpretability of the topics (coherence)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T12:49:02.408321Z",
     "start_time": "2024-04-19T12:48:53.876226Z"
    }
   },
   "source": [
    "# Compute Perplexity\n",
    "print('Perplexity: ', lda_model.log_perplexity(corpus))\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=review_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  -8.555970743205263\n",
      "\n",
      "Coherence Score:  0.45678944834664126\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Visualization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-19T12:49:49.130590Z",
     "start_time": "2024-04-19T12:49:49.117863Z"
    }
   },
   "source": [
    "#!pip install pyLDAvis --user"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-19T12:49:55.925513Z",
     "start_time": "2024-04-19T12:49:49.553671Z"
    }
   },
   "source": [
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "6      0.374290  0.075144       1        1  76.107988\n",
       "7      0.206573 -0.171299       2        1  11.005330\n",
       "0     -0.040936  0.314949       3        1   6.981649\n",
       "8     -0.103981 -0.046251       4        1   4.465302\n",
       "3     -0.101518 -0.043959       5        1   0.632793\n",
       "4     -0.093695 -0.038416       6        1   0.387575\n",
       "5     -0.084576 -0.032604       7        1   0.224474\n",
       "2     -0.083139 -0.031746       8        1   0.186027\n",
       "1     -0.073017 -0.025817       9        1   0.008863, topic_info=            Term        Freq        Total Category  logprob  loglift\n",
       "16          film  3958.00000  3958.000000  Default  30.0000  30.0000\n",
       "30         movie  3810.00000  3810.000000  Default  29.0000  29.0000\n",
       "134          man   607.00000   607.000000  Default  28.0000  28.0000\n",
       "392  performance   412.00000   412.000000  Default  27.0000  27.0000\n",
       "475         girl   304.00000   304.000000  Default  26.0000  26.0000\n",
       "..           ...         ...          ...      ...      ...      ...\n",
       "25        little     0.00103   512.281281   Topic9  -9.7203  -3.7863\n",
       "26          look     0.00103   675.615756   Topic9  -9.7203  -4.0630\n",
       "27           low     0.00103    92.111064   Topic9  -9.7203  -2.0704\n",
       "28         maybe     0.00103   174.905400   Topic9  -9.7203  -2.7117\n",
       "29         money     0.00103   142.918102   Topic9  -9.7203  -2.5097\n",
       "\n",
       "[386 rows x 6 columns], token_table=      Topic      Freq        Term\n",
       "term                             \n",
       "0         1  0.977725    absolute\n",
       "530       4  0.977083      accent\n",
       "2980      5  0.947640  accomplish\n",
       "1         1  0.999903      action\n",
       "2         1  0.997427       actor\n",
       "...     ...       ...         ...\n",
       "1305      1  0.580787         yet\n",
       "1305      2  0.413965         yet\n",
       "238       1  0.481841       young\n",
       "238       2  0.516036       young\n",
       "1654      4  0.978449       youth\n",
       "\n",
       "[371 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[7, 8, 1, 9, 4, 5, 6, 3, 2])"
      ],
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el967222920431753448736990158\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el967222920431753448736990158_data = {\"mdsDat\": {\"x\": [0.37428951252337817, 0.20657295401338968, -0.04093638601714173, -0.10398129406328872, -0.1015183356619591, -0.09369467344332859, -0.08457608642074646, -0.08313892543605604, -0.07301676549424688], \"y\": [0.07514360810717592, -0.17129932909297707, 0.3149491962605794, -0.0462507223882992, -0.043959281110268714, -0.03841646019149204, -0.03260399609661927, -0.03174634186382523, -0.025816673624274384], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [76.107987555782, 11.00532958980169, 6.981648516979148, 4.465301868655505, 0.6327933576673493, 0.3875750706697266, 0.22447421344935262, 0.18602678210182053, 0.008863044893403855]}, \"tinfo\": {\"Term\": [\"film\", \"movie\", \"man\", \"performance\", \"girl\", \"child\", \"young\", \"woman\", \"see\", \"war\", \"good\", \"role\", \"get\", \"become\", \"life\", \"make\", \"fight\", \"view\", \"beautiful\", \"world\", \"eye\", \"watch\", \"create\", \"play\", \"car\", \"time\", \"event\", \"go\", \"early\", \"deal\", \"movie\", \"see\", \"good\", \"get\", \"watch\", \"time\", \"go\", \"show\", \"think\", \"great\", \"really\", \"love\", \"even\", \"say\", \"find\", \"know\", \"scene\", \"end\", \"look\", \"come\", \"give\", \"first\", \"much\", \"want\", \"still\", \"bad\", \"little\", \"thing\", \"feel\", \"never\", \"make\", \"story\", \"people\", \"well\", \"take\", \"character\", \"also\", \"film\", \"way\", \"play\", \"war\", \"art\", \"entertain\", \"country\", \"memorable\", \"believable\", \"rate\", \"sexual\", \"approach\", \"portrayal\", \"indeed\", \"beautifully\", \"holly\", \"public\", \"satire\", \"favourite\", \"personal\", \"law\", \"political\", \"succeed\", \"warm\", \"familiar\", \"fascinating\", \"society\", \"prison\", \"social\", \"terrific\", \"masterpiece\", \"thus\", \"delightful\", \"general\", \"popular\", \"beauty\", \"force\", \"documentary\", \"provide\", \"relationship\", \"performance\", \"romantic\", \"tale\", \"portray\", \"fine\", \"young\", \"male\", \"view\", \"film\", \"early\", \"beautiful\", \"role\", \"become\", \"life\", \"yet\", \"audience\", \"play\", \"today\", \"event\", \"monster\", \"husband\", \"listen\", \"british\", \"attack\", \"tension\", \"era\", \"vampire\", \"gorgeous\", \"intense\", \"meanwhile\", \"hurt\", \"water\", \"control\", \"genius\", \"red\", \"foot\", \"minor\", \"cold\", \"park\", \"creature\", \"cinderella\", \"conflict\", \"pop\", \"surprising\", \"delight\", \"stone\", \"unusual\", \"realise\", \"natural\", \"apartment\", \"city\", \"man\", \"battle\", \"woman\", \"create\", \"discover\", \"local\", \"world\", \"deal\", \"fight\", \"eventually\", \"soldier\", \"soul\", \"friendship\", \"journey\", \"area\", \"club\", \"slightly\", \"enter\", \"wave\", \"accent\", \"system\", \"teenage\", \"rape\", \"impact\", \"youth\", \"necessary\", \"generation\", \"comedic\", \"highly_recommend\", \"tape\", \"island\", \"bond\", \"college\", \"trailer\", \"regret\", \"manner\", \"hang\", \"emotionally\", \"truth\", \"car\", \"plan\", \"child\", \"powerful\", \"girl\", \"english\", \"team\", \"school\", \"edge\", \"muppet\", \"studio\", \"available\", \"visually\", \"vhs\", \"epic\", \"central\", \"accomplish\", \"little_bit\", \"plane\", \"surface\", \"parallel\", \"takes_place\", \"shed\", \"goodness\", \"nasty\", \"remark\", \"relatively\", \"indie\", \"kermit\", \"defend\", \"persona\", \"ad\", \"eale\", \"substance\", \"lone\", \"description\", \"fare\", \"live_action\", \"atmosphere\", \"pure\", \"haunt\", \"magnificent\", \"silent\", \"skill\", \"creation\", \"freedom\", \"directorial\", \"factory\", \"horrify\", \"maintain\", \"selfish\", \"glory\", \"transformation\", \"housewife\", \"social_worker\", \"charismatic\", \"fictional\", \"serial\", \"stop_watche\", \"evident\", \"elevate\", \"recent_year\", \"davy\", \"doll\", \"simplicity\", \"mislead\", \"juvenile\", \"admirer\", \"stunning\", \"robot\", \"collection\", \"rank\", \"bottle\", \"stink\", \"delivery\", \"bored\", \"solution\", \"nomination\", \"operate\", \"dimension\", \"corner\", \"weave\", \"clumsy\", \"wooden\", \"clint_eastwood\", \"void\", \"propaganda\", \"precious\", \"everyone_else\", \"plight\", \"exceptionally\", \"hungry\", \"eastwood\", \"manipulative\", \"pathetically\", \"bait\", \"breathe\", \"crudely\", \"visual\", \"princess\", \"quirky\", \"responsible\", \"extraordinary\", \"upset\", \"whole_thing\", \"sadness\", \"detailed\", \"exceptional\", \"irrelevant\", \"icon\", \"stimulate\", \"cousin\", \"faithful\", \"virus\", \"erotic\", \"banish\", \"overwrought\", \"highly_recommended\", \"jacket\", \"rally\", \"ahmad\", \"imprison\", \"vizier\", \"cunne\", \"avaricious\", \"basra\", \"beset\", \"choral\", \"marra\", \"shepherdess\", \"breakup\", \"godawful\", \"isabel\", \"maryl\", \"fuck\", \"python\", \"shakespere\", \"reaccounte\", \"rematch\", \"supercomputer\", \"rosalie\", \"rosalione\", \"consumer\", \"threshold\", \"hairbrush\", \"proven\", \"divulge\", \"platter\", \"pooch\", \"refrigerate\", \"served\", \"calculator\", \"caravagggio\", \"caravaggio\", \"elucidation\", \"backing\", \"honey\", \"clitarissa\", \"jame\", \"skinemax\", \"absolute\", \"action\", \"actor\", \"actually\", \"bad\", \"bear\", \"buy\", \"case\", \"chase\", \"chevy\", \"class\", \"describe\", \"disappoint\", \"ellen\", \"embarrassment\", \"film\", \"get\", \"good\", \"hell\", \"high\", \"imdb\", \"know\", \"lame\", \"lawsuit\", \"little\", \"look\", \"low\", \"maybe\", \"money\"], \"Freq\": [3958.0, 3810.0, 607.0, 412.0, 304.0, 253.0, 321.0, 350.0, 1782.0, 208.0, 1642.0, 368.0, 1473.0, 415.0, 715.0, 1791.0, 108.0, 218.0, 234.0, 332.0, 204.0, 1125.0, 176.0, 796.0, 94.0, 1043.0, 108.0, 1021.0, 177.0, 167.0, 3810.0610186902704, 1781.918954857133, 1642.530800146605, 1472.6365340455193, 1125.1978516275403, 1042.662942930452, 1020.9660891911254, 983.7789275247069, 934.5885171787752, 909.2628699828646, 896.010961435919, 885.1812198937613, 804.460879349988, 742.8620401402128, 719.3777719712123, 705.7579167549287, 703.4185823212479, 696.8857118352566, 675.1913951862704, 672.7879477939927, 675.3037814313402, 631.3999130842799, 663.5475904949551, 552.6263779565534, 545.4755545477009, 533.8139559389997, 511.8543762094443, 505.33833966071035, 497.0242946158818, 465.7073428665984, 1783.6733871123845, 1144.7125333942597, 710.3553301613949, 1152.4819268560561, 732.5620019190828, 1028.6058853355441, 776.6114443956632, 3544.874187971425, 717.4241438392506, 726.0707534286727, 207.9566428040336, 113.80968498610805, 95.10147656078983, 66.5948680545606, 74.04385133548608, 65.0514780667348, 57.76120446054373, 57.370684365941415, 56.72700615336709, 52.021792040748565, 57.871160474536, 49.02420042758945, 48.418552450072376, 47.45914545120933, 46.78637071281686, 44.749509794617815, 44.61897452033059, 44.634967942087165, 43.670682602551935, 43.219902110723595, 42.78394503211663, 42.639463851009424, 41.04820762416945, 40.93129352975705, 40.56936675814948, 40.48762480591067, 40.03837623203731, 39.862527915327554, 39.05239615796806, 38.19132441035785, 64.68024823997341, 61.05342504505171, 67.39077627595618, 123.91869754479794, 101.45633825841601, 103.78320760578254, 106.89233672752175, 274.53159592517943, 63.86259570741096, 69.19607465745155, 96.45242989055514, 103.66581013919988, 166.33121940128507, 58.5403508223736, 103.58867881381587, 413.0427192435897, 90.1588647417815, 95.53599591704423, 110.68521685416586, 113.10366415772812, 97.02709247254258, 67.10491741095802, 65.82124452962994, 69.7296035071655, 59.63914748215041, 107.6051641977429, 78.32013697520134, 75.692402205868, 61.69103240342351, 60.12200138190591, 50.8065550979758, 49.700304002513526, 49.45570469564967, 47.22781990931969, 46.82453235183707, 46.294205502550824, 45.25353565415337, 44.52748407019717, 43.732492291743526, 42.75230236762462, 42.577221950321515, 42.52299759387395, 41.40413472012998, 40.91679170470307, 37.091894103495825, 37.01328592734303, 36.97054740986922, 36.22363135281557, 36.05369314436299, 35.04637688915191, 34.673043091358785, 34.34948650425503, 34.053270113810335, 33.98063130303875, 33.96334278802267, 54.945209211193095, 46.88805986044355, 55.399944017835814, 186.43909432686218, 50.87091919835858, 115.54425661049743, 63.215176816553, 44.71232682161159, 41.74519683289855, 49.749977425501335, 43.807521364733084, 108.01597423827089, 67.98083595141046, 63.840741384795706, 55.435737308408164, 49.97581473350885, 49.79251277524585, 48.38188466165663, 46.083958079952076, 43.037592420238326, 41.90053935143985, 37.74933313702486, 37.42077917158309, 35.67037682256121, 33.34051933984633, 32.942091132978604, 31.4557674331528, 31.235604333336674, 31.2008552899353, 30.969801809744357, 30.626306258520515, 30.526977299726894, 30.311120868618286, 28.48244617361468, 28.270779742934728, 27.748182291486785, 27.068378357321187, 27.059322485055315, 26.309119302208895, 26.11182720809312, 25.35205320333621, 70.15761660617487, 87.18907724439967, 37.82178816124005, 105.23944621538033, 35.447214543369604, 71.17689505898778, 37.11794855010757, 44.05378922127174, 30.587751415401897, 27.90092904202934, 25.2354380308431, 20.14327221756299, 16.406819743281346, 15.75358026284554, 14.720010656020866, 14.327729685074514, 12.684791291865091, 12.164870831308104, 11.25744015962048, 10.13758410960881, 9.496517250208536, 8.769776826063175, 8.337408473940659, 7.914149490883091, 7.568181605843737, 7.506212468873243, 7.227234763072189, 7.092131839337805, 6.87541745215936, 5.991688927768404, 5.181044019627023, 5.1784454150333294, 5.0871578837603595, 5.122297197962268, 4.929272415451912, 4.894105735514923, 4.800992681664287, 4.6459290197262835, 4.530309929249793, 18.25960041632547, 16.02363397448036, 14.895619967670909, 10.434129912175834, 10.331491569946502, 8.338711101671851, 7.900615148767364, 6.018502260718404, 5.883114935505407, 5.58316734046753, 5.440451249182186, 5.145862648470225, 4.946302178067416, 4.744350465523221, 4.024241932018053, 3.978273374707778, 3.570202036915006, 3.483844603687159, 3.248474863191002, 3.1823857212678677, 2.9772762980512764, 2.8454754868619636, 2.695585494558261, 2.661544689100581, 2.456277956511225, 2.4138815965003646, 2.387420448978143, 2.3090094960966083, 2.246559770239275, 2.2102689693467195, 7.535911622713506, 6.406866913915582, 6.013798288449757, 5.717039929920677, 4.483202538264045, 3.0190846032904934, 2.860154481327734, 2.4903577933026626, 2.4648409744367554, 2.252061607606957, 2.241830611072214, 2.0800804326596944, 1.9283800423353474, 1.6416065523505237, 1.6075090281597153, 1.3892507349615546, 1.3271160574615724, 1.3049400670877465, 1.2605743339133457, 1.201918818139211, 1.0419970046387725, 0.9683567031101707, 0.9411198196928283, 0.9011704356788843, 0.6325281552316067, 0.4983123070252625, 0.40705039415470295, 0.33063667813838415, 0.3158272863853552, 0.2929029516427524, 8.400861262910833, 6.25099969261827, 4.819073428324322, 3.2857918083156346, 3.281651701365409, 3.2651281460254022, 2.5276891518359177, 2.0082046148186445, 1.9494760027267573, 1.8357682050098072, 1.461623009095286, 1.4182876438148522, 1.099346136531621, 1.0601983095942924, 1.0201325259827516, 0.8957219936844921, 0.8302955828719186, 0.7798368771516412, 0.7158739678324436, 0.5315057553792759, 0.3885764067007824, 0.22030145215901886, 0.07144225232147755, 0.02881620529483136, 0.02881620529483136, 0.028147496504900983, 0.023358701358747062, 0.023358701358747062, 0.023358701358747062, 0.023358701358747062, 0.02505825469020751, 0.023605788747273093, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495, 0.001029732001021495], \"Total\": [3958.0, 3810.0, 607.0, 412.0, 304.0, 253.0, 321.0, 350.0, 1782.0, 208.0, 1642.0, 368.0, 1473.0, 415.0, 715.0, 1791.0, 108.0, 218.0, 234.0, 332.0, 204.0, 1125.0, 176.0, 796.0, 94.0, 1043.0, 108.0, 1021.0, 177.0, 167.0, 3810.4853305883325, 1782.3433027459228, 1642.955676729894, 1473.0618846795485, 1125.6221689386748, 1043.0873286663616, 1021.3911492093438, 984.2235428317142, 935.012850742183, 909.6874792532234, 896.4352853027154, 885.6056084274571, 804.8852588451543, 743.2863874177953, 719.8031082752302, 706.1822708735227, 703.8436060164439, 697.3102215529796, 675.6157560858061, 673.213622963348, 675.7516247063252, 631.8242973897998, 664.0246477258314, 553.0507294823637, 545.8999159632197, 534.2382404547207, 512.2812811838687, 505.7626662634951, 497.4564601407142, 466.1317310879257, 1791.5503131589305, 1152.0903848646685, 713.3891786641815, 1167.139975111705, 738.0529441799448, 1055.000384481149, 788.5676332949943, 3958.247517858182, 738.5208629987768, 796.1243905148601, 208.4021043380457, 114.25513135485696, 95.54687314620439, 67.04028222906702, 74.54410389922748, 65.49701923338392, 58.20663233282836, 57.81610152997361, 57.17269419975668, 52.467203556630906, 58.386643747175825, 49.46981531165181, 48.865117480060796, 47.904590357674536, 47.231947257370244, 45.195091824057464, 45.06443266723088, 45.08199065390291, 44.11605071491004, 43.665339645169574, 43.22960019035986, 43.084899766326565, 41.49372030003535, 41.376669376331016, 41.01478064563462, 40.933101667345774, 40.48378761790826, 40.30796063116706, 39.49782828895681, 38.636875206027526, 67.32257947241956, 63.51857184561565, 71.10414717505515, 140.69277179059563, 114.70532566213095, 119.77715635021663, 127.47687128460468, 412.23029597901683, 70.76273457385467, 78.37750971245521, 127.63442997403214, 141.7917726992502, 321.68269046310223, 69.05670767589343, 218.63730511583373, 3958.247517858182, 177.40709943344172, 234.84720935784222, 368.62731893973154, 415.9349228700309, 715.848554765247, 161.84936772711578, 211.09846375527786, 796.1243905148601, 130.607496463471, 108.04870356598052, 78.76356711259263, 76.13590157220062, 62.1346396098407, 60.56552743880843, 51.24992847252986, 50.143783058959364, 49.89927877038087, 47.67127985201666, 47.26813212152273, 46.737815757541306, 45.69724273445866, 44.97096379919567, 44.17589643085662, 43.195763873235585, 43.020792100255825, 42.96653123014481, 41.84775243111665, 41.36036561271025, 37.53541220034416, 37.45672043834153, 37.41410313893075, 36.66725465795977, 36.49723745757687, 35.490025822315765, 35.116595898004505, 34.793563601055304, 34.49674685871673, 34.42426834360943, 34.406916355715026, 56.453383546205984, 48.42992273459928, 76.92357227898806, 607.712774285319, 69.83039821651568, 350.7145968541384, 176.40069695667316, 82.95772349131053, 66.69651185417908, 332.64660839390484, 167.73448389214298, 108.46305889013836, 68.42812215667614, 64.28784174589089, 55.88290399911432, 50.423073620274536, 50.23977857471164, 48.829246811366886, 46.53107834751224, 43.48474792205684, 42.347686683914546, 38.19663972264345, 37.867816660571265, 36.117632474623235, 33.787630218534204, 33.38912817446308, 31.902916641013974, 31.682782075337112, 31.648074031440004, 31.416950934023593, 31.07348769914794, 30.974208925434926, 30.758321107115584, 28.92950060473642, 28.71817638983268, 28.195649934418984, 27.51553503127116, 27.50651561627696, 26.756241080115267, 26.55891402832603, 25.7993279129798, 72.72252029484206, 94.37669041860006, 47.263171610123976, 253.68292306623795, 51.88262427508741, 304.1003194875162, 60.131519250434195, 102.9217620034321, 68.82636057013674, 28.399125121713343, 25.73326936640067, 20.64122516761816, 16.905413839987887, 16.251843637576336, 15.218091324107327, 14.82587189927431, 13.182744079872762, 12.663038765066988, 11.75593278344589, 10.635577252762038, 9.994896410849575, 9.267777932219945, 8.835470928531302, 8.413355827075815, 8.066650785222313, 8.004130961030352, 7.7256950887308635, 7.590190006016198, 7.373504479401593, 6.489590557362969, 5.6790560352718185, 5.677081908508752, 5.585472050199234, 5.627809076757199, 5.42753717959865, 5.3922583605585865, 5.299301483111217, 5.14389037602628, 5.029040669735054, 18.772626754442147, 16.53649384031164, 15.408802124633107, 10.9471143482559, 10.84480603361362, 8.851854962163308, 8.414051933271022, 6.5317040564591355, 6.396301173001502, 6.096177570167644, 5.953645062659095, 5.65911462659772, 5.459811790625452, 5.2571291286667385, 4.537051831923468, 4.4916472211858505, 4.083960649119424, 3.9965795627038205, 3.761767670614708, 3.695266224213744, 3.490028871727916, 3.35866229507522, 3.2088149169727656, 3.175546704509128, 2.9687194399367423, 2.926666704767496, 2.9003207869171517, 2.8224014365640704, 2.7592685812354123, 2.7250587802251194, 8.060223515783765, 6.931099193406277, 6.537818331353161, 6.2414724027278385, 5.007522896629039, 3.542751457839432, 3.3840708803638253, 3.0141477695332735, 2.9890150489215896, 2.7757717025084836, 2.7666706324590398, 2.604762041934385, 2.4533899008242477, 2.166167947403531, 2.131347520382414, 1.9127850026837494, 1.8522852996117518, 1.82908090329295, 1.784831041857717, 1.7261809636719263, 1.5657032951144707, 1.4922466005961472, 1.4649362870120497, 1.4251244956142195, 1.1568385454386754, 1.0244672664126608, 0.9305846792935701, 0.8541610550949478, 0.8393527449588019, 0.8459690545280238, 8.928290231984642, 6.778622681475715, 5.346443782819196, 3.8132852032398343, 3.809589483601303, 3.7923893796483727, 3.055197383281036, 2.5362027659731856, 2.478213674796175, 2.3642061572602486, 1.9894377403637646, 1.9480879033753378, 1.626752675327511, 1.5871754960445705, 1.547914088844653, 1.4226929691670378, 1.357294144990066, 1.316173663520086, 1.244924473354681, 1.05860396793876, 0.9154526523278648, 0.7473728894195016, 0.8633490336207659, 0.6320672392235176, 0.6320672392235176, 0.7030120217552452, 0.5872181062798314, 0.5872181062798314, 0.5872181062798314, 0.5872181062798314, 0.6581713385364482, 1.0543115634221634, 0.5460147057457152, 0.5460147057457152, 0.5460147057457152, 0.5460147057457152, 0.5460205744487563, 0.5460346112290101, 0.5460346112290101, 0.5460399463146596, 0.5460583181456004, 0.5460583181456004, 0.5460633359631655, 0.5460633359631655, 0.5460701557195535, 0.5460701557195535, 0.5460710261689075, 0.5460710261689075, 0.5460785665984282, 0.5460785665984282, 0.5460785665984282, 0.5460785665984282, 0.5460785665984282, 0.5460841921845427, 0.5460841921845427, 0.5460841921845427, 0.5460841921845427, 0.5460905397854412, 0.5460905397854412, 0.5461121813624001, 0.5461121813624001, 0.5461121813624001, 16.364517274330193, 240.02324372625887, 511.3154587139797, 255.4456079582736, 534.2382404547207, 80.05174528120725, 110.77301396321303, 122.5389464441229, 65.08535399417946, 0.9403139409116252, 54.71781542354717, 64.30105361504907, 57.80682189556467, 0.593127735477406, 3.1589841356628217, 3958.247517858182, 1473.0618846795485, 1642.955676729894, 38.065590822582024, 151.40030770358698, 57.44619314924889, 706.1822708735227, 22.301917157916748, 0.5472166251240504, 512.2812811838687, 675.6157560858061, 92.11106410827719, 174.90539959247909, 142.91810205695413], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.6545, -4.4145, -4.4959, -4.6051, -4.8742, -4.9504, -4.9714, -5.0085, -5.0598, -5.0873, -5.102, -5.1141, -5.2097, -5.2894, -5.3215, -5.3406, -5.344, -5.3533, -5.3849, -5.3885, -5.3847, -5.452, -5.4023, -5.5852, -5.5982, -5.6199, -5.6619, -5.6747, -5.6913, -5.7563, -4.4135, -4.857, -5.3341, -4.8502, -5.3034, -4.9639, -5.245, -3.7266, -5.3242, -5.3123, -4.6288, -5.2316, -5.4112, -5.7675, -5.6615, -5.791, -5.9098, -5.9166, -5.9279, -6.0145, -5.9079, -6.0738, -6.0863, -6.1063, -6.1205, -6.1651, -6.168, -6.1676, -6.1895, -6.1998, -6.21, -6.2134, -6.2514, -6.2542, -6.2631, -6.2651, -6.2763, -6.2807, -6.3012, -6.3235, -5.7967, -5.8544, -5.7556, -5.1465, -5.3465, -5.3238, -5.2943, -4.3511, -5.8094, -5.7292, -5.3971, -5.325, -4.8522, -5.8964, -5.3257, -3.9426, -5.4646, -5.4066, -5.2594, -5.2378, -5.3911, -5.7599, -5.7792, -5.7215, -5.8778, -4.8326, -5.1502, -5.1844, -5.3889, -5.4147, -5.583, -5.605, -5.61, -5.6561, -5.6646, -5.676, -5.6988, -5.7149, -5.7329, -5.7556, -5.7597, -5.761, -5.7877, -5.7995, -5.8976, -5.8998, -5.9009, -5.9213, -5.926, -5.9544, -5.9651, -5.9745, -5.9831, -5.9852, -5.9858, -5.5047, -5.6633, -5.4965, -4.2829, -5.5817, -4.7614, -5.3645, -5.7108, -5.7795, -5.604, -5.7312, -4.3818, -4.8449, -4.9077, -5.0489, -5.1526, -5.1562, -5.185, -5.2336, -5.302, -5.3288, -5.4331, -5.4419, -5.4898, -5.5573, -5.5693, -5.6155, -5.6225, -5.6236, -5.6311, -5.6422, -5.6455, -5.6526, -5.7148, -5.7223, -5.7409, -5.7657, -5.7661, -5.7942, -5.8017, -5.8312, -4.8133, -4.596, -5.4312, -4.4079, -5.496, -4.7989, -5.45, -5.2787, -5.6435, -3.7815, -3.8819, -4.1073, -4.3124, -4.3531, -4.4209, -4.4479, -4.5697, -4.6116, -4.6891, -4.7939, -4.8592, -4.9388, -4.9894, -5.0415, -5.0862, -5.0944, -5.1323, -5.1512, -5.1822, -5.3198, -5.4651, -5.4656, -5.4834, -5.4765, -5.515, -5.5221, -5.5413, -5.5742, -5.5994, -3.7152, -3.8458, -3.9188, -4.2748, -4.2847, -4.499, -4.553, -4.8251, -4.8478, -4.9002, -4.926, -4.9817, -5.0213, -5.063, -5.2276, -5.2391, -5.3473, -5.3718, -5.4417, -5.4623, -5.5289, -5.5742, -5.6283, -5.641, -5.7213, -5.7387, -5.7497, -5.7831, -5.8105, -5.8268, -4.0541, -4.2164, -4.2797, -4.3303, -4.5734, -4.9688, -5.0229, -5.1613, -5.1716, -5.2619, -5.2665, -5.3414, -5.4171, -5.5781, -5.5991, -5.745, -5.7908, -5.8076, -5.8422, -5.8898, -6.0326, -6.1059, -6.1344, -6.1778, -6.5318, -6.7703, -6.9726, -7.1805, -7.2263, -7.3017, -3.7576, -4.0531, -4.3133, -4.6963, -4.6975, -4.7026, -4.9586, -5.1886, -5.2183, -5.2784, -5.5063, -5.5364, -5.7912, -5.8274, -5.866, -5.996, -6.0719, -6.1346, -6.2201, -6.5179, -6.8312, -7.3986, -8.5248, -9.4327, -9.4327, -9.4562, -9.6427, -9.6427, -9.6427, -9.6427, -9.5724, -9.6322, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203, -9.7203], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.2729, 0.2728, 0.2728, 0.2727, 0.2726, 0.2726, 0.2726, 0.2726, 0.2726, 0.2725, 0.2725, 0.2725, 0.2725, 0.2724, 0.2724, 0.2724, 0.2724, 0.2724, 0.2724, 0.2724, 0.2724, 0.2723, 0.2723, 0.2722, 0.2722, 0.2722, 0.2722, 0.2722, 0.2721, 0.2721, 0.2686, 0.2666, 0.2688, 0.2604, 0.2655, 0.2477, 0.2577, 0.1627, 0.244, 0.1809, 2.2047, 2.2029, 2.2021, 2.2001, 2.2001, 2.2, 2.1991, 2.1991, 2.199, 2.1983, 2.1979, 2.1977, 2.1976, 2.1974, 2.1973, 2.1969, 2.1969, 2.1968, 2.1966, 2.1965, 2.1964, 2.1964, 2.196, 2.196, 2.1959, 2.1958, 2.1957, 2.1957, 2.1954, 2.1952, 2.1668, 2.1672, 2.1532, 2.0798, 2.0841, 2.0635, 2.0307, 1.8003, 2.1042, 2.0822, 1.9267, 1.8936, 1.5472, 2.0416, 1.4598, -0.0532, 1.5299, 1.3074, 1.0037, 0.9046, 0.2083, 1.3264, 1.0414, -0.2283, 1.4229, 2.6578, 2.6562, 2.656, 2.6547, 2.6545, 2.6532, 2.653, 2.653, 2.6525, 2.6525, 2.6523, 2.6521, 2.652, 2.6518, 2.6516, 2.6515, 2.6515, 2.6512, 2.6511, 2.65, 2.65, 2.65, 2.6497, 2.6497, 2.6493, 2.6492, 2.649, 2.6489, 2.6489, 2.6489, 2.6348, 2.6295, 2.3337, 1.4803, 2.3451, 1.5516, 1.6357, 2.0438, 2.1933, 0.7618, 1.3193, 3.1047, 3.1023, 3.1019, 3.1008, 3.0999, 3.0999, 3.0996, 3.0992, 3.0985, 3.0982, 3.0971, 3.097, 3.0964, 3.0955, 3.0954, 3.0947, 3.0946, 3.0946, 3.0945, 3.0943, 3.0943, 3.0942, 3.0933, 3.0931, 3.0928, 3.0924, 3.0924, 3.092, 3.0919, 3.0913, 3.0729, 3.0296, 2.886, 2.229, 2.7279, 1.6566, 2.6264, 2.2603, 2.2978, 5.0451, 5.0432, 5.0384, 5.0328, 5.0316, 5.0295, 5.0286, 5.0243, 5.0226, 5.0195, 5.0148, 5.0116, 5.0075, 5.0048, 5.0016, 4.999, 4.9986, 4.9961, 4.9949, 4.9928, 4.983, 4.971, 4.9708, 4.9693, 4.9687, 4.9665, 4.9658, 4.964, 4.961, 4.9583, 5.5253, 5.5215, 5.5191, 5.505, 5.5045, 5.4933, 5.4901, 5.4712, 5.4694, 5.4651, 5.4629, 5.4579, 5.4542, 5.4504, 5.4331, 5.4316, 5.4186, 5.4157, 5.4063, 5.4036, 5.3941, 5.3872, 5.3787, 5.3764, 5.3635, 5.3604, 5.3584, 5.3522, 5.3475, 5.3436, 6.0319, 6.0205, 6.0156, 6.0114, 5.9886, 5.9392, 5.931, 5.9083, 5.9063, 5.8901, 5.8888, 5.8742, 5.8584, 5.8219, 5.8171, 5.7794, 5.7658, 5.7615, 5.7514, 5.7372, 5.692, 5.6667, 5.6567, 5.6408, 5.4954, 5.3785, 5.2723, 5.1501, 5.1217, 5.0385, 6.2261, 6.206, 6.1832, 6.1382, 6.1379, 6.1373, 6.0975, 6.0536, 6.0471, 6.0341, 5.9787, 5.9696, 5.8952, 5.8835, 5.8701, 5.8244, 5.7956, 5.7636, 5.7337, 5.598, 5.4301, 5.0655, 3.7951, 3.199, 3.199, 3.0691, 3.0626, 3.0626, 3.0626, 3.0626, 3.0188, 2.4879, 3.0577, 3.0577, 3.0577, 3.0577, 3.0577, 3.0577, 3.0577, 3.0576, 3.0576, 3.0576, 3.0576, 3.0576, 3.0576, 3.0576, 3.0576, 3.0576, 3.0576, 3.0576, 3.0576, 3.0576, 3.0576, 3.0576, 3.0576, 3.0576, 3.0576, 3.0575, 3.0575, 3.0575, 3.0575, 3.0575, -0.3425, -3.0282, -3.7844, -3.0904, -3.8283, -1.9301, -2.2549, -2.3559, -1.7231, 2.5141, -1.5496, -1.711, -1.6045, 2.9749, 1.3023, -5.831, -4.8425, -4.9517, -1.1867, -2.5673, -1.5983, -4.1073, -0.6521, 3.0555, -3.7863, -4.063, -2.0704, -2.7117, -2.5097]}, \"token.table\": {\"Topic\": [1, 4, 5, 1, 1, 2, 1, 5, 6, 1, 2, 1, 3, 2, 4, 2, 6, 3, 1, 2, 5, 1, 8, 1, 3, 1, 1, 2, 2, 2, 3, 1, 2, 3, 2, 4, 7, 7, 3, 1, 1, 4, 1, 2, 5, 1, 2, 6, 1, 1, 1, 2, 4, 3, 3, 4, 1, 7, 4, 7, 3, 7, 4, 1, 4, 3, 3, 7, 2, 8, 1, 2, 3, 6, 3, 6, 1, 3, 5, 3, 2, 7, 1, 5, 8, 7, 6, 1, 2, 3, 1, 2, 6, 5, 1, 2, 7, 5, 6, 2, 4, 1, 2, 4, 4, 2, 5, 3, 8, 1, 3, 4, 7, 6, 8, 7, 8, 1, 3, 4, 6, 8, 2, 5, 2, 2, 1, 6, 4, 1, 2, 1, 1, 2, 3, 1, 3, 1, 2, 3, 6, 4, 1, 2, 4, 3, 1, 1, 4, 1, 6, 1, 1, 5, 3, 1, 4, 6, 1, 1, 2, 4, 8, 2, 6, 6, 7, 3, 3, 8, 1, 4, 2, 5, 3, 8, 4, 4, 6, 5, 1, 1, 2, 1, 2, 4, 3, 1, 5, 5, 1, 3, 5, 1, 1, 1, 6, 6, 1, 2, 2, 3, 1, 2, 3, 4, 4, 2, 1, 3, 2, 3, 6, 1, 3, 1, 1, 5, 5, 1, 3, 4, 1, 7, 7, 8, 5, 3, 1, 4, 1, 2, 5, 2, 3, 4, 5, 1, 2, 7, 2, 3, 1, 2, 1, 2, 2, 3, 4, 7, 8, 2, 7, 1, 2, 2, 6, 8, 7, 4, 2, 3, 1, 6, 3, 4, 1, 2, 3, 5, 5, 8, 7, 1, 2, 1, 2, 8, 2, 1, 1, 1, 4, 1, 6, 6, 2, 5, 1, 1, 6, 6, 6, 4, 2, 6, 2, 4, 7, 4, 1, 8, 7, 3, 6, 1, 2, 5, 7, 5, 2, 5, 3, 4, 1, 2, 4, 5, 1, 2, 4, 1, 4, 4, 3, 2, 1, 1, 2, 1, 1, 2, 4, 6, 3, 4, 3, 8, 3, 5, 1, 2, 8, 8, 5, 7, 1, 2, 2, 1, 3, 4, 1, 2, 7, 1, 2, 8, 1, 2, 3, 4, 7, 1, 2, 3, 1, 2, 1, 2, 4], \"Freq\": [0.9777251434784462, 0.9770830024780686, 0.9476398376907693, 0.9999031605193813, 0.997427305019707, 0.0019557398137641313, 0.9982555661777266, 0.8951794861853528, 0.7339291227453003, 0.9853308292065458, 0.015217464543730436, 0.02064839139802262, 0.9704743957070633, 0.9969794286910234, 0.9830174154729364, 0.9977670030935892, 0.9588429065069799, 0.995123340071317, 0.6868832554276451, 0.31265030936706606, 0.9464423735166879, 0.9995540557813348, 0.759778156725546, 0.2720878082506235, 0.7303409589885158, 0.9993536020854327, 0.5918741822825002, 0.408776413662734, 0.9905029903853078, 0.9422797777891784, 0.0421916318413065, 0.6900117884298939, 0.2716771153051499, 0.03846755614940175, 0.9924115747678088, 0.9749922703975402, 0.6635374749094304, 0.7987981448258015, 0.990662552400294, 0.9930216400587444, 0.07417085690282288, 0.9218377929350843, 0.8813524445409477, 0.11424939095901175, 0.9861376297100561, 0.9753550947813767, 0.024644540781648003, 0.7506418808713516, 0.9986885836990748, 1.0634746083106135, 0.5636958068425518, 0.01970964359589342, 0.41390251551376184, 0.9818024375104145, 0.714995395696456, 0.2729982419931923, 0.9868815043511721, 0.5398736362101481, 0.9885865884399682, 0.9383734847901073, 0.9857358113589798, 0.9177373392628598, 0.9930609886676118, 0.999682681757972, 0.9976350353761559, 0.9863760248113341, 0.995467984457687, 0.8151985949432964, 0.9993991339575603, 0.6300500495957243, 0.5952357434607511, 0.04535129473986675, 0.35714144607645065, 0.9507904233828449, 0.9889318972208675, 0.6736911454464071, 0.7392636095016382, 0.2623193453070329, 0.8804280093286108, 0.9771922298573856, 0.9835163893914441, 0.8865062541708543, 0.9953180609317634, 0.9435205783129936, 0.8070329125935815, 0.7678244568224482, 0.9380421336827803, 0.9860427909871556, 0.4580646430585856, 0.5424449720430619, 0.11333388336555543, 0.8805170938400845, 0.6833712895089934, 0.8884452069722755, 0.49039751102317086, 0.5073077700239699, 0.8644248620025002, 0.98594586558555, 0.9349245991508404, 0.9496723855406562, 0.9690174908557346, 0.9995551168713852, 0.3824949092706305, 0.6153178975223187, 0.9917897124697814, 0.9942763888738926, 0.9442952222381785, 0.9819781208758741, 0.7367599747564807, 0.9989001428024356, 0.9995492443280379, 0.993743476465774, 0.6386906147035276, 0.8932127544942152, 0.8459499159403647, 0.682623544017498, 0.7874864241708329, 0.6997817064720127, 0.15659450774198885, 0.14191377264117738, 0.9842232990983891, 0.646030685557226, 0.9980294774552796, 0.9720269357416911, 0.9881013248157715, 0.9956833404650012, 0.999082411874629, 0.7974974168220686, 0.9957307225623483, 0.8955983636713574, 0.10433910414563345, 0.9988842667307251, 0.17631488431297537, 0.7334699187419775, 0.09168373984274719, 0.9986953692771786, 0.9797419841719793, 0.03553842842361441, 0.8813530249056375, 0.08529222821667459, 0.9185964257009871, 0.9916095233808908, 0.02970771494011681, 0.9655007355537963, 0.986728472317406, 0.9995166964799864, 0.9999579890836956, 0.7661945255192836, 0.23347558502948126, 0.998887720459937, 0.9510894401918659, 0.9996170426876652, 1.0000269777637545, 0.9917374896971598, 0.9943274229488615, 0.9992442687528383, 0.9789556896893478, 0.9734695713964955, 0.9982768999202526, 0.9577259267126619, 0.03963003834673084, 1.0008326628979343, 0.9446403284763144, 0.982295806811192, 0.8398216466345467, 0.8905418887603445, 0.7016930823078769, 1.0006456655217348, 0.99821501329341, 0.5133238588809872, 0.9922328508681219, 0.9716979907770189, 0.9933778733908725, 0.9493450528924199, 0.9842137304539685, 0.5026545841123694, 0.967870146898276, 0.9952273162519005, 0.72482976597535, 0.924557558287327, 0.9997418925948152, 0.9864622778490784, 0.9981812991681676, 0.8605172084226796, 0.1355035214561687, 0.0041908305605000635, 0.9978330990461016, 0.9994509243374681, 0.9356977623663895, 0.9942254056703455, 0.37483219594239614, 0.6297180891832256, 0.9272552733326465, 0.999088600169165, 0.9993161646429357, 0.9987942370511903, 0.9134827390921705, 0.8835304336300426, 0.9957855980356938, 0.0044654062692183576, 0.8543702992170873, 0.14480852529103175, 0.5858030554297008, 0.07404813902903522, 0.30606564132001224, 0.03455579821354977, 0.9717358997532246, 0.9923598061934956, 0.9948234897573853, 0.9847421268169229, 0.9927009130063053, 0.9912871753580558, 0.7086164193690165, 0.9935760268031809, 0.9903055798437734, 0.999872632867935, 0.9999628813088252, 0.9715050056034433, 0.999483896371703, 0.017713730111172516, 0.9742551561144883, 0.9795224811849154, 0.99971739515004, 0.7205203505002182, 0.7228905300600901, 0.8032615804437627, 0.9711065657616806, 0.9878067157776574, 0.9952491868876849, 0.004205278254455006, 0.332338504317435, 0.6671028371335375, 0.88073416599927, 0.9985702101764673, 0.1904231073242694, 0.8040086753691374, 0.9402404554395973, 0.9119177965776051, 0.08792595834770296, 0.6701305264160117, 0.9973694219444077, 0.9861925763376694, 0.03148685403162208, 0.9603490479644735, 0.2428811724728751, 0.7521481470127744, 0.9910953219352233, 0.30838840986851845, 0.674599646587384, 0.5793135372509284, 0.885135562478865, 0.9996396263639119, 0.5602771223427198, 0.13358139805237632, 0.868279087340446, 0.9811168334616681, 0.9675569775859132, 0.9352010800277198, 0.9613116285474077, 0.9883456623236813, 0.996450020821565, 0.9881734139872306, 0.9995144264066218, 0.9447192181869472, 1.0007789497754873, 0.9815856132654901, 0.08629016298526354, 0.8393679490384727, 0.07844560271387595, 0.9222430524732059, 0.9060673401686014, 0.7867232163624025, 0.8656635596425954, 0.6998938677200469, 0.30111712913536903, 0.09892212394214556, 0.9044308474710451, 0.7885804821416023, 0.9950891870685249, 0.9996147011129988, 0.9988014297363323, 0.5521140401035228, 0.450408822189716, 0.999807386856733, 0.9157824833055686, 0.8118494901239007, 0.9858845285590465, 0.9508690901024826, 0.9484862299661451, 0.9997728739234676, 0.922100401704269, 0.6895788938319016, 0.9037653728168267, 0.988852461030113, 0.9772042276461509, 0.979441366767937, 0.9908965757271299, 0.9955226099045503, 0.6691167382116668, 0.9842008210752914, 0.9983515000883784, 0.6147215954623662, 0.8467994539559283, 0.9856001825113775, 0.8595917427223739, 0.9938456348930458, 0.006075912178385433, 0.9689347331657372, 0.9925283069798457, 0.9212281435481083, 0.9847627511757331, 0.9004595575628374, 0.9966797494169664, 0.9967430734917111, 0.9931536833234109, 0.005419665393306472, 0.001354916348326618, 0.9054412678974, 0.11482885885273009, 0.8803545845375973, 0.9753458225345025, 0.5732509709465774, 0.4275091986720238, 0.9766887996157201, 0.9971325845361467, 0.988049843001986, 0.9984920471312572, 0.9999862560796113, 0.9873960592133113, 0.9999162786624268, 0.543613513178837, 0.4593917012778905, 0.9812638558296156, 0.8816297781425639, 0.02750179713094807, 0.9625628995831824, 0.987675312678412, 0.7910580111049034, 0.9859185687042497, 0.9856689436630044, 0.5259852610197202, 0.47567362735696433, 0.7028923468887899, 0.8960282195286235, 0.984503688123479, 0.5467226726820391, 0.999908273365065, 0.998070536095003, 0.9946888199440007, 0.9994472666265436, 0.9960182713862541, 0.9948519104279512, 0.9708595056998242, 0.028435215648112005, 0.9232894441067196, 0.9870281410674362, 0.011995133658805649, 0.9819332840545449, 0.5588589746708378, 0.08839096027957127, 0.33075327072355704, 0.022810570394728072, 0.5227979091204403, 0.7665792873440054, 0.08116721865995351, 0.15030966418509908, 0.5807869460354494, 0.413965163663565, 0.4818412820934139, 0.516036469854882, 0.9784494280295981], \"Term\": [\"absolute\", \"accent\", \"accomplish\", \"action\", \"actor\", \"actor\", \"actually\", \"ad\", \"admirer\", \"also\", \"also\", \"apartment\", \"apartment\", \"approach\", \"area\", \"art\", \"atmosphere\", \"attack\", \"audience\", \"audience\", \"available\", \"bad\", \"banish\", \"battle\", \"battle\", \"bear\", \"beautiful\", \"beautiful\", \"beautifully\", \"beauty\", \"beauty\", \"become\", \"become\", \"become\", \"believable\", \"bond\", \"bored\", \"bottle\", \"british\", \"buy\", \"car\", \"car\", \"case\", \"case\", \"central\", \"character\", \"character\", \"charismatic\", \"chase\", \"chevy\", \"child\", \"child\", \"child\", \"cinderella\", \"city\", \"city\", \"class\", \"clint_eastwood\", \"club\", \"clumsy\", \"cold\", \"collection\", \"college\", \"come\", \"comedic\", \"conflict\", \"control\", \"corner\", \"country\", \"cousin\", \"create\", \"create\", \"create\", \"creation\", \"creature\", \"davy\", \"deal\", \"deal\", \"defend\", \"delight\", \"delightful\", \"delivery\", \"describe\", \"description\", \"detailed\", \"dimension\", \"directorial\", \"disappoint\", \"discover\", \"discover\", \"documentary\", \"documentary\", \"doll\", \"eale\", \"early\", \"early\", \"eastwood\", \"edge\", \"elevate\", \"embarrassment\", \"emotionally\", \"end\", \"english\", \"english\", \"enter\", \"entertain\", \"epic\", \"era\", \"erotic\", \"even\", \"event\", \"eventually\", \"everyone_else\", \"evident\", \"exceptional\", \"exceptionally\", \"extraordinary\", \"eye\", \"eye\", \"eye\", \"factory\", \"faithful\", \"familiar\", \"fare\", \"fascinating\", \"favourite\", \"feel\", \"fictional\", \"fight\", \"film\", \"film\", \"find\", \"fine\", \"fine\", \"fine\", \"first\", \"foot\", \"force\", \"force\", \"force\", \"freedom\", \"friendship\", \"general\", \"general\", \"generation\", \"genius\", \"get\", \"girl\", \"girl\", \"give\", \"glory\", \"go\", \"good\", \"goodness\", \"gorgeous\", \"great\", \"hang\", \"haunt\", \"hell\", \"high\", \"high\", \"highly_recommend\", \"highly_recommended\", \"holly\", \"horrify\", \"housewife\", \"hungry\", \"hurt\", \"husband\", \"icon\", \"imdb\", \"impact\", \"indeed\", \"indie\", \"intense\", \"irrelevant\", \"island\", \"journey\", \"juvenile\", \"kermit\", \"know\", \"lame\", \"law\", \"life\", \"life\", \"life\", \"listen\", \"little\", \"little_bit\", \"live_action\", \"local\", \"local\", \"lone\", \"look\", \"love\", \"low\", \"magnificent\", \"maintain\", \"make\", \"make\", \"male\", \"male\", \"man\", \"man\", \"man\", \"man\", \"manner\", \"masterpiece\", \"maybe\", \"meanwhile\", \"memorable\", \"minor\", \"mislead\", \"money\", \"monster\", \"movie\", \"much\", \"muppet\", \"nasty\", \"natural\", \"natural\", \"necessary\", \"never\", \"nomination\", \"operate\", \"overwrought\", \"parallel\", \"park\", \"people\", \"people\", \"performance\", \"performance\", \"persona\", \"personal\", \"plan\", \"plan\", \"plane\", \"play\", \"play\", \"plight\", \"political\", \"pop\", \"popular\", \"popular\", \"portray\", \"portray\", \"portrayal\", \"powerful\", \"powerful\", \"precious\", \"princess\", \"prison\", \"propaganda\", \"provide\", \"provide\", \"public\", \"pure\", \"quirky\", \"rank\", \"rape\", \"rate\", \"realise\", \"really\", \"recent_year\", \"red\", \"regret\", \"relationship\", \"relationship\", \"relationship\", \"relatively\", \"remark\", \"responsible\", \"robot\", \"role\", \"role\", \"romantic\", \"romantic\", \"sadness\", \"satire\", \"say\", \"scene\", \"school\", \"school\", \"see\", \"selfish\", \"serial\", \"sexual\", \"shed\", \"shepherdess\", \"show\", \"silent\", \"simplicity\", \"skill\", \"slightly\", \"social\", \"social_worker\", \"society\", \"soldier\", \"solution\", \"soul\", \"still\", \"stimulate\", \"stink\", \"stone\", \"stop_watche\", \"story\", \"story\", \"studio\", \"stunning\", \"substance\", \"succeed\", \"surface\", \"surprising\", \"system\", \"take\", \"take\", \"take\", \"takes_place\", \"tale\", \"tale\", \"tape\", \"team\", \"team\", \"teenage\", \"tension\", \"terrific\", \"thing\", \"think\", \"thus\", \"time\", \"today\", \"today\", \"trailer\", \"transformation\", \"truth\", \"truth\", \"unusual\", \"upset\", \"vampire\", \"vhs\", \"view\", \"view\", \"virus\", \"visual\", \"visually\", \"void\", \"want\", \"war\", \"warm\", \"watch\", \"water\", \"wave\", \"way\", \"way\", \"weave\", \"well\", \"well\", \"whole_thing\", \"woman\", \"woman\", \"woman\", \"woman\", \"wooden\", \"world\", \"world\", \"world\", \"yet\", \"yet\", \"young\", \"young\", \"youth\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [7, 8, 1, 9, 4, 5, 6, 3, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el967222920431753448736990158\", ldavis_el967222920431753448736990158_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el967222920431753448736990158\", ldavis_el967222920431753448736990158_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el967222920431753448736990158\", ldavis_el967222920431753448736990158_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T12:51:49.757416Z",
     "start_time": "2024-04-19T12:51:45.278059Z"
    }
   },
   "source": [
    "list(enumerate(lda_model[corpus]))[0][1][0]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.018127156), (6, 0.91621286), (7, 0.045854088), (8, 0.013718232)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T12:53:57.603102Z",
     "start_time": "2024-04-19T12:53:53.092754Z"
    }
   },
   "source": [
    "topic = []\n",
    "\n",
    "for i, row_list in enumerate(lda_model[corpus]):\n",
    "    row = sorted(row_list[0], key=lambda x: (x[1]), reverse=True)\n",
    "    topic.append(row[0][0])\n",
    "    \n",
    "dataset['topic'] = topic"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T12:54:05.565908Z",
     "start_time": "2024-04-19T12:54:05.552431Z"
    }
   },
   "source": [
    "dataset"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                 review  label  topic\n",
       "0     To quote Clark Griswold (in the original Chris...      0      6\n",
       "1     Steven buddy, you remember when you said this:...      0      6\n",
       "2     \"Maximum Risk\" is a step sideways for Van Damm...      0      6\n",
       "3     I am a Catholic taught in parochial elementary...      0      6\n",
       "4     To call this anything at all would be an insul...      0      6\n",
       "...                                                 ...    ...    ...\n",
       "1995  In September 2003 36-year-old Jonny Kennedy di...      1      6\n",
       "1996  Great movie in a Trainspotting style... Being ...      1      6\n",
       "1997  (originally a response to a movie reviewer who...      1      6\n",
       "1998  Ernest P. Worrell comes through with his third...      1      6\n",
       "1999  This movie was funny from START to FINISH. Eve...      1      6\n",
       "\n",
       "[2000 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To quote Clark Griswold (in the original Chris...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Steven buddy, you remember when you said this:...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Maximum Risk\" is a step sideways for Van Damm...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>To call this anything at all would be an insul...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>In September 2003 36-year-old Jonny Kennedy di...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>Great movie in a Trainspotting style... Being ...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>(originally a response to a movie reviewer who...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>Ernest P. Worrell comes through with his third...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>This movie was funny from START to FINISH. Eve...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T12:54:11.266320Z",
     "start_time": "2024-04-19T12:54:11.245187Z"
    }
   },
   "source": [
    "set(list(dataset.topic))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{6}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second part of this lab, we will explore how to use the word embedding model for sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "   <img src=\"img/word2vec.png\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Popular Pre-trained Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T13:16:21.502504Z",
     "start_time": "2024-04-19T13:16:20.894187Z"
    }
   },
   "source": [
    "import gensim.downloader\n",
    "\n",
    "list(gensim.downloader.info()['models'].keys())"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fasttext-wiki-news-subwords-300',\n",
       " 'conceptnet-numberbatch-17-06-300',\n",
       " 'word2vec-ruscorpora-300',\n",
       " 'word2vec-google-news-300',\n",
       " 'glove-wiki-gigaword-50',\n",
       " 'glove-wiki-gigaword-100',\n",
       " 'glove-wiki-gigaword-200',\n",
       " 'glove-wiki-gigaword-300',\n",
       " 'glove-twitter-25',\n",
       " 'glove-twitter-50',\n",
       " 'glove-twitter-100',\n",
       " 'glove-twitter-200',\n",
       " '__testing_word2vec-matrix-synopsis']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T13:17:26.468008Z",
     "start_time": "2024-04-19T13:16:43.309956Z"
    }
   },
   "source": [
    "w2v = gensim.downloader.load('glove-wiki-gigaword-50')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T13:20:02.506724Z",
     "start_time": "2024-04-19T13:20:02.463625Z"
    }
   },
   "source": "w2v.most_similar('machine')",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('machines', 0.8238813877105713),\n",
       " ('device', 0.8175780773162842),\n",
       " ('using', 0.7789539694786072),\n",
       " ('gun', 0.7508804798126221),\n",
       " ('used', 0.7492657899856567),\n",
       " ('devices', 0.7369279265403748),\n",
       " ('uses', 0.7261233329772949),\n",
       " ('portable', 0.7247284650802612),\n",
       " ('automatic', 0.722038209438324),\n",
       " ('drives', 0.7156105041503906)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie Review Sentiment Analysis Using Pre-Trained Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T13:28:52.413807Z",
     "start_time": "2024-04-19T13:28:52.347202Z"
    }
   },
   "source": [
    "dataset"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                 review  label  topic\n",
       "0     To quote Clark Griswold (in the original Chris...      0      6\n",
       "1     Steven buddy, you remember when you said this:...      0      6\n",
       "2     \"Maximum Risk\" is a step sideways for Van Damm...      0      6\n",
       "3     I am a Catholic taught in parochial elementary...      0      6\n",
       "4     To call this anything at all would be an insul...      0      6\n",
       "...                                                 ...    ...    ...\n",
       "1995  In September 2003 36-year-old Jonny Kennedy di...      1      6\n",
       "1996  Great movie in a Trainspotting style... Being ...      1      6\n",
       "1997  (originally a response to a movie reviewer who...      1      6\n",
       "1998  Ernest P. Worrell comes through with his third...      1      6\n",
       "1999  This movie was funny from START to FINISH. Eve...      1      6\n",
       "\n",
       "[2000 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To quote Clark Griswold (in the original Chris...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Steven buddy, you remember when you said this:...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Maximum Risk\" is a step sideways for Van Damm...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>To call this anything at all would be an insul...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>In September 2003 36-year-old Jonny Kennedy di...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>Great movie in a Trainspotting style... Being ...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>(originally a response to a movie reviewer who...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>Ernest P. Worrell comes through with his third...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>This movie was funny from START to FINISH. Eve...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T13:29:03.634874Z",
     "start_time": "2024-04-19T13:29:03.605468Z"
    }
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "reviews = dataset['review'].values\n",
    "y = dataset['label'].values\n",
    "\n",
    "reviews_train, reviews_test, y_train, y_test = train_test_split(reviews, y, test_size=0.25, random_state=1000)"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T13:29:06.815737Z",
     "start_time": "2024-04-19T13:29:06.238742Z"
    }
   },
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english', max_df = 0.5, min_df = 0.02)\n",
    "vectorizer.fit(reviews_train)\n",
    "\n",
    "X_train = vectorizer.transform(reviews_train).toarray()\n",
    "X_test = vectorizer.transform(reviews_test).toarray()"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T13:29:09.013687Z",
     "start_time": "2024-04-19T13:29:09.003276Z"
    }
   },
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 814)\n",
      "(500, 814)\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T13:29:13.592897Z",
     "start_time": "2024-04-19T13:29:13.549115Z"
    }
   },
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "Model_lr = LogisticRegression()\n",
    "Model_lr.fit(X_train, y_train)\n",
    "score_lr = Model_lr.score(X_test, y_test)\n",
    "\n",
    "print(\"Testing accuracy: {:.4f}\".format(score_lr))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy: 0.8060\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T13:31:35.495511Z",
     "start_time": "2024-04-19T13:31:31.854391Z"
    }
   },
   "source": [
    "from keras.api.models import Sequential\n",
    "from keras.api.layers import Dense\n",
    "from keras.api.backend import clear_session\n",
    "\n",
    "\n",
    "text_dim = X_train.shape[1]\n",
    "\n",
    "Model_mlp = Sequential()\n",
    "Model_mlp.add(Dense(100, input_dim = text_dim, activation = 'relu'))\n",
    "Model_mlp.add(Dense(10, activation = 'relu'))\n",
    "Model_mlp.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "print(Model_mlp.summary())"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001B[38;5;33mDense\u001B[0m)                   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m100\u001B[0m)            │        \u001B[38;5;34m81,500\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m10\u001B[0m)             │         \u001B[38;5;34m1,010\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)              │            \u001B[38;5;34m11\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">81,500</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m82,521\u001B[0m (322.35 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">82,521</span> (322.35 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m82,521\u001B[0m (322.35 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">82,521</span> (322.35 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-19T13:32:41.581215Z",
     "start_time": "2024-04-19T13:31:56.373617Z"
    }
   },
   "source": [
    "Model_mlp.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "clear_session()\n",
    "\n",
    "Result_mlp = Model_mlp.fit(X_train, y_train, batch_size=10, epochs=100, verbose=False, validation_data=(X_test, y_test))\n",
    "\n",
    "loss, accuracy = Model_mlp.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = Model_mlp.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing accuracy:  {:.4f}\".format(accuracy))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\David\\IdeaProjects\\MM5427\\venv\\lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:74: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "Training accuracy: 1.0000\n",
      "Testing accuracy:  0.7820\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to tokenize the text data into a format that can be used by the word embeddings. To do so, we:\n",
    "1. Create a vocabulary based on the training movie reviews.\n",
    "2. Transfer all training and testing movie reviews into lists of integers. Each integer map a word to a value in a dictionary.\n",
    "\n",
    "We can use the parameter ***num_words*** to control the size of the vocabulary. Unknown words will be ignored, while you can also use the parameter ***oov_token*** to set all unknown words to a same index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "   <img src=\"img/procedure.png\" width=\"600\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T12:01:43.907288Z",
     "start_time": "2024-04-25T12:01:34.044515Z"
    }
   },
   "source": [
    "from keras_preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(reviews_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(reviews_train)\n",
    "X_test = tokenizer.texts_to_sequences(reviews_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    "print(vocab_size)\n",
    "print(reviews_train[2])\n",
    "print(X_train[2])"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras.preprocessing.text'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtext\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Tokenizer\n\u001B[0;32m      3\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m Tokenizer(num_words\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5000\u001B[39m)\n\u001B[0;32m      4\u001B[0m tokenizer\u001B[38;5;241m.\u001B[39mfit_on_texts(reviews_train)\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'keras.preprocessing.text'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep: 932\n",
      "learning: 3630\n",
      "business: 755\n",
      "analysis: 8103\n"
     ]
    }
   ],
   "source": [
    "for word in ['deep', 'learning', 'business', 'analysis']:\n",
    "    print('{}: {}'.format(word, tokenizer.word_index[word]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue we face is that documents vary in length. To address this, we will pad the sequence of words with zeros.\n",
    "\n",
    "Additionally, we will specify a parameter, **maxlen**, to truncate longer texts. This is necessary to enhance training efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 100\n",
    "\n",
    "X_train = pad_sequences(X_train, padding = 'post', maxlen = maxlen)\n",
    "X_test = pad_sequences(X_test, padding = 'post', maxlen = maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   6  468   28    4  111  106    9   39   47   75  280  302    2    3\n",
      "  167    4 1066   35    9  193   34    3  692  159  917    1   19    6\n",
      " 4387 1720  145    2   10  100  172   74    4    1  229   38 2500    2\n",
      " 1982  331  733    2    3   51  715  647   92    9    3 1469   19    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 517  217   11    6    3 4392  271   19    1  150  549   14    1    2\n",
      " 1098 2503   14    1  261 1525   25   29  365    1  229    6    2  234\n",
      "    2    1  366  311 1161  169  347  439   42   59  118   91   26   11\n",
      "   97  475 3164 1722    3  734    5 1983   12   36   25 2352    2   66\n",
      "  204   35   80 1984    5    1  820   33   25 1471   42   21  186    5\n",
      "  116    3   19    5  120  271   12    6 3165    8   51 1161   17    6\n",
      "   84  569  418  164   11   18    9    6    5   24  288 1132   60   14\n",
      "   67   25]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the nicely in-bulit Keras embedding layer for the word embedding model. The following parameters are needed:\n",
    "\n",
    "1. **input_dim**: the size of the vocabulary\n",
    "2. **output_dim**: the size of the dense vector\n",
    "3. **input_length**: the length of the text sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 50)           1204100   \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 5000)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                50010     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,254,121\n",
      "Trainable params: 1,254,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.api.layers import Embedding\n",
    "from keras.api.layers import Flatten\n",
    "\n",
    "embedding_dim = 50\n",
    "\n",
    "model_em1 = Sequential()\n",
    "model_em1.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen))\n",
    "model_em1.add(Flatten())\n",
    "model_em1.add(Dense(10, activation='relu'))\n",
    "model_em1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_em1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1163450\n",
      "1204100\n"
     ]
    }
   ],
   "source": [
    "print(23269*50)\n",
    "print(vocab_size * embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 1.0000\n",
      "Testing accuracy:  0.7200\n"
     ]
    }
   ],
   "source": [
    "model_em1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "clear_session()\n",
    "\n",
    "Result_em1 = model_em1.fit(X_train, y_train, batch_size=10, epochs=50, verbose=False, validation_data=(X_test, y_test))\n",
    "\n",
    "loss, accuracy = model_em1.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model_em1.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Pooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 50)           1204100   \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 50)               0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                510       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,204,621\n",
      "Trainable params: 1,204,621\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.api.layers import GlobalMaxPool1D\n",
    "\n",
    "embedding_dim = 50\n",
    "\n",
    "model_em2 = Sequential()\n",
    "model_em2.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen))\n",
    "model_em2.add(GlobalMaxPool1D())\n",
    "model_em2.add(Dense(10, activation='relu'))\n",
    "model_em2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_em2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 1.0000\n",
      "Testing accuracy:  0.7860\n"
     ]
    }
   ],
   "source": [
    "model_em2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "Result_em2 = model_em2.fit(X_train, y_train, batch_size=10, epochs=50, verbose=False, validation_data=(X_test, y_test))\n",
    "\n",
    "loss, accuracy = model_em2.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model_em2.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, we learned word embeddings concurrently with the deep learning weights that help predict review sentiment. An alternative approach involves utilizing pre-trained word vectors, such as **word2vec** and **GloVe**.\n",
    "\n",
    "We will demonstrate this concept using GloVe. You can access the pre-trained word vectors by following this link: https://nlp.stanford.edu/projects/glove/. Specifically, we will use the **glove.6B.50d.txt** version, which is pre-trained on Wikipedia 2014 and Gigaword 5 and features 50-dimensional vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we do not need all pre-trained word vectors in **GloVe**, we can first prepare a word embedding matrix that contains words only in our movie review dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the \\* in front of a variable:\n",
    "* \\* unpacks a list or tuple into position arguments.\n",
    "* ** unpacks a dictionary into keyword arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # We add 1 to the vocabulary size as we need to reserve 0 index for sequence padding\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "\n",
    "embedding_matrix = create_embedding_matrix('glove.6B.50d.txt', tokenizer.word_index, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.41800001,  0.24968   , -0.41242   , ..., -0.18411   ,\n",
       "        -0.11514   , -0.78580999],\n",
       "       [ 0.26818001,  0.14346001, -0.27877   , ..., -0.63209999,\n",
       "        -0.25027999, -0.38097   ],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 1.07190001, -0.4815    , -0.59113002, ...,  0.75568002,\n",
       "         0.1908    ,  0.79163998]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed Embedding Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 100, 50)           1204100   \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Glo  (None, 50)               0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                510       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,204,621\n",
      "Trainable params: 521\n",
      "Non-trainable params: 1,204,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_em3 = Sequential()\n",
    "model_em3.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, \n",
    "                        weights=[embedding_matrix], input_length=maxlen, \n",
    "                        trainable=False))\n",
    "model_em3.add(GlobalMaxPool1D())\n",
    "model_em3.add(Dense(10, activation='relu'))\n",
    "model_em3.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_em3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.6940\n",
      "Testing accuracy:  0.7020\n"
     ]
    }
   ],
   "source": [
    "model_em3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "clear_session()\n",
    "\n",
    "Result_em3 = model_em3.fit(X_train, y_train, batch_size=10, epochs=50, verbose=False, validation_data=(X_test, y_test))\n",
    "\n",
    "loss, accuracy = model_em3.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model_em3.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained Embedding Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 1.0000\n",
      "Testing accuracy:  0.7340\n"
     ]
    }
   ],
   "source": [
    "model_em4 = Sequential()\n",
    "model_em4.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, \n",
    "                        weights=[embedding_matrix], input_length=maxlen, \n",
    "                        trainable=True))\n",
    "model_em4.add(GlobalMaxPool1D())\n",
    "model_em4.add(Dense(10, activation='relu'))\n",
    "model_em4.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_em4.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "clear_session()\n",
    "\n",
    "Result_em4 = model_em4.fit(X_train, y_train, batch_size=10, epochs=50, verbose=False, validation_data=(X_test, y_test))\n",
    "\n",
    "loss, accuracy = model_em4.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model_em4.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
